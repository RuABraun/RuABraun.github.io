<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://ruabraun.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ruabraun.github.io/" rel="alternate" type="text/html" /><updated>2022-07-28T17:20:00+00:00</updated><id>https://ruabraun.github.io/feed.xml</id><title type="html">Sharings</title><author><name>Rudolf A. Braun</name></author><entry><title type="html">k2 simple rnnt loss</title><link href="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html" rel="alternate" type="text/html" title="k2 simple rnnt loss" /><published>2022-07-15T10:06:02+00:00</published><updated>2022-07-15T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html">&lt;p&gt;This is a summary of how k2/icefall makes the transducer loss faster and take less memory, see here for an excellent introduction to transducers: &lt;a href=&quot;https://arxiv.org/abs/2206.13236&quot;&gt;https://lorenlugosch.github.io/posts/2020/11/transducer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The normal rnnt loss requires creating a matrix (B,T,U,V) where B=batch size; T=time steps; U=symbols in reference and V=vocab size.&lt;/p&gt;

&lt;p&gt;This matrix is created by combining the encoder and decoder outputs, which have shape (B,T,C) and (B,U,C). The combination is done by adding the two (with dimension unsqueezing so the result of the addition is (B,T,U,C)), and then projecting to (B,T,U,V). (This means time and symbol states are independent in the sense of p(X,Y) = p(X)p(Y))&lt;/p&gt;

&lt;p&gt;The simple rnnt loss (from k2) avoids creating the very large (B,T,U,V) matrix. It does so by recognizing that for training we don’t need to have a distribution across all tokens in V as we have a training transcript so we know at each position (T,U) the token probability we care about. Since we also care about the blank transition, the simple rnnt loss calculates two matrices (B,T,U) which contain the appropriate symbol and blank probabilities respectively.&lt;/p&gt;

&lt;p&gt;The calculation is done by first projecting encoder and decoder outputs to (B,T,V) and (B,U,V), doing matrix-multiply of (B,T,V), (B,V,U) to get a normalization matrix (B,T,U), and then picking out the unnormalized token log probabilities we care about from (B,T,V) plus (B,U,V) and subtracting the normalization value (to get a normalized value). Logprobs are used when adding, normal probs wwhen multiplying (for the matrix multiply) normal probs are used, so the implemenation has some exp() and log() calls.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Note you wouldn’t normally want to take this approach because the encoder and decoder outputs don’t get to interact before the token distribution is calculated (they’re added together after the projection to size V), it’s as if you had separate AM and LM models and you were just adding together P(y&lt;/td&gt;
      &lt;td&gt;x) and P(y&lt;/td&gt;
      &lt;td&gt;y-1) (the AM just seeing audio and the LM just text). The point of RnnT is that you want an output P(y&lt;/td&gt;
      &lt;td&gt;x,y_1), something that directly conditions on both audio and text.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;After some training this approach will tell us which paths in (T,U) are more or less likely, because the simple loss will calculate its own version of (T,U,) and many paths in it will have low probability. This information can be used to set boundaries for each time step in T, which allows doing the proper RNNT loss on a subset (B,T,S,V) with S&amp;lt;&amp;lt;U (because we know that for a given point in time only some tokens are possible, not all in U), which takes much less memory, is faster and trains the P(y&lt;/td&gt;
      &lt;td&gt;x,y_1) output.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Let’s look at some code (I collapsed whitespace to make things more compact). Here’s the joiner that creates (B,T,U,V). Note the dimension unsqueezing of the encoder and decoder outputs is assumed to have already happened (code is from a recipe specific file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;joiner.py&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_joiner.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the following image you can see the separate projection of encoder and decoder outputs to the vocab size, then computing a simple loss and using that (specifically the gradients) to get boundaries for creating (B,T,S,V). Directly after the last line shown the normal rnnt loss is calculated (code is from a recipe specific file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.py&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_losshighlevel.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking slightly deeper into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.rnnt_loss_smoothed&lt;/code&gt; we can see there are two stages: First calculating px ((T,U,) of reference symbols) and py ((T,U,) of blank symbol), then doing the standard dynammic programming triple for-loop (for the batch, time, symbol dimensions) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutual_information_recursion&lt;/code&gt; to calculate the total logprob across all alignments. Despite the intimidating name the implementation of the latter is quite straightforward (for CPU at least), see &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/csrc/torch/mutual_information_cpu.cu&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_smoothloss.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I hope this helps give an overview and basic understanding of how the rnnt loss is calculated in k2/icefall.&lt;/p&gt;

&lt;p&gt;A preprint is available with nice results: &lt;a href=&quot;https://arxiv.org/abs/2206.13236&quot;&gt;https://arxiv.org/abs/2206.13236&lt;/a&gt;&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This is a summary of how k2/icefall makes the transducer loss faster and take less memory, see here for an excellent introduction to transducers: https://lorenlugosch.github.io/posts/2020/11/transducer/</summary></entry><entry><title type="html">A comparison of fairseq, speechbrain, k2 for ASR</title><link href="https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html" rel="alternate" type="text/html" title="A comparison of fairseq, speechbrain, k2 for ASR" /><published>2022-06-26T10:06:02+00:00</published><updated>2022-06-26T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html">&lt;p&gt;WIP&lt;/p&gt;

&lt;p&gt;I have had the opportunity to work with some of the different E2E ASR toolkits framework out there and thought it could be useful to give an overview and comparison of these.&lt;/p&gt;

&lt;p&gt;This will for each give a high level overview of the code base, what the training loop looks like, and name stuff I liked or disliked. In the end I will make some comments comparing them.
Whenever I refer to files, I assume you are starting from the root folder of the repo.&lt;/p&gt;
&lt;h1 id=&quot;fairseq&quot;&gt;fairseq&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/fairseq&quot;&gt;Fairseq&lt;/a&gt; is not just meant for ASR. It’s for many different modeling tasks, including language modeling, translation, masked LM pretraining, summarization an
d text to speech.&lt;/p&gt;

&lt;p&gt;Training models and doing inference is done via command-line scripts (found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq_cli/&lt;/code&gt;). The training loop and data processing code is unified when possible. As a consequence the code is generally a bit more abstract, you cannot look at just one file if you want to read and understand the training loop.&lt;br /&gt;
The codebase can be seen as split into four different parts.\&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq_cli/&lt;/code&gt; contains CLI scripts for data processing, model training and evaluation.\&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/&lt;/code&gt; (except for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tasks/&lt;/code&gt;) is like a library of common models, data processing or utility classes and functions\&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/tasks/&lt;/code&gt; has files which contain code specific to a modeling task\&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/&lt;/code&gt; has the documentation and scripts for training and evaluating models related to a facebook paper(s) and corpus\&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When training you pass a “task” argument (when calling the CLI scripts) which ensures the appropriate code is run for whatever task you are doing. Many modifications to a task, like using a different model, can done by changing a command-line argument or using a config file to redefine a default (the config is based on hydra/omegaconf). Models defined in fairseq (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/models/&lt;/code&gt;) will have a decorator enabling you to specify it using one keyword (making it easy to change).&lt;/p&gt;

&lt;p&gt;There are several tasks related to speech recognition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech_to_text&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech_recognition&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio_pretraining&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio_finetuning&lt;/code&gt;. The latter two are for wav2vec[2]. It seems they are planning to unify all the previously mentioned ASR related ones to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech_to_text&lt;/code&gt;, but this is still WIP.&lt;/p&gt;

&lt;p&gt;The data format depends on the task and tends to favor simplicity. For example for wav2vec pretraining you just need a .tsv file which has on the first line a root path, and every line afterwards has a path to an audio file and its sample count. That’s it. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/*&lt;/code&gt; there always is an explanation for how to get to the data format needed.&lt;/p&gt;

&lt;p&gt;I find fairseq to be a good toolkit for reproducing results from facebook’s papers - I was surprised how easy it was to do the pretraining for wav2vec models - and convenient if you want to toy around with a research idea that is slightly different from what already exists. A lot of small changes that you might want to do can be done fairly easily. If you’re doing something significantly different from what already exists that could get a lot harder and you will likely have to end up learning about how the entire fairseq repo ties together to achieve that. But I do think once you get it it could be quite nice to work with. One downside is you can’t rely much on support from the maintainers; github issues are responded to rarely. It also is not very suited for using in production (but to be fair that is not what it’s made for).&lt;/p&gt;

&lt;p&gt;Let’s skim over the training loop, we start in &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/train.py#L180&quot;&gt;fairseq_cli/train.py&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while epoch_itr.next_epoch_idx &amp;lt;= max_epoch:
    [..]
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;With train defined in the same file and then calling &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/train.py#L312&quot;&gt;trainer.train_step()&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;logger.info(&quot;Start iterating over samples&quot;)
for i, samples in enumerate(progress):
     [..]
	 log_output = trainer.train_step(samples)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The dataloader is wrapped inside of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;progress&lt;/code&gt;.  The trainer object is defined in &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/trainer.py&quot;&gt;fairseq/trainer.py&lt;/a&gt; and its &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/trainer.py#L824&quot;&gt;train_step()&lt;/a&gt; calls&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i, sample in enumerate(samples):  # delayed update loop
    [..]
	loss, sample_size_i, logging_output = self.task.train_step(
		sample=sample,
		model=self.model,
		criterion=self.criterion,
		optimizer=self.optimizer,
		update_num=self.get_num_updates(),
		ignore_grad=is_dummy_batch,
		**extra_kwargs,
	)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;the task specific train step. Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;samples&lt;/code&gt; will have a length bigger 1 only if gradient accumulation is used.&lt;/p&gt;

&lt;p&gt;A lot of tasks actually don’t have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_step()&lt;/code&gt; defined and just use the generic one defined in &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/tasks/fairseq_task.py#L490&quot;&gt;fairseq/tasks/fairseq_task.py&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;speechbrain&quot;&gt;speechbrain&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/speechbrain/speechbrain&quot;&gt;Speechbrain&lt;/a&gt; is for speech processing tasks, they have an impressive array of examples where they get good results (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;recipes/&lt;/code&gt;).  The core training loop is unified in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; class (found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speechbrain/core.py&lt;/code&gt;), the data processing code is specified in each recipe.&lt;/p&gt;

&lt;p&gt;It uses a powerful yaml config (&lt;a href=&quot;https://github.com/speechbrain/HyperPyYAML&quot;&gt;HyperPyYaml&lt;/a&gt;) which is generally used not only to define hyperparameters but also to define data processing related things like the data loading sampler, the loss, the optimizer and the model. A complicated model would be defined by refering to the speechbrain implementation (for example a transformer), a simple one like an 3-layer MLP you could define right in the config.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; calls is initialized with this config, and will therefore hold a reference to everything related to training such as the model, the loss, the optimizer and so on.&lt;/p&gt;

&lt;p&gt;How the model is used is specified by overriding two methods  [of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; class] &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_forward&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_objectives&lt;/code&gt;. The former defines how to get from the data to the outputs you need to compute the loss. You then use the latter to define how to compute the loss from those outputs (it’s  called automatically after the former). Sometimes the implementation will be extremely simple: In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_forward&lt;/code&gt; you call the model and in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_objectives&lt;/code&gt; you call the loss defined in your config. If you have a model with separate stages and/or have multiple losses then the methods would be the location to specify how they are connected.&lt;/p&gt;

&lt;p&gt;The data is typically stored in CSV format (with filepaths pointing to audio data of course), in your recipe you then specify (via a custom “pipeline” construct) how to go from the data in the CSV to the data you want available in a batch.&lt;/p&gt;

&lt;p&gt;I like the config and how powerful it is.  I think they’ve made some good decisions about how to allow people to make the changes they need to write code specific to their problem. I have though sometimes been bitten by behaviour that I did not expect. It definitely feels more like a framework than a toolkit, something that expects you to do things in a certain way and if you don’t you could end up having a hard time. The code is very object oriented.&lt;/p&gt;

&lt;h1 id=&quot;k2icefalllhotse&quot;&gt;k2/icefall/lhotse&lt;/h1&gt;

&lt;p&gt;This is by far the newest of the three tools I’m comparing here. I wrote k2/icefall/lhotse because there are three different repositories you would make use of: &lt;a href=&quot;https://github.com/k2-fsa/k2&quot;&gt;k2&lt;/a&gt; for ragged tensors and lattice related operations, &lt;a href=&quot;https://github.com/lhotse-speech/lhotse&quot;&gt;lhotse&lt;/a&gt; for everything related to data and &lt;a href=&quot;https://github.com/k2-fsa/icefall&quot;&gt;icefall&lt;/a&gt; which contains recipes using the former two.&lt;/p&gt;

&lt;p&gt;So for example icefall contains recipes for training transducer models. The downloading of a corpus, the feature extraction, the code for taking care of dynamic batching will come from lhotse. Ragged tensors, RNNT loss computations, (WFST) decoding graphs will come from k2. Icefall has the contain putting everything together. There is no unified training loop, each recipe will have its own straight-forward implementation making use of lhotse and k2 to do training and inference.&lt;/p&gt;

&lt;p&gt;This makes reading the implementation and making changes very easy. I have achieved very good performance and plan to investigate the implementation details. Installing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lhotse&lt;/code&gt; gives access to a very nice CLI that easily allows one to convert from a kaldi style data dir to a lhotse style manifest (actually consisting of two files, one for the audio data and one for the labels) and to extract features. I will say though the decorator ratio in the lhotse codebase is the highest I’ve ever seen, debugging can be a pain. Which brings me to the fact that this is all very new and in a state of flux, so there are a relatively high amount of bugs. On the bright side everyone involved is very responsive and if something doesn’t work one can get help very quickly.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">WIP</summary></entry><entry><title type="html">Why I don’t like the black code formatter</title><link href="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html" rel="alternate" type="text/html" title="Why I don’t like the black code formatter" /><published>2021-11-18T10:06:02+00:00</published><updated>2021-11-18T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html">&lt;p&gt;First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.&lt;/p&gt;

&lt;p&gt;However:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;’ is cleaner than “&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To me, ‘ has less visual noise than “. It’s a single stroke rather than two. Additionally, on US keyboards ‘ does not require pressing shift, “ does.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Black loves newlines, this leads to too much whitespace and makes skimming code harder&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Take a look at this image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/blackbloat_args.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This could take half the space, and then I could view twice as many arguments in one go.&lt;/p&gt;

&lt;p&gt;Black favoring to break functions with arguments up like this&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function(
    argument
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;generally leads to one seeing much less code on one’s screen than originally. This worsens readability. I can see the argument for doing this
for a function with ten arguments, but one? Nah.&lt;/p&gt;

&lt;p&gt;I might think of more issues this is it for now.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.</summary></entry><entry><title type="html">Why the Temperature Matters for Contrastive Loss</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html" rel="alternate" type="text/html" title="Why the Temperature Matters for Contrastive Loss" /><published>2021-04-30T10:06:02+00:00</published><updated>2021-04-30T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html">&lt;p&gt;Contrastive learning has become very popular recently, see &lt;a href=&quot;https://github.com/HobbitLong/PyContrast/blob/master/AWESOME_CONTRASTIVE_LEARNING.md&quot;&gt;here&lt;/a&gt; for a good overview of recent papers.&lt;/p&gt;

&lt;p&gt;However, one thing which they all use but is not well motivated is the use of a temperature parameter in the softmax that the contrastive loss uses. I want to share why I think it matters.&lt;/p&gt;

&lt;p&gt;As a reminder this is what the equation looks like:&lt;/p&gt;

\[L=-\log \frac{e^{x_a \cdot x_p \over \tau }}{\sum_i e^{x_a \cdot x_i \over \tau}}\]

&lt;p&gt;Where the numerator contains the comparison between the positive pair \(x_a\) and \(x_p\), and the denominator has all the comparisons (all negative pairs except one). Minimizing the loss means maximizing the value in the numerator (maximizing \(x_a \cdot x_p\)) and minimizing the denominator (minimizing all \(x_a \cdot x_i\)).&lt;/p&gt;

&lt;p&gt;Now imagine \(\tau=1\). Remember the similarity function is the cosine distance, which with normalized vectors goes from -1 to 1. After applying \(exp\) the range goes from 1/e to e.&lt;/p&gt;

&lt;p&gt;Note how small that range is, and note that if the vectors were orthogonal to each other then the similarity is 0 and therefore \(e^{0}=1\).&lt;/p&gt;

&lt;p&gt;So making negative pairs orthogonal to each other is not enough to minimize the loss, because the numerator can at most be \(e\) while the denominator would be (if all pairs were orthogonal) \(\sum_i 1\). What the model would have to try and do is make the negative pairs all be antiparallel to each other. That’s obviously not ideal, if two vectors are orthogonal that should be enough of a separation.&lt;/p&gt;

&lt;p&gt;Now set the \(\tau=0.1\). Now the range goes from \(e^{-0.1}=4e-5\) to \(e^{10}=22000\) ! Incase of orthogonality the similarity after exp is still 1 of course. But now making the vectors of a negative pair orthogonal (rather than close to parallel) will result in a much larger decrease in loss, as the numerator value can be so much larger (i.e. \(22000 &amp;gt;&amp;gt; \sum_i 1\)).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Contrastive learning has become very popular recently, see here for a good overview of recent papers.</summary></entry><entry><title type="html">Changing My Mind On E2E ASR</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html" rel="alternate" type="text/html" title="Changing My Mind On E2E ASR" /><published>2021-04-12T10:06:02+00:00</published><updated>2021-04-12T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html">&lt;p&gt;I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).&lt;/p&gt;

&lt;p&gt;First let’s clarify what E2E ASR means for me: No phones, the model outputs are letters or subword units. No alignment needed and the model learns an internal LM.&lt;/p&gt;

&lt;p&gt;Particularly not using phones bothered me knowing how inconsistent pronunciations of English words are, it just seemed suboptimal to force the network to memorize that.&lt;/p&gt;

&lt;p&gt;However, I’ve had a bit of a change in thinking recently. This has come not from realizing the existance of some technical fact, but rather thinking about what the point of doing speech recognition actually is from the perspective of a consumer. There are many different use cases of course but in by far the majority of them what the end-user wants is a clean transcript. It should be easy to read, free of disfluences, filler words and repetitions.&lt;br /&gt;
I believe traditional ASR is flawed for achieving this, and E2E ASR is not.&lt;/p&gt;

&lt;p&gt;Traditional ASR is split up into at least two components, the AM and LM. The AM is tasked with modeling the acoustics by recognizing phones. Training it means tuning the model so that it makes a decision to which phone each frame (25ms of audio) belongs to. But outputting clean transcripts means ignoring parts of the audio. So this design decision of having one model which classifies phones makes it hard for the model to learn to output clean transcripts, since the better the AM gets at modeling phones the harder it is to ignore certain sounds in the input audio.&lt;/p&gt;

&lt;p&gt;Of course, people with some ASR experience will know that in reality traditional ASR does not have such a problem with outputting clean transcripts. The LM alone should make sure only reasonable word sequences are output. And in practice the AM learns from the training data which (often) has no disfluencies, so the AM learns to map those sounds, which really correspond to some phone, to silence.&lt;/p&gt;

&lt;p&gt;Another issue is that people will regularly pronounce words differently than what you would expect, so during training the model will have to learn to map from one phone to another simply because the lexicon entry for a word will not always correspond exactly how people actually pronounce it.&lt;/p&gt;

&lt;p&gt;So if our AM is not actually modeling sounds, and we’re already forcing it do some more complicated memorization, well then why not just make it model letters?&lt;/p&gt;

&lt;p&gt;While fillers like “um” are never wanted, it’s pretty common to have “you know” missing in a transcript and those words are definitely something the model should not learn to ignore. How is a traditional ASR system supposed to deal with these sorts of errors? &lt;br /&gt;
It cannot. But a transformer-based E2E ASR system can. Thanks to the fact that it looks at much more context (basically seeing the entire input), and that it models letters/subwords directly, it can (for example) learn that a certain sound sequence said quickly at the end of certain sentences (“I really like him yaknow”) can be ignored. This is much harder for a traditional ASR system to learn because the AM does not have so much context it can look at, and even if it did the internal representations would have little to do with words - so it doesn’t know what sentence is being said - as its task is discriminating phones.&lt;/p&gt;

&lt;p&gt;So there’s two points I’m making here: (1) With the training data we have we force the AM to learn an ill-defined task (the task is actually more complicated than just learning to classify phones). (2) E2E systems that use lots of context are better suited to outputting clean transcripts because they combine the AM and LM, so they can learn to ignore sounds because of words that were said before and/or after.&lt;/p&gt;

&lt;p&gt;Clean transcripts aren’t just better for a human reader, it also makes post processing easier for any downstream ML system. And in my experience speech recognition by itself does not have that much value (from a commercial perspective), it’s by adding an NLU system on top that a lot more possibilities for use cases open up.&lt;/p&gt;

&lt;p&gt;I feel like some people have spent so much time thinking about how to model phones that they’ve forgotten that we don’t actually care about phones at all. It could be an interesting question for linguists to study, what phones do people use etc., but if your task is speech recognition classifying which sounds were in the audio should only be a means to an end. A more general conclusion is: &lt;strong&gt;Speech recognition is actually not about what a person said, rather it’s about what they meant to say.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Of course, it’s not very satisfying to just cross your fingers and train a NN to do ASR. It would be nice to somehow give the model a good prior. But with wav2vec2 I feel a good solution has been found as the performance is very good! This combined with the above perspective has changed my mind on E2E-ASR: I believe it is the way forward.&lt;/p&gt;

&lt;p&gt;edit: In hindsight one thing I want to clarify, although I keep mentioning transformer models those are not really required. What really matters is just that the model is bidirectional, therefore can see into both the past and future, and of course that the outputs are character based and the model learns an internal LM.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).</summary></entry><entry><title type="html">Why you need a billion words to get a good language model</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" rel="alternate" type="text/html" title="Why you need a billion words to get a good language model" /><published>2021-04-01T10:06:02+00:00</published><updated>2021-04-01T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html">&lt;p&gt;I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;199989 krisenbewältigungen 2
199999 gendersensitiv 2
200002 umgehbar 2
200005 widersinnigen 2
200016 ausmehrungen 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It is kind of expected for this to happen, since it is well known that word counts follow zipf’s law, which put simply states that as you go down a table of words sorted by count, their counts decrease very rapidly, meaning a very large amount of the probability mass is covered by the top words. Here is an image (forgive the lack of axis labels please, y-axis is count in millions x-axis rank of word):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/words.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at it! (yes I know log scale bla bla, shush pedants) The counts drop extremely quickly to almost nothing.&lt;/p&gt;

&lt;p&gt;So the main point is that you need to have seen a word a couple times to know how it is used. But because in language there is a looong tail of words that are used infrequently (but are still normal enough that you do want to estimate them!) you need a &lt;strong&gt;lot&lt;/strong&gt; of text to get the counts to a reasonable level.&lt;/p&gt;

&lt;p&gt;The additional thing to realise is that because the counts are so low there will be a &lt;strong&gt;ton&lt;/strong&gt; of noise in the results. Depending on the corpus some words whose “true” probability is higher will be much lower and vice versa. This is bad.&lt;/p&gt;

&lt;p&gt;This is why you need to train on billion word corpuses to get good results. Then all those 2s will turn into 20s, and then the model can learn something.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The total count is around 400k by the way (a good chunk of them are rubbish).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position. 199989 krisenbewältigungen 2 199999 gendersensitiv 2 200002 umgehbar 2 200005 widersinnigen 2 200016 ausmehrungen 2 The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!1</summary></entry><entry><title type="html">Why does BPE work?</title><link href="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html" rel="alternate" type="text/html" title="Why does BPE work?" /><published>2021-02-09T10:06:02+00:00</published><updated>2021-02-09T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html">&lt;p&gt;BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?&lt;/p&gt;

&lt;p&gt;Let’s create a simple ‘corpus’* (word counts are what actually matter):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;low 5
lowest 2
newer 6
wider 3
new 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Including the separator (_, appears implicitly at the end of a word) there are 11 characters. Let’s pick the 20 most common ngrams, they turn out to be (in order, but starting with unigrams):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'r_', 'we', 'ne', 'new', 'ew', 'lo', 'low']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is what BPE outputs (single character tokens are not ordered):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'ne', 'new', 'lo', 'low', 'newer_', 'low_', 'wi']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first not-unigram is the same for both (“er”), the second as well, but then the methods diverge. Note how the ngram counter picks up “we” while BPE never does. Why is that? Normally “we” would of course be a reasonable token, but for this corpus it makes more sense intuitively, considering the counts, to keep it seperate so that low + est and new + er are valid tokenisations.&lt;/p&gt;

&lt;p&gt;Because BPE redoes the count each time two tokens are merged (from which a new one is created), and only the merged token is counted - instead of also the previous two - the counts of some tokens can go down after a recount. So when “er” is created the count of “we” drops from 8 to 2! The recounting makes sense in hindsight, the counts of a token should be based on when that token could actually be used, and after choosing the “er” token, the token “we” is not possible as often as before.&lt;/p&gt;

&lt;p&gt;What if we change the k most frequent method to iteratively merge with recounts inbetween, and when recounting if two tokens were merged then we only count the merged token (not the previously split tokens)?&lt;/p&gt;

&lt;p&gt;We would have to start with some set of tokens, those could be the unigrams. Then we count ngrams that are not already in our token set, merge the most frequent, repeat. This is basically BPE, except we’re still doing a bunch of wasted computation by counting ngrams: It should be, with a little thinking, obvious that a bigram will always be at least as likely as a trigram. So rather than count all ngrams that are not already in the token set, we just count bigrams of tokens AKA pairs of tokens. This is BPE.&lt;/p&gt;

&lt;p&gt;Now the motivation for the algorithm of BPE is clear. It recounts after each merge to make sure the statistics used are up-to-date, and by considering pairs of tokens it saves a lot of computation.&lt;/p&gt;

&lt;p&gt;* Taken from Jurafsky’s chapter on BPE.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?</summary></entry><entry><title type="html">On WER in ASR</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html" rel="alternate" type="text/html" title="On WER in ASR" /><published>2020-11-27T10:06:02+00:00</published><updated>2020-11-27T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html">&lt;p&gt;This post will be about the python-based &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;tool (“texterrors”)&lt;/a&gt; I created for getting error metrics (relevant for ASR). It is split in two parts: 
First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking &lt;a href=&quot;#newtool&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;wer-calculation-recap&quot;&gt;WER calculation recap&lt;/h2&gt;

&lt;p&gt;Given a hypothesized sentence the Word-Error-Rate is defined as the number of insertion, deletion and substitution errors with the respect to a reference sentence, divided by the count of words in the reference. Insertion/Deletion is defined from the perspective of the model, so for example if the model outputs a word when it shouldn’t, that’s an insertion error.&lt;/p&gt;

&lt;p&gt;To find out the types of errors one has to align the hypothesis to the reference, this is typically done by creating a cost matrix (where the cost of a cell depends on the transition cost plus the lowest cost from the left, top or diagonal cells) and backtracing from the end (bottom right) to the start (top left) to find the alignment. Example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;third&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;second&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;third&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Taking a horizontal or vertical (deletion/insertion) transition costs 1. Taking a diagonal to position &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; costs 1 if word i != word j, else 0. “second” is not recognized by the model (a deletion error) which is why at the end the cost in the bottom right is 1. Notice that if all we want to know is the WER, you can actually just take that value (1) and divide by the count of reference words (3) to get the WER (33.3%).&lt;/p&gt;

&lt;p&gt;However, usually one wants to know how many errors of each type there are, and to do that one needs to get the alignment to then count them. This requires backtracing which is done by finding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transition_cost&lt;/code&gt; = &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; is either to the left, diagonal or above the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt; (then repeat the process until the start is reached).&lt;br /&gt;
This can be ambiguous, for example consider two sentences “first word in sentence” and “first ward sentence”. There are different ways to align this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Clearly the pair “word”/”ward” is a more likely substitution error than “in”/”ward”, but this alignment method has no way of identifying that since the cumulative costs are the same, see cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This has no impact on the WER, as in both cases there are two (one insertion/deletion depending on which sentence is considered the reference, one substitution), but from the perspective of analysing what sorts of errors a model is making - which sorts of words the model is failing to recognize (deletions), which words are confused with each other (substitutions) - having a different alignment will change the result.&lt;/p&gt;

&lt;h2 id=&quot;getting-better-alignments-with-texterrors-&quot;&gt;Getting better alignments with “texterrors” &lt;a name=&quot;newtool&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;As just mentioned, the traditional method of alignment (which we need to do to get statistics for the different error types) leads to ambiguous alignments with no sensible way of resolving them. &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;“texterrors”&lt;/a&gt; is meant to be a tool for getting detailed error metrics. As these are sensitive to suboptimal alignments it uses a smarter method: Instead of having a cost of 1 for the substitution cost (in the cost matrix), it incorporates the character edit distance between the words compared.&lt;/p&gt;

&lt;p&gt;Concretely, the substitution cost is set to the edit distance between two words divided by the maximum edit distance possible (length of the longer word), so it is a value between 0 and 1 (slightly more complicated in practice, you’ll see later why). That way alignments will be favoured when words which are similar to each other are substitution errors instead of deletion/insertion errors.&lt;/p&gt;

&lt;p&gt;Example cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.25&lt;/td&gt;
      &lt;td&gt;1.125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As one can see here, the best (lowest) cumulative cost is achieved by pairing “word”/”ward”.&lt;/p&gt;

&lt;p&gt;One should be aware, this method can result in a higher WER. 
In the below example a normal WER calculation would do a one-to-one mapping and arrive at a WER of 66.67\%.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But character aware alignment would result in the following alignment:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;-&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This results in a WER of 83.3\% because of the extra insertion and deletion. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; has an option to turn character-aware alignment off (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-no-chardiff&lt;/code&gt;) to get identical results with kaldi. But the difference is small for a normal sized test set, and obviously without the feature the alignments and therefore statistics like the most frequent substitution error will not be as accurate!&lt;/p&gt;

&lt;p&gt;There is still one last issue to deal with, see this example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The alignment will end up being&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This happens here because using the character edit distance leads to the substitution cost often being smaller than the insertion/deletion cost and therefore alignments with more substitutions are favored.&lt;br /&gt;
This sort of bad alignment can also happen with normal costs of 1/1/1 for ins/del/sub (consider the above example, as the costs are the same for different errors it depends on the implementation which alignment is chosen, you can think of it as random). That’s why such tools, when meant to be used for getting detailed error metrics, will increase the substitution cost to improve alignments. We can do the same: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; will after the previously mentioned calculation times the cost by 1.5. This will lead to the following cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And the following (obviously superior) alignment.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;-&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; also supports ctm files where the time stamps for reference and hypothesis words will be used for alignment. If you &lt;em&gt;really&lt;/em&gt; care about having the most accurate alignment possible, that is what you should use! But it won’t make a big difference. :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;Here is a link to it.&lt;/a&gt; Thank you for reading.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This post will be about the python-based tool (“texterrors”) I created for getting error metrics (relevant for ASR). It is split in two parts: First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking here.</summary></entry><entry><title type="html">Doing non-standard stuff with kaldi decoding</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html" rel="alternate" type="text/html" title="Doing non-standard stuff with kaldi decoding" /><published>2020-11-06T10:06:02+00:00</published><updated>2020-11-06T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html">&lt;p&gt;Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found &lt;a href=&quot;https://github.com/RuABraun/fst-util&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;adding-words-to-hclg&quot;&gt;Adding words to HCLG&lt;/h2&gt;

&lt;p&gt;Some scripts you will need can be found &lt;a href=&quot;https://github.com/idiap/icassp-oov-recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This method requires you to use a monophone model. Additionally, your language model needs to have been trained with pocolm, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--limit-unk-history&lt;/code&gt; option, and there should have been some OOVs in the training text.&lt;/p&gt;

&lt;p&gt;For simplicity, the modification is done on a graph without self-loops. So you need to modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/mkgraph.sh&lt;/code&gt; and comment L167: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm $dir/HCLGa.fst $dir/Ha.fst 2&amp;gt;/dev/null || true&lt;/code&gt; because we will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Inside the graph dir where the HCLG is there is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt;. You need to assign IDs to the new words you’re adding and append these to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt; file (these should be larger than the existing ones obviously).&lt;/p&gt;

&lt;p&gt;Assuming all this is ready you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;script/compose_hcl.sh&lt;/code&gt; to create the HCL from a lexicon of the OOV words you want to add. Check the script for the input arguments, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final.mdl&lt;/code&gt;, isym is phones osym words. Notice it uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_lfst.py&lt;/code&gt; so you need the fst wrapper installed. There is one hardcoded parameter on L25, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;303&lt;/code&gt;, see &lt;a href=&quot;https://groups.google.com/g/kaldi-help/c/jL8VnwKGRWs/m/-Pe29-G9AgAJ&quot;&gt;here&lt;/a&gt; for what’s about. You can set it to any number larger than the existing phone IDs.&lt;/p&gt;

&lt;p&gt;After calling the script and creating the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCL.fst&lt;/code&gt; you use the fst wrapper to modify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from wrappedfst import WrappedFst
fst = WrappedFst('HCLGa.fst')
ifst = WrappedFst('HCL.fst')
unk_id =  # unk symbol
fst.replace_single(unk_id, ifst)
fst.write('HCLGa_new.fst')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then add the self-loops (check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkgraph.sh&lt;/code&gt; for how to do that) and you are done. Replace an existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLG.fst&lt;/code&gt; with the new version and you can run decoding as you would normally.&lt;/p&gt;

&lt;h2 id=&quot;no-hclg-just-g&quot;&gt;No HCLG just G&lt;/h2&gt;

&lt;p&gt;Imagine you have an acoustic model that you created via pytorch, and you want to then evaluate how well it does at recognizing phonemes (TIMIT for example). You have a phone LM that you want to incorporate. I’m going to show you how you can do that with kaldi.&lt;/p&gt;

&lt;p&gt;Usually kaldi assumes that you have an HMM graph (the H in HCLG) and possibly context dependency as well (the C), then there’s also the lexicon etc. If you had a pytorch AM that you had trained on the PDF-ids (the targets you get by calling ali-to-pdf on the alignments), you would then use that by saving the loglikelihood output of your NN to a file, then passing that file and the final.mdl, HCLG.fst to latgen-faster-mapped, which will do decoding and generate a lattice (from which you could then take the best path for example).&lt;/p&gt;

&lt;p&gt;But if you want to just model phones, you don’t care about the HCL at all. You really just want to use a G that will act as the phone LM.&lt;/p&gt;

&lt;p&gt;So the first issue to get around is to avoid doing any mapping from transition IDs (the typical input symbols of the HCLG) to PDF-IDs. We can do that by creating a file latgen-faster.cc with the only difference being we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaled&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaledMapped&lt;/code&gt; in the code.&lt;/p&gt;

&lt;p&gt;Then you take the arpa LM that you have trained, convert it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arpa2fst&lt;/code&gt;, convert the #0 arcs to &lt;eps&gt; and add self-loops to states that have incoming arcs with input labels not equal 0. The point of that is when the AM predicts a sequence &quot;ae ae ae b b&quot; over five frames, the LM should not care about repetitions, we want it to only score the transition &quot;ae&quot; to &quot;b&quot;, and of course we don't want repetitions in the output transcript. Having self-loops fixes that.&lt;/eps&gt;&lt;/p&gt;

&lt;p&gt;Then you call&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;latgen-faster --acoustic-scale=1.0 --beam=13 --determinize-lattice=false G.fst ark,t:loglikelihoods-file&amp;gt; ark:- | lattice-best-path etc.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to get a hypothesis that you can compare to the reference to get the PER!&lt;/p&gt;

&lt;p&gt;If you wanted to recognize words, all you’d need to do is create a L.fst, compose that with the G.fst (which is then a word LM) and use the LG.fst.&lt;/p&gt;

&lt;h2 id=&quot;g-with-keyword-list&quot;&gt;G with keyword list&lt;/h2&gt;

&lt;p&gt;Imagine you just want to recognize a small number of words and you don’t want to have any sort of prior regarding which one is more likely.&lt;/p&gt;

&lt;p&gt;To do that you just have to create an FST with a single state, and self-loops where each self-loop has a keyword as the input and output symbol.&lt;/p&gt;

&lt;p&gt;In pseudo-code&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fst = Fst()
state = fst.add_state()
fst.set_start(state)
fst.set_final(state)
for keyword in keywords
    fst.add_arc(state, state, keyword, keyword, cost)
fst.write(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The self-loops should have costs, what these should be you will have to find out experimentally.&lt;/p&gt;

&lt;p&gt;You probably will have to create the lang/ folder again. To do that just call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_lang.sh&lt;/code&gt; with the dict/ of keywords and with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--phone-symbol-table&lt;/code&gt; option set to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phones.txt&lt;/code&gt; that you trained the AM model with.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found here.</summary></entry><entry><title type="html">First post: Ark and scp files in kaldi</title><link href="https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi.html" rel="alternate" type="text/html" title="First post: Ark and scp files in kaldi" /><published>2020-10-04T12:40:02+00:00</published><updated>2020-10-04T12:40:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi.html">&lt;p&gt;Both file types are structured by having keys and a value for each key.&lt;/p&gt;

&lt;h1 id=&quot;scp-files&quot;&gt;Scp files&lt;/h1&gt;

&lt;p&gt;In scp files, usually ending with the “.scp” suffix, the first column (aka field) is the key (usually an utterance ID). The rest of the line is treated as a pointer to data (the pointer is the value). 
What this means is that the scp file never contains any data, it just contains something which points to the data you will eventually use. See for example the wav.scp file which will look something like this usually:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wav-id-1 /path/to/file-1.wav
wav-id-2 /path/to/file-2.wav
wav-id-3 /path/to/file-3.wav
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be a file path, but can also be more complex like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wav-id-1 sox /path/to/file-1.wav -r 8k - |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Kaldi’s code is written so that it will recognize and execute the above command (starting from after the key), which results in the wav file being read with a sampling rate of 8kHz. This is a convenient way of doing the resampling on the fly (instead of resampling and having to waste space storing a file somewhere).&lt;/p&gt;

&lt;p&gt;Another file one will see a lot when using kaldi is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats.scp&lt;/code&gt; file which looks like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;utt-id-1 /path/to/file-1.ark:44
utt-id-2 /path/to/file-1.ark:760
utt-id-3 /path/to/file-1.ark:1520
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The number after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt; is a byte offset.&lt;/p&gt;

&lt;h1 id=&quot;ark-files&quot;&gt;Ark files&lt;/h1&gt;

&lt;p&gt;Instead of the value being a pointer to the data, in ark files the value &lt;strong&gt;is&lt;/strong&gt; the data. Usually this means it is in binary format, so you can’t visually inspect it, however kaldi has binaries for converting from binary to text format. So for example take the ark file from the feats.scp example above and call:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats ark:/path/to/file-1.ark ark,t:/path/to/file-1.ark.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark,t&lt;/code&gt; means that the values in the output will be written in textual format, so you can open the file and see what the values for the (example) MFCC coefficients are for each frame.&lt;/p&gt;

&lt;p&gt;Another example of an ark style file, although it doesn’t have the “.ark” suffix, is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; file you will find in kaldi data folders. The first field/column is the utterance ID, the rest of the line is the words belonging to that utterance. The words are the data.&lt;/p&gt;

&lt;p&gt;You can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; to concatenate ark files. And kaldi binaries accepts commands and wildcards in the arguments, so this is valid: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark:gunzip -c lat.*.gz|&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Kaldi has a script for subsetting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; files you may find useful called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/subset_scp.pl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When you call kaldi binaries you often have to prefix with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark:&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp:&lt;/code&gt; to tell kaldi what type of file to expect. You can then mix types, for example I could use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/subset_scp.pl&lt;/code&gt; to subset &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats.scp&lt;/code&gt; and create a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats_subset.scp&lt;/code&gt;, and then copy a subset of the feature data by calling&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats scp:feats_subset.scp ark:copied_feats_subset.ark
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how the input argument is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt;, so kaldi knows the input file is pointing to the data I want to use, and the output is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt;, so kaldi knows to actually write the data pointed to to a new file.&lt;/p&gt;

&lt;p&gt;It’s not possible to create just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; file from an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt; file, you have to create both the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; files. So you have to do something like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats scp:/path/to/feats.ark ark,scp:copied_feats.ark,copied_feats.scp 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details check out the official documentation of &lt;a href=&quot;http://kaldi-asr.org/doc/io.html&quot;&gt;kaldi io&lt;/a&gt;. Probably the section “from a command-line perspective” is more relevant to you.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Both file types are structured by having keys and a value for each key.</summary></entry></feed>