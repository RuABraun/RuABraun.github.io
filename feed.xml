<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://ruabraun.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ruabraun.github.io/" rel="alternate" type="text/html" /><updated>2024-05-04T20:24:10+00:00</updated><id>https://ruabraun.github.io/feed.xml</id><title type="html">Sharings</title><author><name>Rudolf A. Braun</name></author><entry><title type="html">Favourites of 2023</title><link href="https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023.html" rel="alternate" type="text/html" title="Favourites of 2023" /><published>2024-02-26T10:06:02+00:00</published><updated>2024-02-26T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023.html">&lt;p&gt;This is a list of stuff I read or saw in 2023 and really liked. I’m going to say a sentence or more about each and maybe include a quote. I’m hoping somebody who stumbles across this finds something they really like that they otherwise wouldn’t have, or someone who knows me might find an interest in common we didn’t know we had!&lt;/p&gt;

&lt;p&gt;I suggest skimming and using the bolded bylines to pick out what might interest.&lt;/p&gt;
&lt;h1 id=&quot;books&quot;&gt;Books&lt;/h1&gt;

&lt;p&gt;This was a (first?) year of mostly non-fiction for me. It was great! Do want to read more fiction this year though.&lt;/p&gt;

&lt;h2 id=&quot;storyworthy&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/37786022&quot;&gt;Storyworthy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;How to tell a good story.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your story must reflect change over time. A story cannot simply be a series of remarkable events. You must start out as one version of yourself and end as something new.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I loved the practicality of this. He takes his own stories and breaks them down. He goes into detail and provides tons of examples. 
It made me understand that I usually do not do a good job of telling a story and I appreciated having that realization.&lt;/p&gt;

&lt;p&gt;It also made me think there is an opposition between telling an engaging story and explaining something (like in a paper or technical report) or communicating facts efficiently. With the latter you don’t want to surprise a listener, everything should be obvious because of what you said previously, if that’s not the case then you missed a step in the logic or made an error which is why the listener is surprised.
But for storytelling surprise is a key tool to engage the listener, so depending on what you’re talking about you really should have a different structure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=U9v0O0oEmpQ&quot;&gt;He’s got videos of his stories on youtube, check it out!&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;path-to-power&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/86524&quot;&gt;Path to Power&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Lyndon B Johnson’s early life and the first roughly ten years of his political career.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In every election in which he ran—not only in college, but thereafter—he displayed a willingness to do whatever was necessary to win: a willingness so complete that even in the generous terms of political morality, it amounted to amorality.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is hands-down one of the most engaging books I’ve ever read that also goes into incredible detail on things that happened a century ago and one might initially think uninteresting. But the narrative works. It is interesting. It helps of course that LBJ is a singular character. His portrayal is at minimum unsympathetic, yet you can’t help but root for him when he goes to the greatest lengths to help his constituents and when he tries to gain more power (with the hindsight knowledge of him enacting so many liberal policies later). I wonder if someone wrote a similarly detailed biography of say, Obama, would it be close to as interesting?&lt;/p&gt;

&lt;h2 id=&quot;how-to-be-the-greatest-improviser-on-earth&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/30364117-how-to-be-the-greatest-improviser-on-earth&quot;&gt;How To Be The Greatest Improviser On Earth&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Everything you need to know to do improv.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another way to think about the top of the scene is with the Taoist principles of yin and yang. Yin is passive, patient, empathetic, malleable. It is akin to a “yes.” Yang is active, assertive, decisive, altering. It is akin to an “and.” After a series of small moves that simply confirm information without adding a lot, it’s time for a “yang” move: make a decision, add some information. But once someone makes a big “yang” move, then it’s time for a simple “yin” response. Confirm and unpack what was just added. Even a beat of silence is a good idea.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I believe the improv mindset is beneficial for more than just improv. So while a niche book I’m including it here as it really is excellent. This book gives you everything you need to know for improv, literally, and yet it’s so compact! I really felt after reading it I just needed to go practice more, what I should be doing was made clear.&lt;/p&gt;

&lt;h2 id=&quot;articles&quot;&gt;Articles&lt;/h2&gt;

&lt;p&gt;Only started saving these towards the end of the year so the list is short.&lt;/p&gt;
&lt;h2 id=&quot;lessons-from-the-years-of-lyndon-johnson-by-robert-caro&quot;&gt;&lt;a href=&quot;https://www.dwarkeshpatel.com/p/lyndon-johnson&quot;&gt;Lessons from The Years of Lyndon Johnson by Robert Caro&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A collections of thoughts and lessons from reading Robert Caro’s series on LBJ by Dwarkesh Patel.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the first 55 years of his life, he convinced men of tremendous intellect and drive of their own that he was the scion of the Southern cause - that the only hope for that cause was to make Lyndon Johnson President. So much so that when Lyndon Johnson was Majority Leader, the Southern Senators, who at the time controlled all the important committees in Congress, allowed Johnson to pass liberal legislation they scorned because they believed it would help him become President. And, they believed, as President, Johnson would govern as a conservative, especially on the issue of race. The tragic irony and betrayal here is hard to over-emphasize. These powerful conservative men - who, while being ignorant and backwards in certain ways, were famously shrewd and intelligent - conceded on issue after issue, and campaigned with their immense political influence, for a man who ended up being the biggest liberal President since FDR.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Great_man_theory&quot;&gt;great man theory&lt;/a&gt; real? The strongest argument pro I’ve ever read is in this article. Lots of other interesting takeaways too.&lt;/p&gt;

&lt;p&gt;Also loved this story from a footnote:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I remember I was visiting India as a teenager. My uncle was talking about his son, who was studying business in the US, in the hopes that he would succeed my uncle as head of the family import/export conglomerate. My dad said something to the effect of, “You must be proud that your son is studying in the US.”
“No,” my uncle responded. “In fact, I’m a little worried.” In America, he explained, a student learns that if you can’t agree on a price with a counterparty, or if a regulator says that you’re out of line, then you sigh, say, “Ah, too bad”, and move on. But one cannot conduct business this way in India. In India, if an apparatchik of the License Raj tells you that you can’t do something, then you make a personal appeal to the man. And if that fails, you offer him cash under-the-table. If that fails, you make such appeals to his boss. And if that fails, you find some way to avoid their authority altogether. You never just accept a no at face value. You couldn’t get anything done in India if you did. Reading about how Lyndon Johnson got what he wanted - how he flattered, threatened, manipulated, and lied to get bills passed - reminded me of this story.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;if-only-i-could-begin-again&quot;&gt;&lt;a href=&quot;https://harpers.org/archive/2020/12/storm-jameson-if-only-i-could-begin-again/&quot;&gt;If only I could begin again&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A review of the autobiography of an only moderately successful early 20th century English writer.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The speaking voice, throughout the entire performance, has been described in reviews and biographical essays as ruthlessly honest; by which it is meant not that Jameson delivers a tell-all confessional, but rather that the reader can sense her grappling with something recalcitrant in the material that nonetheless draws her on. She realizes, just as we do, that there is much she does not know about this something. But we have no doubt that she is intent on telling us—&lt;em&gt;really&lt;/em&gt; telling us—as much as she does know, and that intent is what counts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What do you do when your work does not meet the standards you aspire to?
This article reviews the autobiography of Storm Jameson, an early 20th century English writer, who wrote dozens of books to little acclaim but found her metier in her final piece of work.
It might seem strange to highlight a book review as a standalone piece, but the subjects covered through talking about her life - not being as good as one wants to be at something, escaping the shadow of one’s parents, asking oneself the hard questions while also being forgiving and understanding of oneself and others - are relevant to many and thought-provoking to me.&lt;/p&gt;
&lt;h2 id=&quot;scott-and-scurvy&quot;&gt;&lt;a href=&quot;https://idlewords.com/2010/03/scott_and_scurvy.htm&quot;&gt;Scott and Scurvy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;How the cure for scurvy was lost and the impact it had on the British attempt to reach the south pole.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Steam power had shortened travel times considerably from the age of sail, so that it was rare for sailors other than whalers to be months at sea without fresh food. Citrus juice was a legal requirement on all British vessels by 1867, but in practical terms it was becoming superfluous. 
So when the Admiralty began to replace lemon juice with an ineffective substitute in 1860, it took a long time for anyone to notice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One generally imagines science as always moving forward but as this account shows that is not actually guaranteed.
In our digital age it’s hard to imagine information being lost but it’s worth remembering that digital devices are actually the least permanent of all (hard drives don’t even last ten years) and simultaneously very complicated to create and interact with, if something cataclysmic did happen it’s basically guaranteed we would lose most of our science knowledge. I guess in the event we’d have other worries anyways.&lt;/p&gt;
&lt;h2 id=&quot;the-humanities-have-sown-the-seeds-of-their-own-destruction&quot;&gt;&lt;a href=&quot;https://web.archive.org/web/20240114171009/https://www.theatlantic.com/ideas/archive/2023/12/humanities-university-conservative-critics/676890/&quot;&gt;The Humanities Have Sown the Seeds of Their Own Destruction&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A humanities professor laments how the increasing politicization of the humanities has caused it to change for the worse.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Instead of trying to prove that the humanities are more &lt;em&gt;economically&lt;/em&gt; useful than other majors—a tricky proposition—humanists have taken to justifying their continued existence within the academy by insisting that they are uniquely &lt;em&gt;socially&lt;/em&gt; and &lt;em&gt;politically&lt;/em&gt; useful. The emergent sales pitch is not that the humanities produce and transmit important &lt;em&gt;knowledge&lt;/em&gt;, but rather that studying the humanities promotes nebulous but nice-sounding values, such as empathy and critical thinking, that are allegedly vital to the cause of moral uplift in a multicultural democracy.
If the arc of the universe bends toward justice, some would have you believe that it is humanities departments that do the bending.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It’s interesting to view the increasing politicization as a reaction to the question what is studying the humanities good for.
Unfortunately I don’t see how one could reverse the trend. Once institutions are captured by ideologues that fill all the positions with their friends you can’t really go back unless you fire everyone which doesn’t seem realistic.
Update May 4th: Think the recent protests further validate the viewpoint of the article writer.&lt;/p&gt;

&lt;h1 id=&quot;movies-and-series&quot;&gt;Movies and Series&lt;/h1&gt;

&lt;p&gt;There were a lot of great movies in 2023 but one connected with me the most:&lt;/p&gt;

&lt;h2 id=&quot;past-lives&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt13238346&quot;&gt;Past Lives&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;As someone who moved between multiple countries as a child this cut deep. I get such a sense of melancholy from the stairs shot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/pastlives_stairs.jpg&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;:/ &lt;/p&gt;

&lt;h2 id=&quot;succession&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt7660850&quot;&gt;Succession&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;What is it that makes Succession such a masterpiece to some and only mildly interesting (“why would I watch a show where everyone is unlikeable”) to others ? I think it’s the originality of the dialogue and being able to recognize how unique it is. Of course the story has interesting twists and turns but I don’t think that is what makes the show stand out. It’s the characters and how they talk to each other. I’ve noticed that people interested in writing tend to belong to the group of Succession superfans and I don’t think that’s an accident.&lt;/p&gt;

&lt;h2 id=&quot;barry&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt5348176&quot;&gt;Barry&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I was debating about putting this on here as there was a lot of criticism about the last season or two about it essentially changing from a dark comedy to a dark drama and I think the criticism was warranted. But the uniqueness of Barry makes it still worth elevating in my opinion: The twists, the way comedy is combined with earnestness and the unique characters who also grow over the course of the show (characters growing in a comedy imagine that!).&lt;/p&gt;

&lt;h1 id=&quot;misc&quot;&gt;Misc&lt;/h1&gt;

&lt;p&gt;I loved this.&lt;/p&gt;
&lt;h2 id=&quot;john-wick-is-so-tired-by-kyra-wilder&quot;&gt;&lt;a href=&quot;https://www.theparisreview.org/poetry/7974/john-wick-is-so-tired-kyra-wilder&quot;&gt;John Wick Is So Tired&lt;/a&gt; by &lt;a href=&quot;https://www.theparisreview.org/authors/34161/kyra-wilder&quot;&gt;Kyra Wilder&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;John Wick is so tired, but he can still throw a hatchet and hit a guy dead in the face&lt;br /&gt;
he can just split other people open with anything, with a pencil&lt;br /&gt;
because he knows what it’s like&lt;br /&gt;
because he’s tired and loves dogs and he’s cracked right open too and&lt;br /&gt;
I want to tell you to&lt;br /&gt;
look at his feet when he runs&lt;br /&gt;
the way they turn so delicately in&lt;br /&gt;
the way they’re listing slightly, his black shoes&lt;br /&gt;
the heels of them&lt;br /&gt;
their heartbreaking glissade hush-hushing across the hotel tiles&lt;br /&gt;
just look at the way he’s slipping&lt;br /&gt;
even before he soaks the floor with other people’s blood&lt;br /&gt;
I want to do push-ups like John Wick does in the morning&lt;br /&gt;
so I won’t just be sad but sad and also ripped, like&lt;br /&gt;
sad with muscles that stand out all obvious in desolate relief&lt;br /&gt;
sad where it looks like I eat clean and have expensive taste&lt;br /&gt;
I want to be sad but with a cut six-pack and&lt;br /&gt;
to drink thimblefuls of espresso out of impeccable cups and&lt;br /&gt;
I want to tell you to wait and be here and look&lt;br /&gt;
at me and also at the way John Wick is leaning&lt;br /&gt;
into those people that he’s stabbing&lt;br /&gt;
how he gets so close to them and just holds them for a second&lt;br /&gt;
how he’s so tired but he knows he has to let them go&lt;br /&gt;
and I wish you would be here and&lt;br /&gt;
we could watch John Wick together&lt;br /&gt;
and we could put our ruthless arms around each other and if we looked&lt;br /&gt;
out the window it would be all California&lt;br /&gt;
and I would lean in close and tell you that John Wick kills women like&lt;br /&gt;
he’s read feminist theory&lt;br /&gt;
which is to say I think he’s familiar with the philosophy of care and&lt;br /&gt;
you would laugh and&lt;br /&gt;
wait, look now, John Wick is riding&lt;br /&gt;
that black horse like he knows just what grief is&lt;br /&gt;
like he knows sometimes it’s killing and killing and&lt;br /&gt;
sometimes it’s just slipping in your shoes and&lt;br /&gt;
I want you to be here and&lt;br /&gt;
wait, now the camera’s right on him, just all cool colors and diaphanous mood and&lt;br /&gt;
it looks like his hand hurts like his knuckles are a little swollen but&lt;br /&gt;
he’s not saying it and&lt;br /&gt;
I want to know what you think&lt;br /&gt;
of all that blue light&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This is a list of stuff I read or saw in 2023 and really liked. I’m going to say a sentence or more about each and maybe include a quote. I’m hoping somebody who stumbles across this finds something they really like that they otherwise wouldn’t have, or someone who knows me might find an interest in common we didn’t know we had!</summary></entry><entry><title type="html">How k2 calculates the transducer loss quickly</title><link href="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html" rel="alternate" type="text/html" title="How k2 calculates the transducer loss quickly" /><published>2022-07-15T10:06:02+00:00</published><updated>2022-07-15T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html">&lt;p&gt;The new ASR toolkit k2/icefall gets great results while training models quickly. This is an explanation of how it does that by efficiently calculating the transducer loss and thereby using much less memory. Code is also shown.&lt;/p&gt;

&lt;p&gt;If you’re new to transducers read this first for an excellent introduction: &lt;a href=&quot;https://lorenlugosch.github.io/posts/2020/11/transducer/&quot;&gt;https://lorenlugosch.github.io/posts/2020/11/transducer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Consider the training scenario.  With CTC training your model will output a tensor (B,T,V) with B=’batch size’, T=’time steps’, and V=’vocab size’ (vocab size is the number of tokens your model can output). For each sample in a batch you can imagine a matrix where every traversing path represents an alignment (and CTC means you are summing across all of them). The following image shows an example with time on the x-axis, the tokens that should be output on the y. The path shown represents a particular sequence of token (diagonal) and blank (horizontal) that correspond to one alignment. Note that the probability distribution across V (the vocab) is, as long as you are at the same time-step, the same. In other words if you traverse vertically, the probability distribution across V does not change. It only changes if you traverse horizontally (by changing t).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_align.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;This shows a specific alignment which can be repesented as a path through a matrix. Taken from the preprint linked at the bottom. &lt;/p&gt;

&lt;p&gt;With transducer models this is different. Remember that the point of a transducer model is that you model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2..)&lt;/code&gt; (instead of just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt;), which means the token history matters. This means that you need an extra dimension (for the history) in the output tensor, and therefore you need a shape (B,T,U,V) with U = ‘tokens in reference’.&lt;/p&gt;

&lt;p&gt;The way I imagine it is: Instead of a matrix representing different alignments for a single example, imagine stacked matrices (one behind the other), or a (3D) lattice. The z-axis goes along the stack and corresponds to the V dimension. Time is still on the x-axis, and the tokens that should be output still on y (this is the U dimension). This means that now for every combination of x,y (T,U) you have a different probability distribution across V.&lt;/p&gt;

&lt;p&gt;This is a much larger tensor than the one CTC outputs! If the last paragraph was not clear, the main point is that the output tensor for transducers has shape (B,T,U,V) which is much larger than the output tensor of a CTC  model which has shape (B,T,V).&lt;/p&gt;

&lt;p&gt;Because it’s large, training is slow and uses a lot of memory. k2/icefall has created an approach for avoiding that!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The summary&lt;/strong&gt; is one trains a simpler model to get bounds on what alignments are possible, and then uses those bounds to decrease the size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B,T,U,V)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B,T,S,V)&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&amp;lt;&amp;lt;U&lt;/code&gt; ) and thereby efficiently train a model that models &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2..)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now in more detail.&lt;/p&gt;

&lt;p&gt;Let’s first review the normal method for creating the (B,T,U,V) tensor. This happens by  combining the encoder and decoder outputs, which have shape (B,T,C) and (B,U,C). The combination is done by adding the two (with dimension unsqueezing so the result of the addition is (B,T,U,C)), and then projecting to (B,T,U,V).&lt;/p&gt;

&lt;p&gt;Code:
&lt;img src=&quot;https://ruabraun.github.io/images/k2_joiner.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;The joiner that combines the encoder and decoder outputs. Note the dimension unsqueezing of the encoder and decoder outputs is assumed to have already happened (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/joiner.py&quot;&gt;source code&lt;/a&gt;). The first projection you see just projects to a joiner dimension, `output_linear` projects to vocab size V.&lt;/p&gt;

&lt;p&gt;Let’s just consider the case where B=1 (everything that follows holds true with B&amp;gt;1, this is just to simplify things) and we can work with the shape (T,U,V).&lt;/p&gt;

&lt;p&gt;Remember our end-goal is to calculate the logprob of all alignments by summing across all of them. This requires stepping, from start to end, through each combination of time (T) and token history (U) in the (T,U,V) tensor. The first insight is that for training we don’t need to have a distribution across all tokens in V as we have a training transcript so we know at each position (T,U) the token probability that matters: In the first row on the y-axis (equals U axis, see figure at the start) it is the first token in U, in the second row it is the second token in U and so on. These token probabilities govern the vertical transitions, the horizontal transitions are dependent on the blank token probabilities. These probabilities are what we need (instead of a distribution across all).&lt;/p&gt;

&lt;p&gt;Okay cool, but how do we actually get the logprobs we need for each position in (T,U,) without creating (T,U,V)?&lt;/p&gt;

&lt;p&gt;This is done by initially treating the encoder and decoder as separate models that act as an AM and LM by modeling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|y-1,y-2..)&lt;/code&gt;. First the encoder and decoder outputs are projected to (T,V) and (U,V), then these are matrix-multiplied with V on the inner dimension to get a matrix (T,U) with marginalized values across V.&lt;/p&gt;

&lt;p&gt;This lets us get normalized probabilities for tokens we care about: We can add together the unnormalized token log probabilities from the encoder and decoder, and subtract the marginalized value. In equation form (everything is in logprob here):&lt;/p&gt;

\[\log p(t,u,v)=l_{encoder}(t,v) + l_{decoder}(u,v) - l_{marginalized}(t,u)\]

&lt;p&gt;Because time and token history are independent of each other, we avoid having to create a (T,U,V) matrix.&lt;/p&gt;

&lt;p&gt;Logprobs are used when adding, normal probs when multiplying (the previously mentioned matrix multiply), so the implementation has some exp() and log() calls.&lt;/p&gt;

&lt;p&gt;Using the probabilities we efficiently calculate above, we create a matrix (T,U) containing the token probabilities we care about (those in the reference transcript), and additionally a matrix (T,U) with the blank probabilities (since blank transitions are always possible). We then calculate a &lt;strong&gt;simple&lt;/strong&gt; transducer loss by using both matrices to traverse across (T,U) (we need both since we need non-blank token probs for vertical and blank for horizontal transitions) to find the logprob of all alignments.&lt;/p&gt;

&lt;p&gt;Note you wouldn’t normally want to take this approach because the encoder and decoder outputs don’t get to interact before the token distribution is calculated (they’re added together after the projection to size V), as already mentioned this effectively uses separate AM and LM models (where the AM just sees audio and the LM just text). But the point of a transducer model is that you want an output &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2)&lt;/code&gt;; something that directly conditions on both audio and text!&lt;/p&gt;

&lt;p&gt;The idea here is to use the simple loss so that we can do a pruned version of the normal transducer loss. After some training with the simple loss the model will learn that some alignments (paths in (T,U)) are more or less likely than others.&lt;/p&gt;

&lt;p&gt;This information can be used to set boundaries for each time step in T, which allows doing the proper transducer loss on a subset (T,S,V) with S&amp;lt;&amp;lt;U (because we know that for a given point in time only some tokens are possible, not all in U), which takes much less memory, is faster and trains the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y_1)&lt;/code&gt; output. Effectively this means we are not considering all alignments, just those that are “reasonable” (according to the simple loss).&lt;/p&gt;

&lt;p&gt;Let’s look at the high level code (I collapsed whitespace to make things more compact).&lt;/p&gt;

&lt;p&gt;The following image shows all the steps to computing the pruned transducer loss. You can see the separate projection of encoder and decoder outputs to the vocab size, then computing a simple loss and using that (specifically the gradients) to get boundaries (here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ranges&lt;/code&gt;) for creating (B,T,S,V) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logits&lt;/code&gt;. Finally the normal transducer loss is calculated. (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/model.py#L146&quot;&gt;source code&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_losshighlevel.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking slightly deeper into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.rnnt_loss_smoothed&lt;/code&gt; we can see there are two stages: First calculating the matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;px&lt;/code&gt; (with shape (B,T,U,) of reference tokens) and matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py&lt;/code&gt; (shape (B,T,U,) of blank token), then in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutual_information_recursion&lt;/code&gt; calculating the total logprob across all alignments ( &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1152&quot;&gt;source code&lt;/a&gt; ). Despite the intimidating name the implementation of the latter is quite straightforward (for CPU at least) and just involves doing the standard dynammic programming triple for-loop (for the batch, time, token dimensions), see &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/csrc/torch/mutual_information_cpu.cu#L89&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_smoothloss.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thank you for reading, hope it is helpful, and please send an email / DM if something is unclear! :)&lt;/p&gt;

&lt;p&gt;A preprint is available with nice results and additional details: &lt;a href=&quot;https://arxiv.org/abs/2206.13236&quot;&gt;https://arxiv.org/abs/2206.13236&lt;/a&gt;&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">The new ASR toolkit k2/icefall gets great results while training models quickly. This is an explanation of how it does that by efficiently calculating the transducer loss and thereby using much less memory. Code is also shown.</summary></entry><entry><title type="html">Why I don’t like the black code formatter</title><link href="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html" rel="alternate" type="text/html" title="Why I don’t like the black code formatter" /><published>2021-11-18T10:06:02+00:00</published><updated>2021-11-18T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html">&lt;p&gt;First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.&lt;/p&gt;

&lt;p&gt;However:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;’ is cleaner than “. To me, ‘ has less visual noise than “. It’s a single stroke rather than two. Additionally, on US keyboards ‘ does not require pressing shift, “ does.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Black loves newlines, this leads to too much whitespace and makes skimming code harder&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Take a look at this image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/blackbloat_args.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This could take half the space, and then I could view twice as many arguments in one go.&lt;/p&gt;

&lt;p&gt;Black favoring to break functions with arguments up like this&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function(
    argument
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;generally leads to one seeing much less code on one’s screen than originally. This worsens readability. I can see the argument for doing this
for a function with ten arguments, but one? Nah.&lt;/p&gt;

&lt;p&gt;I might think of more issues this is it for now.&lt;/p&gt;

&lt;p&gt;2024 Update: &lt;a href=&quot;https://x.com/karpathy/status/1783538648685892026&quot;&gt;Andrej Karpathy agrees&lt;/a&gt;!&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.</summary></entry><entry><title type="html">Why the Temperature Matters for Contrastive Loss</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html" rel="alternate" type="text/html" title="Why the Temperature Matters for Contrastive Loss" /><published>2021-04-30T10:06:02+00:00</published><updated>2021-04-30T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html">&lt;p&gt;Contrastive learning has become very popular recently, see &lt;a href=&quot;https://github.com/HobbitLong/PyContrast/blob/master/AWESOME_CONTRASTIVE_LEARNING.md&quot;&gt;here&lt;/a&gt; for a good overview of recent papers.&lt;/p&gt;

&lt;p&gt;However, one thing which they all use but is not well motivated is the use of a temperature parameter in the softmax that the contrastive loss uses. I want to share why I think it matters.&lt;/p&gt;

&lt;p&gt;As a reminder this is what the equation looks like:&lt;/p&gt;

\[L=-\log \frac{e^{x_a \cdot x_p \over \tau }}{\sum_i e^{x_a \cdot x_i \over \tau}}\]

&lt;p&gt;Where the numerator contains the comparison between the positive pair \(x_a\) and \(x_p\), and the denominator has all the comparisons (all negative pairs except one). Minimizing the loss means maximizing the value in the numerator (maximizing \(x_a \cdot x_p\)) and minimizing the denominator (minimizing all \(x_a \cdot x_i\)).&lt;/p&gt;

&lt;p&gt;Now imagine \(\tau=1\). Remember the similarity function is the cosine distance, which with normalized vectors goes from -1 to 1. After applying \(exp\) the range goes from 1/e to e.&lt;/p&gt;

&lt;p&gt;Note how small that range is, and note that if the vectors were orthogonal to each other then the similarity is 0 and therefore \(e^{0}=1\).&lt;/p&gt;

&lt;p&gt;So making negative pairs orthogonal to each other is not enough to minimize the loss, because the numerator can at most be \(e\) while the denominator would be (if all pairs were orthogonal) \(\sum_i 1\). What the model would have to try and do is make the negative pairs all be antiparallel to each other. That’s obviously not ideal, if two vectors are orthogonal that should be enough of a separation.&lt;/p&gt;

&lt;p&gt;Now set the \(\tau=0.1\). Now the range goes from \(e^{-0.1}=4e-5\) to \(e^{10}=22000\) ! Incase of orthogonality the similarity after exp is still 1 of course. But now making the vectors of a negative pair orthogonal (rather than close to parallel) will result in a much larger decrease in loss, as the numerator value can be so much larger (i.e. \(22000 &amp;gt;&amp;gt; \sum_i 1\)).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Contrastive learning has become very popular recently, see here for a good overview of recent papers.</summary></entry><entry><title type="html">Changing My Mind On E2E ASR</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html" rel="alternate" type="text/html" title="Changing My Mind On E2E ASR" /><published>2021-04-12T10:06:02+00:00</published><updated>2021-04-12T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html">&lt;p&gt;I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).&lt;/p&gt;

&lt;p&gt;First let’s clarify what E2E ASR means for me: No phones, the model outputs are letters or subword units. No alignment needed and the model learns an internal LM.&lt;/p&gt;

&lt;p&gt;Particularly not using phones bothered me knowing how inconsistent pronunciations of English words are, it just seemed suboptimal to force the network to memorize that.&lt;/p&gt;

&lt;p&gt;However, I’ve had a bit of a change in thinking recently. This has come not from realizing the existance of some technical fact, but rather thinking about what the point of doing speech recognition actually is from the perspective of a consumer. There are many different use cases of course but in by far the majority of them what the end-user wants is a clean transcript. It should be easy to read, free of disfluences, filler words and repetitions.&lt;br /&gt;
I believe traditional ASR is flawed for achieving this, and E2E ASR is not.&lt;/p&gt;

&lt;p&gt;Traditional ASR is split up into at least two components, the AM and LM. The AM is tasked with modeling the acoustics by recognizing phones. Training it means tuning the model so that it makes a decision to which phone each frame (25ms of audio) belongs to. But outputting clean transcripts means ignoring parts of the audio. So this design decision of having one model which classifies phones makes it hard for the model to learn to output clean transcripts, since the better the AM gets at modeling phones the harder it is to ignore certain sounds in the input audio.&lt;/p&gt;

&lt;p&gt;Of course, people with some ASR experience will know that in reality traditional ASR does not have such a problem with outputting clean transcripts. The LM alone should make sure only reasonable word sequences are output. And in practice the AM learns from the training data which (often) has no disfluencies, so the AM learns to map those sounds, which really correspond to some phone, to silence.&lt;/p&gt;

&lt;p&gt;Another issue is that people will regularly pronounce words differently than what you would expect, so during training the model will have to learn to map from one phone to another simply because the lexicon entry for a word will not always correspond exactly how people actually pronounce it.&lt;/p&gt;

&lt;p&gt;So if our AM is not actually modeling sounds, and we’re already forcing it do some more complicated memorization, well then why not just make it model letters?&lt;/p&gt;

&lt;p&gt;While fillers like “um” are never wanted, it’s pretty common to have “you know” missing in a transcript and those words are definitely something the model should not learn to ignore. How is a traditional ASR system supposed to deal with these sorts of errors? &lt;br /&gt;
It cannot. But a transformer-based E2E ASR system can. Thanks to the fact that it looks at much more context (basically seeing the entire input), and that it models letters/subwords directly, it can (for example) learn that a certain sound sequence said quickly at the end of certain sentences (“I really like him yaknow”) can be ignored. This is much harder for a traditional ASR system to learn because the AM does not have so much context it can look at, and even if it did the internal representations would have little to do with words - so it doesn’t know what sentence is being said - as its task is discriminating phones.&lt;/p&gt;

&lt;p&gt;So there’s two points I’m making here: (1) With the training data we have we force the AM to learn an ill-defined task (the task is actually more complicated than just learning to classify phones). (2) E2E systems that use lots of context are better suited to outputting clean transcripts because they combine the AM and LM, so they can learn to ignore sounds because of words that were said before and/or after.&lt;/p&gt;

&lt;p&gt;Clean transcripts aren’t just better for a human reader, it also makes post processing easier for any downstream ML system. And in my experience speech recognition by itself does not have that much value (from a commercial perspective), it’s by adding an NLU system on top that a lot more possibilities for use cases open up.&lt;/p&gt;

&lt;p&gt;I feel like some people have spent so much time thinking about how to model phones that they’ve forgotten that we don’t actually care about phones at all. It could be an interesting question for linguists to study, what phones do people use etc., but if your task is speech recognition classifying which sounds were in the audio should only be a means to an end. A more general conclusion is: &lt;strong&gt;Speech recognition is actually not about what a person said, rather it’s about what they meant to say.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Of course, it’s not very satisfying to just cross your fingers and train a NN to do ASR. It would be nice to somehow give the model a good prior. But with wav2vec2 I feel a good solution has been found as the performance is very good! This combined with the above perspective has changed my mind on E2E-ASR: I believe it is the way forward.&lt;/p&gt;

&lt;p&gt;edit: In hindsight one thing I want to clarify, although I keep mentioning transformer models those are not really required. What really matters is just that the model is bidirectional, therefore can see into both the past and future, and of course that the outputs are character based and the model learns an internal LM.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).</summary></entry><entry><title type="html">Why you need (at least) a billion words to get a good language model</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" rel="alternate" type="text/html" title="Why you need (at least) a billion words to get a good language model" /><published>2021-04-01T10:06:02+00:00</published><updated>2021-04-01T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html">&lt;p&gt;2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail.&lt;/p&gt;

&lt;p&gt;I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;199989 krisenbewältigungen 2
199999 gendersensitiv 2
200002 umgehbar 2
200005 widersinnigen 2
200016 ausmehrungen 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It is kind of expected for this to happen, since it is well known that word counts follow zipf’s law, which put simply states that as you go down a table of words sorted by count, their counts decrease very rapidly, meaning a very large amount of the probability mass is covered by the top words. Here is an image (forgive the lack of axis labels please, y-axis is count in millions x-axis rank of word):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/words.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at it! (yes I know log scale bla bla, shush pedants) The counts drop extremely quickly to almost nothing.&lt;/p&gt;

&lt;p&gt;So the main point is that you need to have seen a word a couple times to know how it is used. But because in language there is a looong tail of words that are used infrequently (but are still normal enough that you do want to estimate them!) you need a &lt;strong&gt;lot&lt;/strong&gt; of text to get the counts to a reasonable level.&lt;/p&gt;

&lt;p&gt;The additional thing to realise is that because the counts are so low there will be a &lt;strong&gt;ton&lt;/strong&gt; of noise in the results. Depending on the corpus some words whose “true” probability is higher will be much lower and vice versa. This is bad.&lt;/p&gt;

&lt;p&gt;This is why you need to train on (at least) billion word corpuses to get good results. Then all those 2s will turn into 20s, and then the model can learn something.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The total count is around 400k by the way (a good chunk of them are rubbish).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail.</summary></entry><entry><title type="html">Deriving BPE from scratch</title><link href="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html" rel="alternate" type="text/html" title="Deriving BPE from scratch" /><published>2021-02-09T10:06:02+00:00</published><updated>2021-02-09T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html">&lt;p&gt;BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?&lt;/p&gt;

&lt;p&gt;Let’s create a simple ‘corpus’* (word counts are what actually matter):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;low 5
lowest 2
newer 6
wider 3
new 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Including the separator (_, appears implicitly at the end of a word) there are 11 characters. Let’s pick the 20 most common ngrams, they turn out to be (in order, but starting with unigrams):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'r_', 'we', 'ne', 'new', 'ew', 'lo', 'low']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is what BPE outputs (single character tokens are not ordered):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'ne', 'new', 'lo', 'low', 'newer_', 'low_', 'wi']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first not-unigram is the same for both (“er”), the second as well, but then the methods diverge. Note how the ngram counter picks up “we” while BPE never does. Why is that? Normally “we” would of course be a reasonable token, but for this corpus it makes more sense intuitively, considering the counts, to keep it seperate so that low + est and new + er are valid tokenisations.&lt;/p&gt;

&lt;p&gt;Because BPE redoes the count each time two tokens are merged (from which a new one is created), and only the merged token is counted - instead of also the previous two - the counts of some tokens can go down after a recount. So when “er” is created the count of “we” drops from 8 to 2! The recounting makes sense in hindsight, the counts of a token should be based on when that token could actually be used, and after choosing the “er” token, the token “we” is not possible as often as before.&lt;/p&gt;

&lt;p&gt;What if we change the k most frequent method to iteratively merge with recounts inbetween, and when recounting if two tokens were merged then we only count the merged token (not the previously split tokens)?&lt;/p&gt;

&lt;p&gt;We would have to start with some set of tokens, those could be the unigrams. Then we count ngrams that are not already in our token set, merge the most frequent, repeat. This is basically BPE, except we’re still doing a bunch of wasted computation by counting ngrams: It should be, with a little thinking, obvious that a bigram will always be at least as likely as a trigram. So rather than count all ngrams that are not already in the token set, we just count bigrams of tokens AKA pairs of tokens. This is BPE.&lt;/p&gt;

&lt;p&gt;Now the motivation for the algorithm of BPE is clear. It recounts after each merge to make sure the statistics used are up-to-date, and by considering pairs of tokens it saves a lot of computation.&lt;/p&gt;

&lt;p&gt;* Taken from Jurafsky’s chapter on BPE.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?</summary></entry><entry><title type="html">On WER in ASR</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html" rel="alternate" type="text/html" title="On WER in ASR" /><published>2020-11-27T10:06:02+00:00</published><updated>2020-11-27T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html">&lt;p&gt;This post will be about the python-based &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;tool (“texterrors”)&lt;/a&gt; I created for getting error metrics (relevant for ASR). It is split in two parts: 
First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking &lt;a href=&quot;#newtool&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;wer-calculation-recap&quot;&gt;WER calculation recap&lt;/h2&gt;

&lt;p&gt;Given a hypothesized sentence the Word-Error-Rate is defined as the number of insertion, deletion and substitution errors with the respect to a reference sentence, divided by the count of words in the reference. Insertion/Deletion is defined from the perspective of the model, so for example if the model outputs a word when it shouldn’t, that’s an insertion error.&lt;/p&gt;

&lt;p&gt;To find out the types of errors one has to align the hypothesis to the reference, this is typically done by creating a cost matrix (where the cost of a cell depends on the transition cost plus the lowest cost from the left, top or diagonal cells) and backtracing from the end (bottom right) to the start (top left) to find the alignment. Example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;third&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;second&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;third&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Taking a horizontal or vertical (deletion/insertion) transition costs 1. Taking a diagonal to position &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; costs 1 if word i != word j, else 0. “second” is not recognized by the model (a deletion error) which is why at the end the cost in the bottom right is 1. Notice that if all we want to know is the WER, you can actually just take that value (1) and divide by the count of reference words (3) to get the WER (33.3%).&lt;/p&gt;

&lt;p&gt;However, usually one wants to know how many errors of each type there are, and to do that one needs to get the alignment to then count them. This requires backtracing which is done by finding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transition_cost&lt;/code&gt; = &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; is either to the left, diagonal or above the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt; (then repeat the process until the start is reached).&lt;br /&gt;
This can be ambiguous, for example consider two sentences “first word in sentence” and “first ward sentence”. There are different ways to align this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Clearly the pair “word”/”ward” is a more likely substitution error than “in”/”ward”, but this alignment method has no way of identifying that since the cumulative costs are the same, see cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This has no impact on the WER, as in both cases there are two (one insertion/deletion depending on which sentence is considered the reference, one substitution), but from the perspective of analysing what sorts of errors a model is making - which sorts of words the model is failing to recognize (deletions), which words are confused with each other (substitutions) - having a different alignment will change the result.&lt;/p&gt;

&lt;h2 id=&quot;getting-better-alignments-with-texterrors-&quot;&gt;Getting better alignments with “texterrors” &lt;a name=&quot;newtool&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;As just mentioned, the traditional method of alignment (which we need to do to get statistics for the different error types) leads to ambiguous alignments with no sensible way of resolving them. &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;“texterrors”&lt;/a&gt; is meant to be a tool for getting detailed error metrics. As these are sensitive to suboptimal alignments it uses a smarter method: Instead of having a cost of 1 for the substitution cost (in the cost matrix), it incorporates the character edit distance between the words compared.&lt;/p&gt;

&lt;p&gt;Concretely, the substitution cost is set to the edit distance between two words divided by the maximum edit distance possible (length of the longer word), so it is a value between 0 and 1 (slightly more complicated in practice, you’ll see later why). That way alignments will be favoured when words which are similar to each other are substitution errors instead of deletion/insertion errors.&lt;/p&gt;

&lt;p&gt;Example cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.25&lt;/td&gt;
      &lt;td&gt;1.125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As one can see here, the best (lowest) cumulative cost is achieved by pairing “word”/”ward”.&lt;/p&gt;

&lt;p&gt;One should be aware, this method can result in a higher WER. 
In the below example a normal WER calculation would do a one-to-one mapping and arrive at a WER of 66.67\%.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But character aware alignment would result in the following alignment:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;-&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This results in a WER of 83.3\% because of the extra insertion and deletion. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; has an option to turn character-aware alignment off (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-no-chardiff&lt;/code&gt;) to get identical results with kaldi. But the difference is small for a normal sized test set, and obviously without the feature the alignments and therefore statistics like the most frequent substitution error will not be as accurate!&lt;/p&gt;

&lt;p&gt;There is still one last issue to deal with, see this example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The alignment will end up being&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This happens here because using the character edit distance leads to the substitution cost often being smaller than the insertion/deletion cost and therefore alignments with more substitutions are favored.&lt;br /&gt;
This sort of bad alignment can also happen with normal costs of 1/1/1 for ins/del/sub (consider the above example, as the costs are the same for different errors it depends on the implementation which alignment is chosen, you can think of it as random). That’s why such tools, when meant to be used for getting detailed error metrics, will increase the substitution cost to improve alignments. We can do the same: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; will after the previously mentioned calculation times the cost by 1.5. This will lead to the following cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And the following (obviously superior) alignment.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;-&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; also supports ctm files where the time stamps for reference and hypothesis words will be used for alignment. If you &lt;em&gt;really&lt;/em&gt; care about having the most accurate alignment possible, that is what you should use! But it won’t make a big difference. :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;Here is a link to it.&lt;/a&gt; Thank you for reading.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This post will be about the python-based tool (“texterrors”) I created for getting error metrics (relevant for ASR). It is split in two parts: First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking here.</summary></entry><entry><title type="html">Doing non-standard stuff with kaldi decoding</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html" rel="alternate" type="text/html" title="Doing non-standard stuff with kaldi decoding" /><published>2020-11-06T10:06:02+00:00</published><updated>2020-11-06T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html">&lt;p&gt;Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found &lt;a href=&quot;https://github.com/RuABraun/fst-util&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;adding-words-to-hclg&quot;&gt;Adding words to HCLG&lt;/h2&gt;

&lt;p&gt;Some scripts you will need can be found &lt;a href=&quot;https://github.com/idiap/icassp-oov-recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This method requires you to use a monophone model. Additionally, your language model needs to have been trained with pocolm, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--limit-unk-history&lt;/code&gt; option, and there should have been some OOVs in the training text.&lt;/p&gt;

&lt;p&gt;For simplicity, the modification is done on a graph without self-loops. So you need to modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/mkgraph.sh&lt;/code&gt; and comment L167: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm $dir/HCLGa.fst $dir/Ha.fst 2&amp;gt;/dev/null || true&lt;/code&gt; because we will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Inside the graph dir where the HCLG is there is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt;. You need to assign IDs to the new words you’re adding and append these to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt; file (these should be larger than the existing ones obviously).&lt;/p&gt;

&lt;p&gt;Assuming all this is ready you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;script/compose_hcl.sh&lt;/code&gt; to create the HCL from a lexicon of the OOV words you want to add. Check the script for the input arguments, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final.mdl&lt;/code&gt;, isym is phones osym words. Notice it uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_lfst.py&lt;/code&gt; so you need the fst wrapper installed. There is one hardcoded parameter on L25, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;303&lt;/code&gt;, see &lt;a href=&quot;https://groups.google.com/g/kaldi-help/c/jL8VnwKGRWs/m/-Pe29-G9AgAJ&quot;&gt;here&lt;/a&gt; for what’s about. You can set it to any number larger than the existing phone IDs.&lt;/p&gt;

&lt;p&gt;After calling the script and creating the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCL.fst&lt;/code&gt; you use the fst wrapper to modify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from wrappedfst import WrappedFst
fst = WrappedFst('HCLGa.fst')
ifst = WrappedFst('HCL.fst')
unk_id =  # unk symbol
fst.replace_single(unk_id, ifst)
fst.write('HCLGa_new.fst')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then add the self-loops (check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkgraph.sh&lt;/code&gt; for how to do that) and you are done. Replace an existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLG.fst&lt;/code&gt; with the new version and you can run decoding as you would normally.&lt;/p&gt;

&lt;h2 id=&quot;no-hclg-just-g&quot;&gt;No HCLG just G&lt;/h2&gt;

&lt;p&gt;Imagine you have an acoustic model that you created via pytorch, and you want to then evaluate how well it does at recognizing phonemes (TIMIT for example). You have a phone LM that you want to incorporate. I’m going to show you how you can do that with kaldi.&lt;/p&gt;

&lt;p&gt;Usually kaldi assumes that you have an HMM graph (the H in HCLG) and possibly context dependency as well (the C), then there’s also the lexicon etc. If you had a pytorch AM that you had trained on the PDF-ids (the targets you get by calling ali-to-pdf on the alignments), you would then use that by saving the loglikelihood output of your NN to a file, then passing that file and the final.mdl, HCLG.fst to latgen-faster-mapped, which will do decoding and generate a lattice (from which you could then take the best path for example).&lt;/p&gt;

&lt;p&gt;But if you want to just model phones, you don’t care about the HCL at all. You really just want to use a G that will act as the phone LM.&lt;/p&gt;

&lt;p&gt;So the first issue to get around is to avoid doing any mapping from transition IDs (the typical input symbols of the HCLG) to PDF-IDs. We can do that by creating a file latgen-faster.cc with the only difference being we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaled&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaledMapped&lt;/code&gt; in the code.&lt;/p&gt;

&lt;p&gt;Then you take the arpa LM that you have trained, convert it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arpa2fst&lt;/code&gt;, convert the #0 arcs to &lt;eps&gt; and add self-loops to states that have incoming arcs with input labels not equal 0. The point of that is when the AM predicts a sequence &quot;ae ae ae b b&quot; over five frames, the LM should not care about repetitions, we want it to only score the transition &quot;ae&quot; to &quot;b&quot;, and of course we don't want repetitions in the output transcript. Having self-loops fixes that.&lt;/eps&gt;&lt;/p&gt;

&lt;p&gt;Then you call&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;latgen-faster --acoustic-scale=1.0 --beam=13 --determinize-lattice=false G.fst ark,t:loglikelihoods-file&amp;gt; ark:- | lattice-best-path etc.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to get a hypothesis that you can compare to the reference to get the PER!&lt;/p&gt;

&lt;p&gt;If you wanted to recognize words, all you’d need to do is create a L.fst, compose that with the G.fst (which is then a word LM) and use the LG.fst.&lt;/p&gt;

&lt;h2 id=&quot;g-with-keyword-list&quot;&gt;G with keyword list&lt;/h2&gt;

&lt;p&gt;Imagine you just want to recognize a small number of words and you don’t want to have any sort of prior regarding which one is more likely.&lt;/p&gt;

&lt;p&gt;To do that you just have to create an FST with a single state, and self-loops where each self-loop has a keyword as the input and output symbol.&lt;/p&gt;

&lt;p&gt;In pseudo-code&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fst = Fst()
state = fst.add_state()
fst.set_start(state)
fst.set_final(state)
for keyword in keywords
    fst.add_arc(state, state, keyword, keyword, cost)
fst.write(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The self-loops should have costs, what these should be you will have to find out experimentally.&lt;/p&gt;

&lt;p&gt;You probably will have to create the lang/ folder again. To do that just call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_lang.sh&lt;/code&gt; with the dict/ of keywords and with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--phone-symbol-table&lt;/code&gt; option set to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phones.txt&lt;/code&gt; that you trained the AM model with.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found here.</summary></entry><entry><title type="html">First post: Ark and scp files in kaldi</title><link href="https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi.html" rel="alternate" type="text/html" title="First post: Ark and scp files in kaldi" /><published>2020-10-04T12:40:02+00:00</published><updated>2020-10-04T12:40:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi.html">&lt;p&gt;This is about the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.ark&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.scp&lt;/code&gt; files that are used with kaldi and have spread to other toolkits like ESPNet.
It’s not complicated to understand to them, but I’ve noticed a surprising number of people who use them don’t. This is supposed to be a concise summary of what they are.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Both file types are structured by having keys and a value for each key.&lt;/p&gt;
&lt;h1 id=&quot;scp-files&quot;&gt;Scp files&lt;/h1&gt;

&lt;p&gt;In scp files, usually ending with the “.scp” suffix, the first column (aka field) is the key (usually an utterance ID). The rest of the line is treated as a pointer to data (the pointer is the value). 
What this means is that the scp file never contains any data, it just contains something which points to the data you will eventually use. See for example the wav.scp file which will look something like this usually:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wav-id-1 /path/to/file-1.wav
wav-id-2 /path/to/file-2.wav
wav-id-3 /path/to/file-3.wav
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be a file path, but can also be more complex like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wav-id-1 sox /path/to/file-1.wav -r 8k - |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Kaldi’s code is written so that it will recognize and execute the above command (starting from after the key), which results in the wav file being read with a sampling rate of 8kHz. This is a convenient way of doing the resampling on the fly (instead of resampling and having to waste space storing a file somewhere).&lt;/p&gt;

&lt;p&gt;Another file one will see a lot when using kaldi is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats.scp&lt;/code&gt; file which looks like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;utt-id-1 /path/to/file-1.ark:44
utt-id-2 /path/to/file-1.ark:760
utt-id-3 /path/to/file-1.ark:1520
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The number after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt; is a byte offset.&lt;/p&gt;

&lt;h1 id=&quot;ark-files&quot;&gt;Ark files&lt;/h1&gt;

&lt;p&gt;Instead of the value being a pointer to the data, in ark files the value &lt;strong&gt;is&lt;/strong&gt; the data. Usually this means it is in binary format, so you can’t visually inspect it, however kaldi has binaries for converting from binary to text format. So for example take the ark file from the feats.scp example above and call:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats ark:/path/to/file-1.ark ark,t:/path/to/file-1.ark.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark,t&lt;/code&gt; means that the values in the output will be written in textual format, so you can open the file and see what the values for the (example) MFCC coefficients are for each frame.&lt;/p&gt;

&lt;p&gt;Another example of an ark style file, although it doesn’t have the “.ark” suffix, is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; file you will find in kaldi data folders. The first field/column is the utterance ID, the rest of the line is the words belonging to that utterance. The words are the data.&lt;/p&gt;

&lt;p&gt;You can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; to concatenate ark files. And kaldi binaries accepts commands and wildcards in the arguments, so this is valid: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark:gunzip -c lat.*.gz|&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Kaldi has a script for subsetting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; files you may find useful called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/subset_scp.pl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When you call kaldi binaries you often have to prefix with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark:&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp:&lt;/code&gt; to tell kaldi what type of file to expect. You can then mix types, for example I could use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/subset_scp.pl&lt;/code&gt; to subset &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats.scp&lt;/code&gt; and create a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats_subset.scp&lt;/code&gt;, and then copy a subset of the feature data by calling&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats scp:feats_subset.scp ark:copied_feats_subset.ark
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how the input argument is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt;, so kaldi knows the input file is pointing to the data I want to use, and the output is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt;, so kaldi knows to actually write the data pointed to to a new file.&lt;/p&gt;

&lt;p&gt;It’s not possible to create just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; file from an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt; file, you have to create both the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; files. So you have to do something like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats scp:/path/to/feats.ark ark,scp:copied_feats.ark,copied_feats.scp 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details check out the official documentation of &lt;a href=&quot;http://kaldi-asr.org/doc/io.html&quot;&gt;kaldi io&lt;/a&gt;. Probably the section “from a command-line perspective” is more relevant to you.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This is about the .ark and .scp files that are used with kaldi and have spread to other toolkits like ESPNet. It’s not complicated to understand to them, but I’ve noticed a surprising number of people who use them don’t. This is supposed to be a concise summary of what they are.</summary></entry></feed>