<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://ruabraun.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ruabraun.github.io/" rel="alternate" type="text/html" /><updated>2025-02-24T03:09:17+00:00</updated><id>https://ruabraun.github.io/feed.xml</id><title type="html">Sharings</title><author><name>Rudolf A. Braun</name></author><entry><title type="html">2024 Year in Review</title><link href="https://ruabraun.github.io/jekyll/update/2024/12/31/Year-in-Review.html" rel="alternate" type="text/html" title="2024 Year in Review" /><published>2024-12-31T10:06:02+00:00</published><updated>2024-12-31T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2024/12/31/Year-in-Review</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2024/12/31/Year-in-Review.html">&lt;p&gt;Like last year I’m going talk about some favourite media I’ve consumed, but inspired by this &lt;a href=&quot;https://zhengdongwang.com/2024/12/29/2024-letter.html&quot;&gt;post&lt;/a&gt; I’m going try something new and include some freeform thoughts.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;a-jumble-of-thoughts&quot;&gt;A jumble of thoughts&lt;/h1&gt;

&lt;p&gt;o1 and &lt;a href=&quot;https://www.youtube.com/watch?v=eaAonE58sLU&quot;&gt;test time compute&lt;/a&gt; have me convinced. I think we’re going to see AGI-like agents very soon. The ability to consider a choice before making one will I expect lead to a step change in reliability, which was the major reason why these systems were not good enough to do tasks independently. Jobs which don’t require longterm memory like frontline call centers will be automated very soon. Who knows how long it will take for longterm memory to be cracked, but based on current trends not long! But I do think as the capabilities get better we will discover more challenges to tackle, so I don’t expect things to improve as straightforwardly as some make it out to be (the “we’re all going to lose our jobs” commentators).&lt;/p&gt;

&lt;p&gt;I’ve been surprised by how the EU’s moves to focus on regulation have trickled down to the  people. Talking to European friends they have developed a focus on being careful and minimizing excess. This wouldn’t be bad by itself but I believe the emphasis to be taken to the point that there isn’t much thought put towards future possibilities. I’m not optimistic on Europe. On the bright side I think there is, in Europe, a growing recognition for a need for change.
The best clothes shopping though for sure. I’m no expert but think New York may be overrated if you have the option to shop in Europe instead. Style is better, prices are cheaper and things are closer together.&lt;/p&gt;

&lt;p&gt;The best museum I visited in 2024 was the Royal Greenwich Observatory in London. Fascinating to learn how mapping the stars and accurate timekeeping was critically important for seafaring navigation, and disasters where multiple ships were lost lead to the government creating a massive reward which was eventually won by &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Harrison&quot;&gt;John Harrison&lt;/a&gt; (though he never got the full amount of ~10M Pounds because some Admirals didn’t like him). His clocks were used by &lt;a href=&quot;https://en.wikipedia.org/wiki/James_Cook&quot;&gt;James Cook&lt;/a&gt; on his Pacific voyages where he created the famously accurate maps of the southern Pacific Ocean which were still used in the 20th century, 300 years later!&lt;/p&gt;

&lt;h1 id=&quot;books&quot;&gt;Books&lt;/h1&gt;

&lt;p&gt;Another year of not much fiction. Honourable mention to “Wide Sargasso Sea”, great prose. Some great non-fiction finds though:&lt;/p&gt;

&lt;h2 id=&quot;price-of-peace---money-democracy-and-the-life-of-keynes&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/49644992-the-price-of-peace&quot;&gt;Price of Peace - Money, Democracy and the Life of Keynes&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;The past fifty years of economic policy have been ignoring that we live in a post-scarcity society.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Most of us understand our work as something functional. [..] We want to believe our economic status, even if shaded by luck and circumstance, has at least something to do with our contribution to society. [..] But in a post-scarcity economy, the very meaning of work was a technicality, something people just have to do because it keeps the system running, not because it is truly essential to clothing and feeding the public. Much of what we imagine ourselves to be contributing to society through our work is in fact an accounting trick to enable consumption.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I vaguely knew John Maynard Keynes was a major figure in economics, but this book came as a revelation: The story of one person’s life intertwining with major historical events and along the way pulling the entire field of economics into and beyond the 20th century. The cool thing is you basically get an education of economics by following along with Keynes’ realizations.&lt;/p&gt;

&lt;p&gt;I think his ideas of a reduced work week (“my grandchildren will work 15 hours a week”) are relevant again with AI now becoming a thing. The biggest hindrance I see to this happening is that the people in power, whether in government or corporations, like to work. They wouldn’t work less if they could and they want their reports to keep working at the same rate to help achieve their (not the report’s) goals. Maybe part-time work can become more of a thing though.&lt;/p&gt;

&lt;h2 id=&quot;the-road-less-traveled&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/347852.The_Road_Less_Traveled&quot;&gt;The Road Less Traveled&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A how-to to life.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This tendency to avoid problems and the emotional suffering inherent in them is the primary basis of all human mental illness. Since most of us have this tendency to a greater or lesser degree, most of us are mentally ill to a greater or lesser degree, lacking complete mental health. Some of us will go to quite extraordinary lengths to avoid our problems and the suffering they cause, proceeding far afield from all that is clearly good and sensible in order to try to find an easy way out, building the most elaborate fantasies in which to live, sometimes to the total exclusion of reality.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I’m still reading this book. It’s taking me a surprisingly long time. Usually that would mean I’m not enjoying it. But in this case it’s because I feel almost scared about the prospect of turning the next page and being met with yet another in hindsight obvious fundamental truth which prompts to me reflect on and reevaluate many things.
I think to some most of what’s in here will be obvious. I wish that were the case for me.
Rereading some of the earlier parts for this blogpost it’s incredible how dense this ~350 page book is. Every sentence is a mighty swing of a hammer mercilessly driving the point home. I feel after finishing it (looking to be 5 months!) I would benefit from a reread!&lt;/p&gt;

&lt;h2 id=&quot;the-ode-less-traveled&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/66856.The_Ode_Less_Travelled&quot;&gt;The Ode Less Traveled&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;If you have any interest in poetry you must read this.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Each English word is given its own weight or push as we speak it within a sentence. [..] We always say &lt;em&gt;Brit&lt;/em&gt;ish, we never say Brit&lt;em&gt;ish&lt;/em&gt; or &lt;em&gt;British&lt;/em&gt;. [..] Sometimes the stress will change according to the meaning or nature of the word: [..] “to rebel” versus “the rebel”.&lt;/p&gt;

  &lt;p&gt;You may think “[..] surely this is how everyone talks [..]” Not so.
The French, for instance, tend toward equal stress in a word. They pronounce Canada, &lt;em&gt;Can&lt;/em&gt;-&lt;em&gt;a&lt;/em&gt;-&lt;em&gt;da&lt;/em&gt; as opposed to our &lt;em&gt;Can&lt;/em&gt;ada. [..]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Amusing coincidence to read this in the same year as the just mentioned.
I loved the numerous poems used as examples for some stylistic choice that a poem writer can make. I loved learning about how English poetry developed over the course of history. I wish I could write 20 lines of pentameter with trochaecs, spondees and caesuras in 45 minutes, alas I can’t to a level I’d be happy with, but I loved the opportunity to try, and I plan to try again.&lt;/p&gt;

&lt;h2 id=&quot;the-thin-red-line&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/92417.The_Thin_Red_Line&quot;&gt;The Thin Red Line&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A war novel and about how difficult it is to truly know each other.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;He could not believe that any of them might actually hit somebody. If one did, what a nowhere way to go: killed by accident; slain not as an individual but by sheer statistical probability, by the calculated chance of searching fire, even as he himself might be at any moment. Mathematics! Mathematics! Algebra! Geometry! When 1st and 3d Squads came diving and tumbling back over the tiny crest, Bell was content to throw himself prone, press his cheek to the earth, shut his eyes, and lie there. God, oh, God! Why am I &lt;em&gt;here&lt;/em&gt;? Why am I &lt;em&gt;here&lt;/em&gt;? After a moment’s thought, he decided he better change it to: why are &lt;em&gt;we&lt;/em&gt; here. That way, no agency of retribution could exact payment from him for being selfish.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This isn’t a story where you get the perspective of just one person’s experiences and feelings (like e.g. All Quiet on The Western Front), instead you spend time with and get to know multiple, and get the different takes people had on the same situation. This is a great example for one advantage novels still have over other media: You learn about people thoughts. You get to see how people’s intuitions and thought patterns develop who they are and influence their actions. Loved it!&lt;/p&gt;

&lt;h1 id=&quot;articles&quot;&gt;Articles&lt;/h1&gt;

&lt;p&gt;If you get blocked from reading use archive.is.&lt;/p&gt;
&lt;h2 id=&quot;trump-is-planning-for-a-landslide-win&quot;&gt;&lt;a href=&quot;https://www.theatlantic.com/politics/archive/2024/07/trump-campain-election-2024-susie-wiles-chris-lacivita/678806/&quot;&gt;Trump Is Planning for a Landslide Win&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Trump’s campaign managers bet on a new strategy for targeting and motivating voters.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Political consultants often consider eligible voters on a one-to-five scale: Ones being the people who never miss an election and hand out campaign literature in their spare time, fives being the reclusive types who can’t be canvassed, have never cast a vote, and probably never will. Most campaigns [..] focus their resources on the ones and twos. “There was this other bucket that we identified: low-propensity Trump supporters,” Wiles said. “We sort of took a gamble, but we were really sure that those tier-three people would be participating, that they would be our voters.”
Several times in the summer and fall of 2023, I heard from DeSantis allies who were bewildered by what Trump’s team was (and wasn’t) doing on the ground. “Our opponents were spending tens of millions of dollars paying for voter contacts for people to knock on doors,” LaCivita said. “And we were spending tens of thousands printing training brochures and pretty hats with golden embroidery on them.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;How did Trump manage to come back from his 2020 election loss and Jan 6? How did Harris lose despite having access to enormous amounts of money (more than Trump)? Written before Biden dropped out, I think this article shows how a significant factor was the Trump campaign strategy that focused on motivating their supporters to turn out.&lt;/p&gt;
&lt;h2 id=&quot;american-vulcan&quot;&gt;&lt;a href=&quot;https://www.tabletmag.com/feature/american-vulcan-palmer-luckey-anduril&quot;&gt;American Vulcan&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Palmer Luckey’s journey from whizkid to outcast to Tony Stark.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“At some point, in business and in life and in romance, you have to commit to a path,” said the 31-year-old Luckey. “A lot of my peers in the tech industry do not share this philosophy. They’re always pursuing everything with optionality. [..]
“In keeping their options open, they ensure that they’re going to jump from option to option. If you don’t commit to a path, you’re going to fail at it … You have to commit to it to make it work, and I think marriage is the same way. You just have to commit to it. You have to say, ’This is the path I’m on. For better or for worse, I’m going to double down on it.’”
“The one thing money can’t buy,” said Luckey, “is people who liked you before you had money. I’m very lucky that I met my wife back when I had literally nothing. When we met, I had less than $300 in my bank account. I probably should have gotten married, should have married her when I was 16. Looking back, I think that’s probably my radical belief.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I had never looked into Luckey’s story before and I’m glad I waited for this deep dive so all the details were new and exciting to me. It’s refreshing to read about someone who’s just excited to get things done but also has a relatively humble background and remembers that but doesn’t let it hold him back. Exciting to see him succeed despite his detractors, who seem to be acting out of deference to what a social media hive says rather than reality.&lt;/p&gt;

&lt;h3 id=&quot;last-boys-at-the-beginning-of-history&quot;&gt;&lt;a href=&quot;https://thepointmag.com/politics/last-boys-at-the-beginning-of-history/&quot;&gt;Last Boys at the Beginning of History&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;An up close and personal examination of the young new right.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Lucas, born in 2005, was raised in a “typical” and “apolitical” family outside of Philadelphia. “I’ve never in my life remembered a time when the Democratic Party supported ambitious people,” he says. “I think their whole ideology is based off of oppressing those with ambition, who actually have the gumption to go out and do something and build something on their own. … The people who make humanity great, the innovators, the builders, the winners in society, they look at the winners and tell them, ‘You’re evil, and the only reason you’re at the position that you’re at is because you exploited other people.’ It’s antithetical to the way that a lot of young men work.”
“All young men, even if they’re not actively trying to be great, still admire greatness,” [..] “It’s really rare that you meet one that doesn’t have some respect for somebody who’s gone out and done something great.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is about a world I didn’t know existed and I instinctively recoiled from some of the statements from people in it, like comparing Trump to Napoleon. But the writing and the undercurrent of truth kept me going and I’m glad I did. It’s refreshing to hear from a new perspective, and this is exactly the sort of article I hope to keep finding and reading.&lt;/p&gt;

&lt;h1 id=&quot;movies&quot;&gt;Movies&lt;/h1&gt;

&lt;h2 id=&quot;anora&quot;&gt;Anora&lt;/h2&gt;

&lt;p&gt;**The best movies mix genres and 5 years after Parasite we have another gem.&lt;/p&gt;

&lt;p&gt;To me this movie is a class commentary. It’s about how for most people they’re just trying to get by and if possible ahead. That’s Anora, Toros (the priest), Igor and most of the other characters. All their actions are to some level expected being motivated by the need to not fall behind and seize opportunity where it exists. Separately though, there are a class of people who don’t have to worry about getting by. See Ivan. His actions are the ones we’re curious to know, because all the other’s are based on what he does. And as long as that uncertainty exists we get to see the comedy and the tragedy of the other class trying to get by. And what a comedy! What could’ve been a mundane plot is turned into something marvelous by the detail and care put into in the execution.
The ending really makes the movie for me. I’ve never seen such a switch in tone pulled of so seamlessly.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Like last year I’m going talk about some favourite media I’ve consumed, but inspired by this post I’m going try something new and include some freeform thoughts.</summary></entry><entry><title type="html">Don’t underestimate your own edge</title><link href="https://ruabraun.github.io/jekyll/update/2024/07/27/Dont-underestimate-your-own-edge.html" rel="alternate" type="text/html" title="Don’t underestimate your own edge" /><published>2024-07-27T10:06:02+00:00</published><updated>2024-07-27T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2024/07/27/Dont-underestimate-your-own-edge</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2024/07/27/Dont-underestimate-your-own-edge.html">&lt;p&gt;It is May 27th 2024. Nvidia’s stock is at $1064.69.&lt;/p&gt;

&lt;p&gt;Roughly five years ago Google &lt;a href=&quot;https://blog.google/products/search/search-language-understanding-bert/&quot;&gt;publicly announced&lt;/a&gt; their integration of BERT into Google search. Nvidia’s stock on that date was $51.14.&lt;/p&gt;

&lt;p&gt;I remember thinking about whether investing in Nvidia could be a good idea. Deep learning models were clearly an impactful tool that were only going to get better and be applied to more tasks. The hype had already been building since &lt;a href=&quot;https://en.wikipedia.org/wiki/AlexNet&quot;&gt;2012 with the release of AlexNet&lt;/a&gt; which smashed prior records in a computer vision competition and made people in the field aware of its potential. It was one of the most exciting tools that had recently been developed in science.&lt;/p&gt;

&lt;p&gt;Nvidia had essentially a monopoly on the hardware and software required to both train and use deep learning models. They were still led by their original founder Jensen Huang and clearly in a fantastic position.
But I had read just enough about finance to know of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Efficient-market_hypothesis&quot;&gt;Efficient market hypothesis&lt;/a&gt;. Surely if I was aware that Nvidia was in a great position, all the professional investors did too. It must be priced in already!&lt;/p&gt;

&lt;p&gt;Clearly not.&lt;/p&gt;

&lt;p&gt;The takeaway for me is that if you do work in a field and are aware of a great product or service don’t underestimate the degree to which most people will be unaware of them. Even those who you might expect to know better.&lt;/p&gt;

&lt;p&gt;To be fair, professional investors don’t buy stock based on how a company is doing now, but on what they expect growth to be like in the future. I think this is main reason Nvidia was underpriced by professionals. They may have known that they were in a good position, but it was only with the release of ChatGPT and the explosion in AI startups that it became clear that there was huge demand for what Nvidia offered and therefore huge growth on the horizon.&lt;/p&gt;

&lt;p&gt;But I believe that for anybody in the industry it should have been obvious that the growth would happen. Deep learning gets such good results it was guaranteed that some use case would be found for it. Some people took decisive action based on deep learning’s effectiveness even earlier than 2019: &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenAI&quot;&gt;OpenAI was founded in 2015&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Nvidia’s stock price at the time? $8.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">It is May 27th 2024. Nvidia’s stock is at $1064.69.</summary></entry><entry><title type="html">Favourites of 2023</title><link href="https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023.html" rel="alternate" type="text/html" title="Favourites of 2023" /><published>2024-02-26T10:06:02+00:00</published><updated>2024-02-26T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023.html">&lt;p&gt;This is a list of stuff I read or saw in 2023 and really liked. I’m going to say a sentence or more about each and maybe include a quote. I’m hoping somebody who stumbles across this finds something they really like that they otherwise wouldn’t have, or someone who knows me might find an interest in common we didn’t know we had!&lt;/p&gt;

&lt;p&gt;I suggest skimming and using the bolded bylines to pick out what might interest.&lt;/p&gt;
&lt;h1 id=&quot;books&quot;&gt;Books&lt;/h1&gt;

&lt;p&gt;This was a (first?) year of mostly non-fiction for me. It was great! Do want to read more fiction this year though.&lt;/p&gt;

&lt;h2 id=&quot;storyworthy&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/37786022&quot;&gt;Storyworthy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;How to tell a good story.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your story must reflect change over time. A story cannot simply be a series of remarkable events. You must start out as one version of yourself and end as something new.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I loved the practicality of this. He takes his own stories and breaks them down. He goes into detail and provides tons of examples. 
It made me understand that I usually do not do a good job of telling a story and I appreciated having that realization.&lt;/p&gt;

&lt;p&gt;It also made me think there is an opposition between telling an engaging story and explaining something (like in a paper or technical report) or communicating facts efficiently. With the latter you don’t want to surprise a listener, everything should be obvious because of what you said previously, if that’s not the case then you missed a step in the logic or made an error which is why the listener is surprised.
But for storytelling surprise is a key tool to engage the listener, so depending on what you’re talking about you really should have a different structure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=U9v0O0oEmpQ&quot;&gt;He’s got videos of his stories on youtube, check it out!&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;path-to-power&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/86524&quot;&gt;Path to Power&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Lyndon B Johnson’s early life and the first roughly ten years of his political career.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In every election in which he ran—not only in college, but thereafter—he displayed a willingness to do whatever was necessary to win: a willingness so complete that even in the generous terms of political morality, it amounted to amorality.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is hands-down one of the most engaging books I’ve ever read that also goes into incredible detail on things that happened a century ago and one might initially think uninteresting. But the narrative works. It is interesting. It helps of course that LBJ is a singular character. His portrayal is at minimum unsympathetic, yet you can’t help but root for him when he goes to the greatest lengths to help his constituents and when he tries to gain more power (with the hindsight knowledge of him enacting so many liberal policies later). I wonder if someone wrote a similarly detailed biography of say, Obama, would it be close to as interesting?&lt;/p&gt;

&lt;h2 id=&quot;how-to-be-the-greatest-improviser-on-earth&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/30364117-how-to-be-the-greatest-improviser-on-earth&quot;&gt;How To Be The Greatest Improviser On Earth&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Everything you need to know to do improv.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another way to think about the top of the scene is with the Taoist principles of yin and yang. Yin is passive, patient, empathetic, malleable. It is akin to a “yes.” Yang is active, assertive, decisive, altering. It is akin to an “and.” After a series of small moves that simply confirm information without adding a lot, it’s time for a “yang” move: make a decision, add some information. But once someone makes a big “yang” move, then it’s time for a simple “yin” response. Confirm and unpack what was just added. Even a beat of silence is a good idea.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I believe the improv mindset is beneficial for more than just improv. So while a niche book I’m including it here as it really is excellent. This book gives you everything you need to know for improv, literally, and yet it’s so compact! I really felt after reading it I just needed to go practice more, what I should be doing was made clear.&lt;/p&gt;

&lt;h2 id=&quot;articles&quot;&gt;Articles&lt;/h2&gt;

&lt;p&gt;Only started saving these towards the end of the year so the list is short.&lt;/p&gt;
&lt;h2 id=&quot;lessons-from-the-years-of-lyndon-johnson-by-robert-caro&quot;&gt;&lt;a href=&quot;https://www.dwarkeshpatel.com/p/lyndon-johnson&quot;&gt;Lessons from The Years of Lyndon Johnson by Robert Caro&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A collections of thoughts and lessons from reading Robert Caro’s series on LBJ by Dwarkesh Patel.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the first 55 years of his life, he convinced men of tremendous intellect and drive of their own that he was the scion of the Southern cause - that the only hope for that cause was to make Lyndon Johnson President. So much so that when Lyndon Johnson was Majority Leader, the Southern Senators, who at the time controlled all the important committees in Congress, allowed Johnson to pass liberal legislation they scorned because they believed it would help him become President. And, they believed, as President, Johnson would govern as a conservative, especially on the issue of race. The tragic irony and betrayal here is hard to over-emphasize. These powerful conservative men - who, while being ignorant and backwards in certain ways, were famously shrewd and intelligent - conceded on issue after issue, and campaigned with their immense political influence, for a man who ended up being the biggest liberal President since FDR.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Great_man_theory&quot;&gt;great man theory&lt;/a&gt; real? The strongest argument pro I’ve ever read is in this article. Lots of other interesting takeaways too.&lt;/p&gt;

&lt;p&gt;Also loved this story from a footnote:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I remember I was visiting India as a teenager. My uncle was talking about his son, who was studying business in the US, in the hopes that he would succeed my uncle as head of the family import/export conglomerate. My dad said something to the effect of, “You must be proud that your son is studying in the US.”
“No,” my uncle responded. “In fact, I’m a little worried.” In America, he explained, a student learns that if you can’t agree on a price with a counterparty, or if a regulator says that you’re out of line, then you sigh, say, “Ah, too bad”, and move on. But one cannot conduct business this way in India. In India, if an apparatchik of the License Raj tells you that you can’t do something, then you make a personal appeal to the man. And if that fails, you offer him cash under-the-table. If that fails, you make such appeals to his boss. And if that fails, you find some way to avoid their authority altogether. You never just accept a no at face value. You couldn’t get anything done in India if you did. Reading about how Lyndon Johnson got what he wanted - how he flattered, threatened, manipulated, and lied to get bills passed - reminded me of this story.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;if-only-i-could-begin-again&quot;&gt;&lt;a href=&quot;https://harpers.org/archive/2020/12/storm-jameson-if-only-i-could-begin-again/&quot;&gt;If only I could begin again&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A review of the autobiography of an only moderately successful early 20th century English writer.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The speaking voice, throughout the entire performance, has been described in reviews and biographical essays as ruthlessly honest; by which it is meant not that Jameson delivers a tell-all confessional, but rather that the reader can sense her grappling with something recalcitrant in the material that nonetheless draws her on. She realizes, just as we do, that there is much she does not know about this something. But we have no doubt that she is intent on telling us—&lt;em&gt;really&lt;/em&gt; telling us—as much as she does know, and that intent is what counts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What do you do when your work does not meet the standards you aspire to?
This article reviews the autobiography of Storm Jameson, an early 20th century English writer, who wrote dozens of books to little acclaim but found her metier in her final piece of work.
It might seem strange to highlight a book review as a standalone piece, but the subjects covered through talking about her life - not being as good as one wants to be at something, escaping the shadow of one’s parents, asking oneself the hard questions while also being forgiving and understanding of oneself and others - are relevant to many and thought-provoking to me.&lt;/p&gt;
&lt;h2 id=&quot;scott-and-scurvy&quot;&gt;&lt;a href=&quot;https://idlewords.com/2010/03/scott_and_scurvy.htm&quot;&gt;Scott and Scurvy&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;How the cure for scurvy was lost and the impact it had on the British attempt to reach the south pole.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Steam power had shortened travel times considerably from the age of sail, so that it was rare for sailors other than whalers to be months at sea without fresh food. Citrus juice was a legal requirement on all British vessels by 1867, but in practical terms it was becoming superfluous. 
So when the Admiralty began to replace lemon juice with an ineffective substitute in 1860, it took a long time for anyone to notice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One generally imagines science as always moving forward but as this account shows that is not actually guaranteed.
In our digital age it’s hard to imagine information being lost but it’s worth remembering that digital devices are actually the least permanent of all (hard drives don’t even last ten years) and simultaneously very complicated to create and interact with, if something cataclysmic did happen it’s basically guaranteed we would lose most of our science knowledge. I guess in the event we’d have other worries anyways.&lt;/p&gt;
&lt;h2 id=&quot;the-humanities-have-sown-the-seeds-of-their-own-destruction&quot;&gt;&lt;a href=&quot;https://web.archive.org/web/20240114171009/https://www.theatlantic.com/ideas/archive/2023/12/humanities-university-conservative-critics/676890/&quot;&gt;The Humanities Have Sown the Seeds of Their Own Destruction&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;A humanities professor laments how the increasing politicization of the humanities has caused it to change for the worse.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Instead of trying to prove that the humanities are more &lt;em&gt;economically&lt;/em&gt; useful than other majors—a tricky proposition—humanists have taken to justifying their continued existence within the academy by insisting that they are uniquely &lt;em&gt;socially&lt;/em&gt; and &lt;em&gt;politically&lt;/em&gt; useful. The emergent sales pitch is not that the humanities produce and transmit important &lt;em&gt;knowledge&lt;/em&gt;, but rather that studying the humanities promotes nebulous but nice-sounding values, such as empathy and critical thinking, that are allegedly vital to the cause of moral uplift in a multicultural democracy.
If the arc of the universe bends toward justice, some would have you believe that it is humanities departments that do the bending.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It’s interesting to view the increasing politicization as a reaction to the question what is studying the humanities good for.
Unfortunately I don’t see how one could reverse the trend. Once institutions are captured by ideologues that fill all the positions with their friends you can’t really go back unless you fire everyone which doesn’t seem realistic.
Update May 4th: Think the recent protests further validate the viewpoint of the article writer.&lt;/p&gt;

&lt;h1 id=&quot;movies-and-series&quot;&gt;Movies and Series&lt;/h1&gt;

&lt;p&gt;There were a lot of great movies in 2023 but one connected with me the most:&lt;/p&gt;

&lt;h2 id=&quot;past-lives&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt13238346&quot;&gt;Past Lives&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;As someone who moved between multiple countries as a child this cut deep. I get such a sense of melancholy from the stairs shot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/pastlives_stairs.jpg&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;:/ &lt;/p&gt;

&lt;h2 id=&quot;succession&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt7660850&quot;&gt;Succession&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;What is it that makes Succession such a masterpiece to some and only mildly interesting (“why would I watch a show where everyone is unlikeable”) to others ? I think it’s the originality of the dialogue and being able to recognize how unique it is. Of course the story has interesting twists and turns but I don’t think that is what makes the show stand out. It’s the characters and how they talk to each other. I’ve noticed that people interested in writing tend to belong to the group of Succession superfans and I don’t think that’s an accident.&lt;/p&gt;

&lt;h2 id=&quot;barry&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt5348176&quot;&gt;Barry&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;I was debating about putting this on here as there was a lot of criticism about the last season or two about it essentially changing from a dark comedy to a dark drama and I think the criticism was warranted. But the uniqueness of Barry makes it still worth elevating in my opinion: The twists, the way comedy is combined with earnestness and the unique characters who also grow over the course of the show (characters growing in a comedy imagine that!).&lt;/p&gt;

&lt;h1 id=&quot;misc&quot;&gt;Misc&lt;/h1&gt;

&lt;p&gt;I loved this.&lt;/p&gt;
&lt;h2 id=&quot;john-wick-is-so-tired-by-kyra-wilder&quot;&gt;&lt;a href=&quot;https://www.theparisreview.org/poetry/7974/john-wick-is-so-tired-kyra-wilder&quot;&gt;John Wick Is So Tired&lt;/a&gt; by &lt;a href=&quot;https://www.theparisreview.org/authors/34161/kyra-wilder&quot;&gt;Kyra Wilder&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;John Wick is so tired, but he can still throw a hatchet and hit a guy dead in the face&lt;br /&gt;
he can just split other people open with anything, with a pencil&lt;br /&gt;
because he knows what it’s like&lt;br /&gt;
because he’s tired and loves dogs and he’s cracked right open too and&lt;br /&gt;
I want to tell you to&lt;br /&gt;
look at his feet when he runs&lt;br /&gt;
the way they turn so delicately in&lt;br /&gt;
the way they’re listing slightly, his black shoes&lt;br /&gt;
the heels of them&lt;br /&gt;
their heartbreaking glissade hush-hushing across the hotel tiles&lt;br /&gt;
just look at the way he’s slipping&lt;br /&gt;
even before he soaks the floor with other people’s blood&lt;br /&gt;
I want to do push-ups like John Wick does in the morning&lt;br /&gt;
so I won’t just be sad but sad and also ripped, like&lt;br /&gt;
sad with muscles that stand out all obvious in desolate relief&lt;br /&gt;
sad where it looks like I eat clean and have expensive taste&lt;br /&gt;
I want to be sad but with a cut six-pack and&lt;br /&gt;
to drink thimblefuls of espresso out of impeccable cups and&lt;br /&gt;
I want to tell you to wait and be here and look&lt;br /&gt;
at me and also at the way John Wick is leaning&lt;br /&gt;
into those people that he’s stabbing&lt;br /&gt;
how he gets so close to them and just holds them for a second&lt;br /&gt;
how he’s so tired but he knows he has to let them go&lt;br /&gt;
and I wish you would be here and&lt;br /&gt;
we could watch John Wick together&lt;br /&gt;
and we could put our ruthless arms around each other and if we looked&lt;br /&gt;
out the window it would be all California&lt;br /&gt;
and I would lean in close and tell you that John Wick kills women like&lt;br /&gt;
he’s read feminist theory&lt;br /&gt;
which is to say I think he’s familiar with the philosophy of care and&lt;br /&gt;
you would laugh and&lt;br /&gt;
wait, look now, John Wick is riding&lt;br /&gt;
that black horse like he knows just what grief is&lt;br /&gt;
like he knows sometimes it’s killing and killing and&lt;br /&gt;
sometimes it’s just slipping in your shoes and&lt;br /&gt;
I want you to be here and&lt;br /&gt;
wait, now the camera’s right on him, just all cool colors and diaphanous mood and&lt;br /&gt;
it looks like his hand hurts like his knuckles are a little swollen but&lt;br /&gt;
he’s not saying it and&lt;br /&gt;
I want to know what you think&lt;br /&gt;
of all that blue light&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This is a list of stuff I read or saw in 2023 and really liked. I’m going to say a sentence or more about each and maybe include a quote. I’m hoping somebody who stumbles across this finds something they really like that they otherwise wouldn’t have, or someone who knows me might find an interest in common we didn’t know we had!</summary></entry><entry><title type="html">How k2 calculates the transducer loss quickly</title><link href="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html" rel="alternate" type="text/html" title="How k2 calculates the transducer loss quickly" /><published>2022-07-15T10:06:02+00:00</published><updated>2022-07-15T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html">&lt;p&gt;The new ASR toolkit k2/icefall gets great results while training models quickly. This is an explanation of how it does that by efficiently calculating the transducer loss and thereby using much less memory. Code is also shown.&lt;/p&gt;

&lt;p&gt;If you’re new to transducers read this first for an excellent introduction: &lt;a href=&quot;https://lorenlugosch.github.io/posts/2020/11/transducer/&quot;&gt;https://lorenlugosch.github.io/posts/2020/11/transducer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Consider the training scenario.  With CTC training your model will output a tensor (B,T,V) with B=’batch size’, T=’time steps’, and V=’vocab size’ (vocab size is the number of tokens your model can output). For each sample in a batch you can imagine a matrix where every traversing path represents an alignment (and CTC means you are summing across all of them). The following image shows an example with time on the x-axis, the tokens that should be output on the y. The path shown represents a particular sequence of token (diagonal) and blank (horizontal) that correspond to one alignment. Note that the probability distribution across V (the vocab) is, as long as you are at the same time-step, the same. In other words if you traverse vertically, the probability distribution across V does not change. It only changes if you traverse horizontally (by changing t).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_align.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;This shows a specific alignment which can be repesented as a path through a matrix. Taken from the preprint linked at the bottom. &lt;/p&gt;

&lt;p&gt;With transducer models this is different. Remember that the point of a transducer model is that you model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2..)&lt;/code&gt; (instead of just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt;), which means the token history matters. This means that you need an extra dimension (for the history) in the output tensor, and therefore you need a shape (B,T,U,V) with U = ‘tokens in reference’.&lt;/p&gt;

&lt;p&gt;The way I imagine it is: Instead of a matrix representing different alignments for a single example, imagine stacked matrices (one behind the other), or a (3D) lattice. The z-axis goes along the stack and corresponds to the V dimension. Time is still on the x-axis, and the tokens that should be output still on y (this is the U dimension). This means that now for every combination of x,y (T,U) you have a different probability distribution across V.&lt;/p&gt;

&lt;p&gt;This is a much larger tensor than the one CTC outputs! If the last paragraph was not clear, the main point is that the output tensor for transducers has shape (B,T,U,V) which is much larger than the output tensor of a CTC  model which has shape (B,T,V).&lt;/p&gt;

&lt;p&gt;Because it’s large, training is slow and uses a lot of memory. k2/icefall has created an approach for avoiding that!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The summary&lt;/strong&gt; is one trains a simpler model to get bounds on what alignments are possible, and then uses those bounds to decrease the size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B,T,U,V)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B,T,S,V)&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&amp;lt;&amp;lt;U&lt;/code&gt; ) and thereby efficiently train a model that models &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2..)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now in more detail.&lt;/p&gt;

&lt;p&gt;Let’s first review the normal method for creating the (B,T,U,V) tensor. This happens by  combining the encoder and decoder outputs, which have shape (B,T,C) and (B,U,C). The combination is done by adding the two (with dimension unsqueezing so the result of the addition is (B,T,U,C)), and then projecting to (B,T,U,V).&lt;/p&gt;

&lt;p&gt;Code:
&lt;img src=&quot;https://ruabraun.github.io/images/k2_joiner.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;The joiner that combines the encoder and decoder outputs. Note the dimension unsqueezing of the encoder and decoder outputs is assumed to have already happened (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/joiner.py&quot;&gt;source code&lt;/a&gt;). The first projection you see just projects to a joiner dimension, `output_linear` projects to vocab size V.&lt;/p&gt;

&lt;p&gt;Let’s just consider the case where B=1 (everything that follows holds true with B&amp;gt;1, this is just to simplify things) and we can work with the shape (T,U,V).&lt;/p&gt;

&lt;p&gt;Remember our end-goal is to calculate the logprob of all alignments by summing across all of them. This requires stepping, from start to end, through each combination of time (T) and token history (U) in the (T,U,V) tensor. The first insight is that for training we don’t need to have a distribution across all tokens in V as we have a training transcript so we know at each position (T,U) the token probability that matters: In the first row on the y-axis (equals U axis, see figure at the start) it is the first token in U, in the second row it is the second token in U and so on. These token probabilities govern the vertical transitions, the horizontal transitions are dependent on the blank token probabilities. These probabilities are what we need (instead of a distribution across all).&lt;/p&gt;

&lt;p&gt;Okay cool, but how do we actually get the logprobs we need for each position in (T,U,) without creating (T,U,V)?&lt;/p&gt;

&lt;p&gt;This is done by initially treating the encoder and decoder as separate models that act as an AM and LM by modeling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|y-1,y-2..)&lt;/code&gt;. First the encoder and decoder outputs are projected to (T,V) and (U,V), then these are matrix-multiplied with V on the inner dimension to get a matrix (T,U) with marginalized values across V.&lt;/p&gt;

&lt;p&gt;This lets us get normalized probabilities for tokens we care about: We can add together the unnormalized token log probabilities from the encoder and decoder, and subtract the marginalized value. In equation form (everything is in logprob here):&lt;/p&gt;

\[\log p(t,u,v)=l_{encoder}(t,v) + l_{decoder}(u,v) - l_{marginalized}(t,u)\]

&lt;p&gt;Because time and token history are independent of each other, we avoid having to create a (T,U,V) matrix.&lt;/p&gt;

&lt;p&gt;Logprobs are used when adding, normal probs when multiplying (the previously mentioned matrix multiply), so the implementation has some exp() and log() calls.&lt;/p&gt;

&lt;p&gt;Using the probabilities we efficiently calculate above, we create a matrix (T,U) containing the token probabilities we care about (those in the reference transcript), and additionally a matrix (T,U) with the blank probabilities (since blank transitions are always possible). We then calculate a &lt;strong&gt;simple&lt;/strong&gt; transducer loss by using both matrices to traverse across (T,U) (we need both since we need non-blank token probs for vertical and blank for horizontal transitions) to find the logprob of all alignments.&lt;/p&gt;

&lt;p&gt;Note you wouldn’t normally want to take this approach because the encoder and decoder outputs don’t get to interact before the token distribution is calculated (they’re added together after the projection to size V), as already mentioned this effectively uses separate AM and LM models (where the AM just sees audio and the LM just text). But the point of a transducer model is that you want an output &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2)&lt;/code&gt;; something that directly conditions on both audio and text!&lt;/p&gt;

&lt;p&gt;The idea here is to use the simple loss so that we can do a pruned version of the normal transducer loss. After some training with the simple loss the model will learn that some alignments (paths in (T,U)) are more or less likely than others.&lt;/p&gt;

&lt;p&gt;This information can be used to set boundaries for each time step in T, which allows doing the proper transducer loss on a subset (T,S,V) with S&amp;lt;&amp;lt;U (because we know that for a given point in time only some tokens are possible, not all in U), which takes much less memory, is faster and trains the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y_1)&lt;/code&gt; output. Effectively this means we are not considering all alignments, just those that are “reasonable” (according to the simple loss).&lt;/p&gt;

&lt;p&gt;Let’s look at the high level code (I collapsed whitespace to make things more compact).&lt;/p&gt;

&lt;p&gt;The following image shows all the steps to computing the pruned transducer loss. You can see the separate projection of encoder and decoder outputs to the vocab size, then computing a simple loss and using that (specifically the gradients) to get boundaries (here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ranges&lt;/code&gt;) for creating (B,T,S,V) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logits&lt;/code&gt;. Finally the normal transducer loss is calculated. (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/model.py#L146&quot;&gt;source code&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_losshighlevel.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking slightly deeper into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.rnnt_loss_smoothed&lt;/code&gt; we can see there are two stages: First calculating the matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;px&lt;/code&gt; (with shape (B,T,U,) of reference tokens) and matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py&lt;/code&gt; (shape (B,T,U,) of blank token), then in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutual_information_recursion&lt;/code&gt; calculating the total logprob across all alignments ( &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1152&quot;&gt;source code&lt;/a&gt; ). Despite the intimidating name the implementation of the latter is quite straightforward (for CPU at least) and just involves doing the standard dynammic programming triple for-loop (for the batch, time, token dimensions), see &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/csrc/torch/mutual_information_cpu.cu#L89&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_smoothloss.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thank you for reading, hope it is helpful, and please send an email / DM if something is unclear! :)&lt;/p&gt;

&lt;p&gt;A preprint is available with nice results and additional details: &lt;a href=&quot;https://arxiv.org/abs/2206.13236&quot;&gt;https://arxiv.org/abs/2206.13236&lt;/a&gt;&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">The new ASR toolkit k2/icefall gets great results while training models quickly. This is an explanation of how it does that by efficiently calculating the transducer loss and thereby using much less memory. Code is also shown.</summary></entry><entry><title type="html">Why I don’t like the black code formatter</title><link href="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html" rel="alternate" type="text/html" title="Why I don’t like the black code formatter" /><published>2021-11-18T10:06:02+00:00</published><updated>2021-11-18T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html">&lt;p&gt;First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.&lt;/p&gt;

&lt;p&gt;However:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;’ is cleaner than “. To me, ‘ has less visual noise than “. It’s a single stroke rather than two. Additionally, on US keyboards ‘ does not require pressing shift, “ does.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Black loves newlines, this leads to too much whitespace and makes skimming code harder&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Take a look at this image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/blackbloat_args.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This could take half the space, and then I could view twice as many arguments in one go.&lt;/p&gt;

&lt;p&gt;Black favoring to break functions with arguments up like this&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function(
    argument
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;generally leads to one seeing much less code on one’s screen than originally. This worsens readability. I can see the argument for doing this
for a function with ten arguments, but one? Nah.&lt;/p&gt;

&lt;p&gt;I might think of more issues this is it for now.&lt;/p&gt;

&lt;p&gt;2024 Update: &lt;a href=&quot;https://x.com/karpathy/status/1783538648685892026&quot;&gt;Andrej Karpathy agrees&lt;/a&gt;!&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.</summary></entry><entry><title type="html">Why the Temperature Matters for Contrastive Loss</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html" rel="alternate" type="text/html" title="Why the Temperature Matters for Contrastive Loss" /><published>2021-04-30T10:06:02+00:00</published><updated>2021-04-30T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html">&lt;p&gt;Contrastive learning has become very popular recently, see &lt;a href=&quot;https://github.com/HobbitLong/PyContrast/blob/master/AWESOME_CONTRASTIVE_LEARNING.md&quot;&gt;here&lt;/a&gt; for a good overview of recent papers.&lt;/p&gt;

&lt;p&gt;However, one thing which they all use but is not well motivated is the use of a temperature parameter in the softmax that the contrastive loss uses. I want to share why I think it matters.&lt;/p&gt;

&lt;p&gt;As a reminder this is what the equation looks like:&lt;/p&gt;

\[L=-\log \frac{e^{x_a \cdot x_p \over \tau }}{\sum_i e^{x_a \cdot x_i \over \tau}}\]

&lt;p&gt;Where the numerator contains the comparison between the positive pair \(x_a\) and \(x_p\), and the denominator has all the comparisons (all negative pairs except one). Minimizing the loss means maximizing the value in the numerator (maximizing \(x_a \cdot x_p\)) and minimizing the denominator (minimizing all \(x_a \cdot x_i\)).&lt;/p&gt;

&lt;p&gt;Now imagine \(\tau=1\). Remember the similarity function is the cosine distance, which with normalized vectors goes from -1 to 1. After applying \(exp\) the range goes from 1/e to e.&lt;/p&gt;

&lt;p&gt;Note how small that range is, and note that if the vectors were orthogonal to each other then the similarity is 0 and therefore \(e^{0}=1\).&lt;/p&gt;

&lt;p&gt;So making negative pairs orthogonal to each other is not enough to minimize the loss, because the numerator can at most be \(e\) while the denominator would be (if all pairs were orthogonal) \(\sum_i 1\). What the model would have to try and do is make the negative pairs all be antiparallel to each other. That’s obviously not ideal, if two vectors are orthogonal that should be enough of a separation.&lt;/p&gt;

&lt;p&gt;Now set the \(\tau=0.1\). Now the range goes from \(e^{-0.1}=4e-5\) to \(e^{10}=22000\) ! Incase of orthogonality the similarity after exp is still 1 of course. But now making the vectors of a negative pair orthogonal (rather than close to parallel) will result in a much larger decrease in loss, as the numerator value can be so much larger (i.e. \(22000 &amp;gt;&amp;gt; \sum_i 1\)).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Contrastive learning has become very popular recently, see here for a good overview of recent papers.</summary></entry><entry><title type="html">Changing My Mind On E2E ASR</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html" rel="alternate" type="text/html" title="Changing My Mind On E2E ASR" /><published>2021-04-12T10:06:02+00:00</published><updated>2021-04-12T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html">&lt;p&gt;I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).&lt;/p&gt;

&lt;p&gt;First let’s clarify what E2E ASR means for me: No phones, the model outputs are letters or subword units. No alignment needed and the model learns an internal LM.&lt;/p&gt;

&lt;p&gt;Particularly not using phones bothered me knowing how inconsistent pronunciations of English words are, it just seemed suboptimal to force the network to memorize that.&lt;/p&gt;

&lt;p&gt;However, I’ve had a bit of a change in thinking recently. This has come not from realizing the existence of some technical fact, but rather thinking about what the point of doing speech recognition actually is from the perspective of a consumer. There are many different use cases of course but in by far the majority of them what the end-user wants is a clean transcript. It should be easy to read, free of disfluencies, filler words and repetitions.&lt;br /&gt;
I believe traditional ASR is flawed for achieving this, and E2E ASR is not.&lt;/p&gt;

&lt;p&gt;Traditional ASR is split up into at least two components, the AM and LM. The AM is tasked with modeling the acoustics by recognizing phones. Training it means tuning the model so that it makes a decision to which phone each frame (25ms of audio) belongs to. But outputting clean transcripts means ignoring parts of the audio. So this design decision of having one model which classifies phones makes it hard for the model to learn to output clean transcripts, since the better the AM gets at modeling phones the harder it is to ignore certain sounds in the input audio.&lt;/p&gt;

&lt;p&gt;Of course, people with some ASR experience will know that in reality traditional ASR does not have such a problem with outputting clean transcripts. The LM alone should make sure only reasonable word sequences are output. And in practice the AM learns from the training data which (often) has no disfluencies, so the AM learns to map those sounds, which really correspond to some phone, to silence.&lt;/p&gt;

&lt;p&gt;Another issue is that people will regularly pronounce words differently than what you would expect, so during training the model will have to learn to map from one phone to another simply because the lexicon entry for a word will not always correspond exactly how people actually pronounce it.&lt;/p&gt;

&lt;p&gt;So if our AM is not actually modeling sounds, and we’re already forcing it do some more complicated memorization, well then why not just make it model letters?&lt;/p&gt;

&lt;p&gt;While fillers like “um” are never wanted, it’s pretty common to have “you know” missing in a transcript and those words are definitely something the model should not learn to ignore. How is a traditional ASR system supposed to deal with these sorts of errors? &lt;br /&gt;
It cannot. But a transformer-based E2E ASR system can. Thanks to the fact that it looks at much more context (basically seeing the entire input), and that it models letters/subwords directly, it can (for example) learn that a certain sound sequence said quickly at the end of certain sentences (“I really like him yaknow”) can be ignored. This is much harder for a traditional ASR system to learn because the AM does not have so much context it can look at, and even if it did the internal representations would have little to do with words - so it doesn’t know what sentence is being said - as its task is discriminating phones.&lt;/p&gt;

&lt;p&gt;So there’s two points I’m making here: (1) With the training data we have we force the AM to learn an ill-defined task (the task is actually more complicated than just learning to classify phones). (2) E2E systems that use lots of context are better suited to outputting clean transcripts because they combine the AM and LM, so they can learn to ignore sounds because of words that were said before and/or after.&lt;/p&gt;

&lt;p&gt;Clean transcripts aren’t just better for a human reader, it also makes post processing easier for any downstream ML system. And in my experience speech recognition by itself does not have that much value (from a commercial perspective), it’s by adding an NLU system on top that a lot more possibilities for use cases open up.&lt;/p&gt;

&lt;p&gt;I feel like some people have spent so much time thinking about how to model phones that they’ve forgotten that we don’t actually care about phones at all. It could be an interesting question for linguists to study, what phones do people use etc., but if your task is speech recognition classifying which sounds were in the audio should only be a means to an end. A more general conclusion is: &lt;strong&gt;Speech recognition is actually not about what a person said, rather it’s about what they meant to say.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Of course, it’s not very satisfying to just cross your fingers and train a NN to do ASR. It would be nice to somehow give the model a good prior. But with wav2vec2 I feel a good solution has been found as the performance is very good! This combined with the above perspective has changed my mind on E2E-ASR: I believe it is the way forward.&lt;/p&gt;

&lt;p&gt;edit: In hindsight one thing I want to clarify, although I keep mentioning transformer models those are not really required. What really matters is just that the model is bidirectional, therefore can see into both the past and future, and of course that the outputs are character based and the model learns an internal LM.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).</summary></entry><entry><title type="html">The do everything abstraction</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/12/The-do-everything-abstraction.html" rel="alternate" type="text/html" title="The do everything abstraction" /><published>2021-04-12T10:06:02+00:00</published><updated>2021-04-12T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/12/The-do-everything-abstraction</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/12/The-do-everything-abstraction.html">&lt;p&gt;Premature abstraction is something most people are aware of but I think a more common mistake is the “do everything” abstraction.&lt;/p&gt;

&lt;p&gt;Here’s what happens:
A coder needs code that accomplishes a task. They think “cool let me create a class/function that does what I need”.&lt;/p&gt;

&lt;p&gt;Sometime later they finish coding it up, pat themselves on the back and move on.&lt;/p&gt;

&lt;p&gt;A week/month/year later it turns out that solution contains a step (one of several) which would be useful on its own. Oh and the final step is also something that would be nice to use. At that point it would be good to refactor.&lt;/p&gt;

&lt;p&gt;The reason this happens is because it’s easy and tempting to think I need code that does “X”, without realizing that doing “X” actually consists of doing “A”, “B”, “C” and that these individual transformations are useful by themselves and therefore should be separated out.&lt;/p&gt;

&lt;p&gt;Of course this requires having some experience to judge what tasks could be needed and whether the code is for longterm use and so on.&lt;/p&gt;

&lt;p&gt;But in general this is the most common abstraction mistake, going too high-level.&lt;/p&gt;

&lt;p&gt;The opposite can also happen, forcing a user (coder) to call many low-level abstractions repeatedly although they always call the same ones. But this mistake is rarer and (usually) less disruptive.&lt;/p&gt;

&lt;p&gt;Choosing the right level of abstraction is difficult, but I think it’s something a coder should ask themselves when developing something. Who are my users? Do they do one thing 50% of the time and 4 other things the remaining 50? Or do they do one thing 95% of the time? What are the things they could reasonably want to do? Could what they want change?&lt;/p&gt;

&lt;p&gt;Judging these things is hard. It’s so easy to have a flaw in one’s logic that leads one to misjudging the validity of a usecase.&lt;/p&gt;

&lt;p&gt;Because of this difficulty it’s best to just do whatever is simple first because simple is easy to refactor. The “do-everything” abstraction may be right if it does the job and can be easily refactored to support more use-cases. But it must be simple! If one can’t make a “do-everything” abstraction without it being complex then that’s a big red warnings sign that what you are doing is bad and you should spend some time thinking about what the core tasks are (what are “A”, “B” and “C”?).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Premature abstraction is something most people are aware of but I think a more common mistake is the “do everything” abstraction.</summary></entry><entry><title type="html">Why you need (at least) a billion words to get a good language model</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" rel="alternate" type="text/html" title="Why you need (at least) a billion words to get a good language model" /><published>2021-04-01T10:06:02+00:00</published><updated>2021-04-01T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html">&lt;p&gt;2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail.&lt;/p&gt;

&lt;p&gt;I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;199989 krisenbewältigungen 2
199999 gendersensitiv 2
200002 umgehbar 2
200005 widersinnigen 2
200016 ausmehrungen 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It is kind of expected for this to happen, since it is well known that word counts follow zipf’s law, which put simply states that as you go down a table of words sorted by count, their counts decrease very rapidly, meaning a very large amount of the probability mass is covered by the top words. Here is an image (forgive the lack of axis labels please, y-axis is count in millions x-axis rank of word):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/words.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at it! (yes I know log scale bla bla, shush pedants) The counts drop extremely quickly to almost nothing.&lt;/p&gt;

&lt;p&gt;So the main point is that you need to have seen a word a couple times to know how it is used. But because in language there is a looong tail of words that are used infrequently (but are still normal enough that you do want to estimate them!) you need a &lt;strong&gt;lot&lt;/strong&gt; of text to get the counts to a reasonable level.&lt;/p&gt;

&lt;p&gt;The additional thing to realise is that because the counts are so low there will be a &lt;strong&gt;ton&lt;/strong&gt; of noise in the results. Depending on the corpus some words whose “true” probability is higher will be much lower and vice versa. This is bad.&lt;/p&gt;

&lt;p&gt;This is why you need to train on (at least) billion word corpuses to get good results. Then all those 2s will turn into 20s, and then the model can learn something.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The total count is around 400k by the way (a good chunk of them are rubbish).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail.</summary></entry><entry><title type="html">Deriving BPE from scratch</title><link href="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html" rel="alternate" type="text/html" title="Deriving BPE from scratch" /><published>2021-02-09T10:06:02+00:00</published><updated>2021-02-09T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html">&lt;p&gt;BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?&lt;/p&gt;

&lt;p&gt;Let’s create a simple ‘corpus’* (word counts are what actually matter):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;low 5
lowest 2
newer 6
wider 3
new 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Including the separator (_, appears implicitly at the end of a word) there are 11 characters. Let’s pick the 20 most common ngrams, they turn out to be (in order, but starting with unigrams):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'r_', 'we', 'ne', 'new', 'ew', 'lo', 'low']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is what BPE outputs (single character tokens are not ordered):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'ne', 'new', 'lo', 'low', 'newer_', 'low_', 'wi']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first not-unigram is the same for both (“er”), the second as well, but then the methods diverge. Note how the ngram counter picks up “we” while BPE never does. Why is that? Normally “we” would of course be a reasonable token, but for this corpus it makes more sense intuitively, considering the counts, to keep it seperate so that low + est and new + er are valid tokenisations.&lt;/p&gt;

&lt;p&gt;Because BPE redoes the count each time two tokens are merged (from which a new one is created), and only the merged token is counted - instead of also the previous two - the counts of some tokens can go down after a recount. So when “er” is created the count of “we” drops from 8 to 2! The recounting makes sense in hindsight, the counts of a token should be based on when that token could actually be used, and after choosing the “er” token, the token “we” is not possible as often as before.&lt;/p&gt;

&lt;p&gt;What if we change the k most frequent method to iteratively merge with recounts inbetween, and when recounting if two tokens were merged then we only count the merged token (not the previously split tokens)?&lt;/p&gt;

&lt;p&gt;We would have to start with some set of tokens, those could be the unigrams. Then we count ngrams that are not already in our token set, merge the most frequent, repeat. This is basically BPE, except we’re still doing a bunch of wasted computation by counting ngrams: It should be, with a little thinking, obvious that a bigram will always be at least as likely as a trigram. So rather than count all ngrams that are not already in the token set, we just count bigrams of tokens AKA pairs of tokens. This is BPE.&lt;/p&gt;

&lt;p&gt;Now the motivation for the algorithm of BPE is clear. It recounts after each merge to make sure the statistics used are up-to-date, and by considering pairs of tokens it saves a lot of computation.&lt;/p&gt;

&lt;p&gt;* Taken from Jurafsky’s chapter on BPE.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?</summary></entry></feed>