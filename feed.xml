<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://ruabraun.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ruabraun.github.io/" rel="alternate" type="text/html" /><updated>2021-11-19T00:35:48+00:00</updated><id>https://ruabraun.github.io/feed.xml</id><title type="html">Sharings</title><subtitle>Blog on speech recognition and maybe other things</subtitle><author><name>Rudolf A. Braun</name></author><entry><title type="html">The black code formatter is disgustingly ugly</title><link href="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html" rel="alternate" type="text/html" title="The black code formatter is disgustingly ugly" /><published>2021-11-18T10:06:02+00:00</published><updated>2021-11-18T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html">&lt;p&gt;fin&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">fin</summary></entry><entry><title type="html">Notes on Decoupled Contrastive Learning</title><link href="https://ruabraun.github.io/jekyll/update/2021/10/15/Notes-on-Decoupled-Contrastive-Learning.html" rel="alternate" type="text/html" title="Notes on Decoupled Contrastive Learning" /><published>2021-10-15T10:06:02+00:00</published><updated>2021-10-15T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/10/15/Notes-on-Decoupled-Contrastive-Learning</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/10/15/Notes-on-Decoupled-Contrastive-Learning.html">&lt;p&gt;Trying to summarize the recent paper &lt;a href=&quot;https://arxiv.org/abs/2110.06848&quot;&gt;Decoupled Contrastive Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The contrastive loss is defined as 
\(L_i=-\log \frac{e^{z_i^{(1)} \cdot z_i^{(2)} / \tau }}{\sum_j e^{z_i^{(1)} \cdot z_j^{(2)} / \tau}}\)&lt;/p&gt;

&lt;p&gt;for one positive pair \(z_i^{(1)}, z_i^{(2)}\). Minimizing the loss means the numerator needs to be increased (by increasing the dot product of the positive pair) while decreasing the denominator (decreasing dot product of negative pairs). As the loss goes down the ratio should go to 1.&lt;/p&gt;

&lt;p&gt;Through some rearranging they find that there’s a shared term in the gradients of \(z_i^{(1)}, z_i^{(2)}, z_j\):&lt;/p&gt;

\[q_B^{1} = 1 - \frac{e^{z_i^{(1)} \cdot z_i^{(2)} / \tau }}{\sum_j e^{z_i^{(1)} \cdot z_j^{(2)} / \tau}}\]

&lt;p&gt;The fraction is the same as in the loss. So when the ratio is close to 1 the gradient is small. The end result is that if there are no difficult negatives then the gradient will be small. Even if the positive pair is not that close to each other!
This slows learning down and is why large batch sizes were required to ensure the existence of hard negatives.&lt;/p&gt;

&lt;p&gt;They empirically show that when using small batch sizes the distribution of \(q_B\) has a strong bias towards small values («0.5). This changes as the batch is increased.&lt;/p&gt;

&lt;p&gt;They propose removing the positive pair from the denominator:
\(L_i^{DCL}=-\log \frac{e^{z_i^{(1)} \cdot z_i^{(2)} / \tau }}{\sum_{j \ne i} e^{z_i^{(1)} \cdot z_j^{(2)} / \tau}}\)&lt;/p&gt;

&lt;p&gt;And a weighting function to ensure that the gradient is larger when positive samples are further from each other.&lt;/p&gt;

&lt;p&gt;Experiments show learning is faster.&lt;/p&gt;

&lt;p&gt;Personal thoughts: I had wondered whether it might be suboptimal to include the positive pair in the denominator! Interesting to see this feeling validated! I hesitated pursuing this because allowing the ratio inside the log to be larger than 1 seemed wrong.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Trying to summarize the recent paper Decoupled Contrastive Learning.</summary></entry><entry><title type="html">ICLR 2021 Papers</title><link href="https://ruabraun.github.io/jekyll/update/2021/06/07/ICLR-2021-Papers.html" rel="alternate" type="text/html" title="ICLR 2021 Papers" /><published>2021-06-07T10:06:02+00:00</published><updated>2021-06-07T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/06/07/ICLR-2021-Papers</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/06/07/ICLR-2021-Papers.html">&lt;p&gt;Selected list of papers I liked or am interested in.&lt;/p&gt;

&lt;p&gt;Learning Better Structured Representations Using Low-rank Adaptive Label Smoothing&lt;/p&gt;

&lt;p&gt;Learning explanations that are hard to vary&lt;/p&gt;

&lt;p&gt;PMI-Masking: Principled masking of correlated spans&lt;/p&gt;

&lt;p&gt;Sharpness-aware Minimization for Efficiently Improving Generalization&lt;/p&gt;

&lt;p&gt;Knowledge distillation via softmax regression representation learning&lt;/p&gt;

&lt;p&gt;Tent: Fully Test-Time Adaptation by Entropy Minimization&lt;/p&gt;

&lt;p&gt;AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights&lt;/p&gt;

&lt;p&gt;Robust Overfitting may be mitigated by properly learned smoothening&lt;/p&gt;

&lt;p&gt;Better Fine-Tuning by Reducing Representational Collapse&lt;/p&gt;

&lt;p&gt;Estimating informativeness of samples with Smooth Unique Information&lt;/p&gt;

&lt;p&gt;On the Origin of Implicit Regularization in Stochastic Gradient Descent&lt;/p&gt;

&lt;p&gt;EigenGame: PCA as a Nash Equilibrium&lt;/p&gt;

&lt;p&gt;Rethinking Attention with Performers&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Selected list of papers I liked or am interested in.</summary></entry><entry><title type="html">Why the Temperature Matters for Contrastive Loss</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html" rel="alternate" type="text/html" title="Why the Temperature Matters for Contrastive Loss" /><published>2021-04-30T10:06:02+00:00</published><updated>2021-04-30T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html">&lt;p&gt;Contrastive learning has become very popular recently, see &lt;a href=&quot;https://github.com/HobbitLong/PyContrast/blob/master/AWESOME_CONTRASTIVE_LEARNING.md&quot;&gt;here&lt;/a&gt; for a good overview of recent papers.&lt;/p&gt;

&lt;p&gt;However, one thing which they all use but is not well motivated is the use of a temperature parameter in the softmax that the contrastive loss uses. I want to share why I think it matters.&lt;/p&gt;

&lt;p&gt;As a reminder this is what the equation looks like:&lt;/p&gt;

\[L=-\log \frac{e^{x_a \cdot x_p \over \tau }}{\sum_i e^{x_a \cdot x_i \over \tau}}\]

&lt;p&gt;Where the numerator contains the comparison between the positive pair \(x_a\) and \(x_p\), and the denominator has all the comparisons (all negative pairs except one). Minimizing the loss means maximizing the value in the numerator (maximizing \(x_a \cdot x_p\)) and minimizing the denominator (minimizing all \(x_a \cdot x_p\)).&lt;/p&gt;

&lt;p&gt;Now imagine \(\tau=1\). Remember the similarity function is the cosine distance, which with normalized vectors goes from -1 to 1. After applying \(exp\) the range goes from 1/e to e.&lt;/p&gt;

&lt;p&gt;Note how small that range is, and note that if the vectors were orthogonal to each other then the similarity is 0 and then \(e^{0}=1\).&lt;/p&gt;

&lt;p&gt;So making negative pairs orthogonal to each other is not enough to minimize the loss, because the numerator can at most be \(e\) while the denominator would be (if all pairs were orthogonal) \(\sum_i 1\). What the model would have to try and do is make the negative pairs all be antiparallel to each other. That’s obviously not ideal, if two vectors are orthogonal that should be enough of a separation.&lt;/p&gt;

&lt;p&gt;Now set the \(\tau=0.1\). Now the range goes from \(e^{-0.1}=4e-5\) to \(e^{10}=22000\) ! Incase of orthogonality the similarity after exp is still 1 of course. But now making the vectors of a negative pair orthogonal (rather than close to parallel) will result in a much larger decrease in loss, as the numerator value can be so much larger (i.e. \(22000 &amp;gt;&amp;gt; \sum_i 1\)).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Contrastive learning has become very popular recently, see here for a good overview of recent papers.</summary></entry><entry><title type="html">Changing My Mind On E2E ASR</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html" rel="alternate" type="text/html" title="Changing My Mind On E2E ASR" /><published>2021-04-12T10:06:02+00:00</published><updated>2021-04-12T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html">&lt;p&gt;I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).&lt;/p&gt;

&lt;p&gt;First let’s clarify what E2E ASR means for me: No phones, the model outputs are letters or subword units. No alignment needed and the model learns an internal LM.&lt;/p&gt;

&lt;p&gt;Particularly not using phones bothered me knowing how inconsistent pronunciations of English words are, it just seemed suboptimal to have the network be forced to memorize that.&lt;/p&gt;

&lt;p&gt;However, I’ve had a bit of a change in thinking recently. This has come not from realizing the existance of some technical fact, but rather thinking about what the point of doing speech recognition actually is from the perspective of a consumer. There are many different use cases of course but in by far the majority of them what the end-user wants is a clean transcript. It should be easy to read, free of disfluences, filler words and repetitions.&lt;br /&gt;
I believe traditional ASR is flawed for achieving this, and E2E ASR is not.&lt;/p&gt;

&lt;p&gt;Traditional ASR is split up into at least two components, the AM and LM. The AM is tasked with modeling the acoustics by recognizing phones. Training it means tuning the model so that it makes a decision to which phone each frame (25ms of audio) belongs to. But outputting clean transcripts means ignoring parts of the audio. So this design decision of having one model which classifies phones makes it hard for the model to learn to output clean transcripts, since the better the AM gets at modeling phones the harder it is to ignore certain sounds in the input audio.&lt;/p&gt;

&lt;p&gt;Of course, people with some ASR experience will know that in reality traditional ASR does not have such a problem with outputting clean transcripts. The LM alone should make sure only reasonable word sequences are output. And in practice the AM learns from the training data which (usually) has no disfluencies, so the AM learns to map those sounds, which really correspond to some phone, to silence.&lt;/p&gt;

&lt;p&gt;But if our AM is not actually modeling sounds, and we’re already forcing it do some more complicated memorization, well then why not just make it model letters?&lt;/p&gt;

&lt;p&gt;While fillers like “um” are never wanted, it’s pretty common to have “you know” missing in a transcript and those words are definitely something the model should not learn to ignore. How is a traditional ASR system supposed to deal with these sorts of errors? &lt;br /&gt;
It cannot. But a transformer-based E2E ASR system can. Thanks to the fact that it looks at much more context (basically seeing the entire input), and that it models letters/subwords directly, it can (for example) learn that a certain sound sequence said quickly at the end of certain sentences (“I really like him yaknow”) can be ignored. This is much harder for a traditional ASR system to learn because the AM does not have so much context it can look at, and even if it did the internal representations would have little to do with words - so it doesn’t know what sentence is being said - as its task is discriminating phones.&lt;/p&gt;

&lt;p&gt;So there’s two points I’m making here: (1) With the training data we have we force the AM to learn an ill-defined task (it has to learn to classify phones while classifying some of the nonsilence ones as silence). (2) E2E systems that use lots of context are better suited to outputting clean transcripts because they combine the AM and LM, so they can learn to ignore sounds because of words that were said before and/or after.&lt;/p&gt;

&lt;p&gt;Clean transcripts aren’t just better for a human reader, it also makes post processing easier for any downstream ML system. And in my experience speech recognition by itself does not have that much value (from a commercial perspective), it’s by adding an NLU system on top that a lot more possibilities for use cases open up.&lt;/p&gt;

&lt;p&gt;I feel like some people have spent so much time thinking about how to model phones that they’ve forgotten that we don’t actually care about phones at all. It could be an interesting question for linguists to study, what phones do people use etc., but if your task is speech recognition classifying which sounds were in the audio should only be a means to an end.&lt;/p&gt;

&lt;p&gt;Of course, it’s not very satisfying to just cross your fingers and train a NN to do ASR. It would be nice to somehow give the model a good prior. But with wav2vec2 I feel a good solution has been found as the performance is very good! This combined with the above perspective has changed my mind on E2E-ASR: I believe it is the way forward.&lt;/p&gt;

&lt;p&gt;edit: In hindsight one thing I want to clarify, although I keep mentioning transformer models those are not really required. What really matters is just that the model is bidirectional, therefore can see into both the past and future, and of course that the outputs are character based and the model learns an internal LM.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).</summary></entry><entry><title type="html">Why you need a billion words to get a good language model</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" rel="alternate" type="text/html" title="Why you need a billion words to get a good language model" /><published>2021-04-01T10:06:02+00:00</published><updated>2021-04-01T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html">&lt;p&gt;WIP&lt;/p&gt;

&lt;p&gt;I have a text corpus with nearly 90 million words. Seems enough to get a decent model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and see what is at the nth position.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;199989 krisenbewältigungen 2
199999 gendersensitiv 2
200002 umgehbar 2
200005 widersinnigen 2
200016 ausmehrungen 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Some of these are completely legitimite words. Good we have them in our vocabulary! But (!) the thing to realize is that their counts are very low. We will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It is kind of expected for this to happen, since it is well known that word counts follow zipf’s law, which put very simply states that as you go down a table of words sorted by count, their counts decrease very rapidly, meaning a very large amount of the probability mass is covered by the top words. Here is an image&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/words.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at it! (yes I know log scale bla bla, shush pedants) Very quickly the counts drop to nothing. The additional thing to realise is that because the counts are so low there will be a &lt;strong&gt;ton&lt;/strong&gt; of noise in the results. Depending on the corpus some words whose “true” probability is higher will be much lower and vice versa. This is bad.&lt;/p&gt;

&lt;p&gt;But the main point is that you need to have seen a word a couple times to know how it is used. But because in language there is a looong tail of words that are used infrequently (but are still normal enough that you do want to estimate them!) you need a &lt;strong&gt;lot&lt;/strong&gt; of text to get the counts to a reasonable level.&lt;/p&gt;

&lt;p&gt;This is why you need to train on billion word corpuses to get good results. Then all those 2s will turn into 20s, and then the model can learn something.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The total count is around 400k by the way (a good chunk of them are rubbish).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">WIP</summary></entry><entry><title type="html">Why does BPE work?</title><link href="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html" rel="alternate" type="text/html" title="Why does BPE work?" /><published>2021-02-09T10:06:02+00:00</published><updated>2021-02-09T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html">&lt;p&gt;WIP&lt;/p&gt;

&lt;p&gt;BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?&lt;/p&gt;

&lt;p&gt;Let’s create a simple ‘corpus’* (word counts are what actually matter):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;low 5
lowest 2
newer 6
wider 3
new 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Including the separator (_, appears implicitly at the end of a word) there are 11 characters. Let’s pick the 20 most common ngrams, they turn out to be (in order, but starting with unigrams):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'r_', 'we', 'ne', 'new', 'ew', 'lo', 'low']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is what BPE outputs (single character tokens are not ordered):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'ne', 'new', 'lo', 'low', 'newer_', 'low_', 'wi']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first not-unigram is the same for both (“er”), the second as well, but then the methods diverge. Note how the ngram counter picks up “we” while BPE never does. Why is that? Normally “we” would of course be a reasonable token, but for this corpus it makes more sense intuitively, considering the counts, to keep it seperate so that low + est and new + er are valid tokenisations.&lt;/p&gt;

&lt;p&gt;Because BPE redoes the count each time two tokens are merged (from which a new one is created), and only the merged token is counted - instead of also the previous two - the counts of some tokens can go down after a recount. So when “er” is created the count of “we” drops from 8 to 2! The recounting makes sense in hindsight, the counts of a token should be based on when that token could actually be used, and after choosing the “er” token, the token “we” is not possible as often as before.&lt;/p&gt;

&lt;p&gt;What if we change the k most frequent method to iteratively merge with recounts inbetween, and when recounting if two tokens were merged then we only count the merged token (not the previously split tokens)?&lt;/p&gt;

&lt;p&gt;We would have to start with some set of tokens, those could be the unigrams. Then we count ngrams that are not already in our token set, merge the most frequent, repeat. This is basically BPE, except we’re still doing a bunch of wasted computation by counting ngrams: It should be, with a little thinking, obvious that a bigram will always be at least as likely as a trigram. So rather than count all ngrams that are not already in the token set, we just count bigrams of tokens AKA pairs of tokens. This is BPE.&lt;/p&gt;

&lt;p&gt;Now the motivation for the algorithm of BPE is clear. It recounts after each merge to make sure the statistics used are up-to-date, and by considering pairs of tokens it saves a lot of computation.&lt;/p&gt;

&lt;p&gt;* Taken from Jurafsky’s chapter on BPE.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">WIP</summary></entry><entry><title type="html">On WER in ASR</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html" rel="alternate" type="text/html" title="On WER in ASR" /><published>2020-11-27T10:06:02+00:00</published><updated>2020-11-27T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html">&lt;p&gt;This post will be about the python-based &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;tool (“texterrors”)&lt;/a&gt; I created for getting error metrics (relevant for ASR). It is split in two parts: 
First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking &lt;a href=&quot;#newtool&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;wer-calculation-recap&quot;&gt;WER calculation recap&lt;/h2&gt;

&lt;p&gt;Given a hypothesized sentence the Word-Error-Rate is defined as the number of insertion, deletion and substitution errors with the respect to a reference sentence, divided by the count of words in the reference. Insertion/Deletion is defined from the perspective of the model, so for example if the model outputs a word when it shouldn’t, that’s an insertion error.&lt;/p&gt;

&lt;p&gt;To find out the types of errors one has to align the hypothesis to the reference, this is typically done by creating a cost matrix (where the cost of a cell depends on the transition cost plus the lowest cost from the left, top or diagonal cells) and backtracing from the end (bottom right) to the start (top left) to find the alignment. Example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;third&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;second&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;third&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Taking a horizontal or vertical (deletion/insertion) transition costs 1. Taking a diagonal to position &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; costs 1 if word i != word j, else 0. “second” is not recognized by the model (a deletion error) which is why at the end the cost in the bottom right is 1. Notice that if all we want to know is the WER, you can actually just take that value (1) and divide by the count of reference words (3) to get the WER (33.3%).&lt;/p&gt;

&lt;p&gt;However, usually one wants to know how many errors of each type there are, and to do that one needs to get the alignment to then count them. This requires backtracing which is done by finding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transition_cost&lt;/code&gt; = &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; is either to the left, diagonal or above the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt; (then repeat the process until the start is reached).&lt;br /&gt;
This can be ambiguous, for example consider two sentences “first word in sentence” and “first ward sentence”. There are different ways to align this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Clearly the pair “word”/”ward” is a more likely substitution error than “in”/”ward”, but this alignment method has no way of identifying that since the cumulative costs are the same, see cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This has no impact on the WER, as in both cases there are two (one insertion/deletion depending on which sentence is considered the reference, one substitution), but from the perspective of analysing what sorts of errors a model is making - which sorts of words the model is failing to recognize (deletions), which words are confused with each other (substitutions) - having a different alignment will change the result.&lt;/p&gt;

&lt;h2 id=&quot;getting-better-alignments-with-texterrors-&quot;&gt;Getting better alignments with “texterrors” &lt;a name=&quot;newtool&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;As just mentioned, the traditional method of alignment (which we need to do to get statistics for the different error types) leads to ambiguous alignments with no sensible way of resolving them. &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;“texterrors”&lt;/a&gt; is meant to be a tool for getting detailed error metrics. As these are sensitive to suboptimal alignments it uses a smarter method: Instead of having a cost of 1 for the substitution cost (in the cost matrix), it incorporates the character edit distance between the words compared.&lt;/p&gt;

&lt;p&gt;Concretely, the substitution cost is set to the edit distance between two words divided by the maximum edit distance possible (length of the longer word), so it is a value between 0 and 1 (slightly more complicated in practice, you’ll see later why). That way alignments will be favoured when words which are similar to each other are substitution errors instead of deletion/insertion errors.&lt;/p&gt;

&lt;p&gt;Example cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.25&lt;/td&gt;
      &lt;td&gt;1.125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As one can see here, the best (lowest) cumulative cost is achieved by pairing “word”/”ward”.&lt;/p&gt;

&lt;p&gt;One should be aware, this method can result in a higher WER. 
In the below example a normal WER calculation would do a one-to-one mapping and arrive at a WER of 66.67\%.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But character aware alignment would result in the following alignment:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;-&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This results in a WER of 83.3\% because of the extra insertion and deletion. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; has an option to turn character-aware alignment off (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-no-chardiff&lt;/code&gt;) to get identical results with kaldi. But the difference is small for a normal sized test set, and obviously without the feature the alignments and therefore statistics like the most frequent substitution error will not be as accurate!&lt;/p&gt;

&lt;p&gt;There is still one last issue to deal with, see this example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The alignment will end up being&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This happens here because using the character edit distance leads to the substitution cost often being smaller than the insertion/deletion cost and therefore alignments with more substitutions are favored.&lt;br /&gt;
This sort of bad alignment can also happen with normal costs of 1/1/1 for ins/del/sub (consider the above example, as the costs are the same for different errors it depends on the implementation which alignment is chosen, you can think of it as random). That’s why such tools, when meant to be used for getting detailed error metrics, will increase the substitution cost to improve alignments. We can do the same: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; will after the previously mentioned calculation times the cost by 1.5. This will lead to the following cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And the following (obviously superior) alignment.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;-&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; also supports ctm files where the time stamps for reference and hypothesis words will be used for alignment. If you &lt;em&gt;really&lt;/em&gt; care about having the most accurate alignment possible, that is what you should use! But it won’t make a big difference. :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;Here is a link to it.&lt;/a&gt; Thank you for reading.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This post will be about the python-based tool (“texterrors”) I created for getting error metrics (relevant for ASR). It is split in two parts: First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking here.</summary></entry><entry><title type="html">Doing non-standard stuff with kaldi decoding</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html" rel="alternate" type="text/html" title="Doing non-standard stuff with kaldi decoding" /><published>2020-11-06T10:06:02+00:00</published><updated>2020-11-06T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html">&lt;p&gt;Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found &lt;a href=&quot;https://github.com/RuABraun/fst-util&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;adding-words-to-hclg&quot;&gt;Adding words to HCLG&lt;/h2&gt;

&lt;p&gt;Some scripts you will need can be found &lt;a href=&quot;https://github.com/idiap/icassp-oov-recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This method requires you to use a monophone model. Additionally, your language model needs to have been trained with pocolm, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--limit-unk-history&lt;/code&gt; option, and there should have been some OOVs in the training text.&lt;/p&gt;

&lt;p&gt;For simplicity, the modification is done on a graph without self-loops. So you need to modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/mkgraph.sh&lt;/code&gt; and comment L167: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm $dir/HCLGa.fst $dir/Ha.fst 2&amp;gt;/dev/null || true&lt;/code&gt; because we will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Inside the graph dir where the HCLG is there is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt;. You need to assign IDs to the new words you’re adding and append these to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt; file (these should be larger than the existing ones obviously).&lt;/p&gt;

&lt;p&gt;Assuming all this is ready you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;script/compose_hcl.sh&lt;/code&gt; to create the HCL from a lexicon of the OOV words you want to add. Check the script for the input arguments, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final.mdl&lt;/code&gt;, isym is phones osym words. Notice it uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_lfst.py&lt;/code&gt; so you need the fst wrapper installed. There is one hardcoded parameter on L25, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;303&lt;/code&gt;, see &lt;a href=&quot;https://groups.google.com/g/kaldi-help/c/jL8VnwKGRWs/m/-Pe29-G9AgAJ&quot;&gt;here&lt;/a&gt; for what’s about. You can set it to any number larger than the existing phone IDs.&lt;/p&gt;

&lt;p&gt;After calling the script and creating the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCL.fst&lt;/code&gt; you use the fst wrapper to modify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from wrappedfst import WrappedFst
fst = WrappedFst('HCLGa.fst')
ifst = WrappedFst('HCL.fst')
unk_id =  # unk symbol
fst.replace_single(unk_id, ifst)
fst.write('HCLGa_new.fst')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then add the self-loops (check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkgraph.sh&lt;/code&gt; for how to do that) and you are done. Replace an existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLG.fst&lt;/code&gt; with the new version and you can run decoding as you would normally.&lt;/p&gt;

&lt;h2 id=&quot;no-hclg-just-g&quot;&gt;No HCLG just G&lt;/h2&gt;

&lt;p&gt;Imagine you have an acoustic model that you created via pytorch, and you want to then evaluate how well it does at recognizing phonemes (TIMIT for example). You have a phone LM that you want to incorporate. I’m going to show you how you can do that with kaldi.&lt;/p&gt;

&lt;p&gt;Usually kaldi assumes that you have an HMM graph (the H in HCLG) and possibly context dependency as well (the C), then there’s also the lexicon etc. If you had a pytorch AM that you had trained on the PDF-ids (the targets you get by calling ali-to-pdf on the alignments), you would then use that by saving the loglikelihood output of your NN to a file, then passing that file and the final.mdl, HCLG.fst to latgen-faster-mapped, which will do decoding and generate a lattice (from which you could then take the best path for example).&lt;/p&gt;

&lt;p&gt;But if you want to just model phones, you don’t care about the HCL at all. You really just want to use a G that will act as the phone LM.&lt;/p&gt;

&lt;p&gt;So the first issue to get around is to avoid doing any mapping from transition IDs (the typical input symbols of the HCLG) to PDF-IDs. We can do that by creating a file latgen-faster.cc with the only difference being we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaled&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaledMapped&lt;/code&gt; in the code.&lt;/p&gt;

&lt;p&gt;Then you take the arpa LM that you have trained, convert it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arpa2fst&lt;/code&gt;, convert the #0 arcs to &lt;eps&gt; and add self-loops to states that have incoming arcs with input labels not equal 0. The point of that is when the AM predicts a sequence &quot;ae ae ae b b&quot; over five frames, the LM should not care about repetitions, we want it to only score the transition &quot;ae&quot; to &quot;b&quot;, and of course we don't want repetitions in the output transcript. Having self-loops fixes that.&lt;/eps&gt;&lt;/p&gt;

&lt;p&gt;Then you call&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;latgen-faster --acoustic-scale=1.0 --beam=13 --determinize-lattice=false G.fst ark,t:loglikelihoods-file&amp;gt; ark:- | lattice-best-path etc.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to get a hypothesis that you can compare to the reference to get the PER!&lt;/p&gt;

&lt;p&gt;If you wanted to recognize words, all you’d need to do is create a L.fst, compose that with the G.fst (which is then a word LM) and use the LG.fst.&lt;/p&gt;

&lt;h2 id=&quot;g-with-keyword-list&quot;&gt;G with keyword list&lt;/h2&gt;

&lt;p&gt;Imagine you just want to recognize a small number of words and you don’t want to have any sort of prior regarding which one is more likely.&lt;/p&gt;

&lt;p&gt;To do that you just have to create an FST with a single state, and self-loops where each self-loop has a keyword as the input and output symbol.&lt;/p&gt;

&lt;p&gt;In pseudo-code&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fst = Fst()
state = fst.add_state()
fst.set_start(state)
fst.set_final(state)
for keyword in keywords
    fst.add_arc(state, state, keyword, keyword, cost)
fst.write(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The self-loops should have costs, what these should be you will have to find out experimentally.&lt;/p&gt;

&lt;p&gt;You probably will have to create the lang/ folder again. To do that just call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_lang.sh&lt;/code&gt; with the dict/ of keywords and with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--phone-symbol-table&lt;/code&gt; option set to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phones.txt&lt;/code&gt; that you trained the AM model with.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found here.</summary></entry><entry><title type="html">First post: Ark and scp files in kaldi</title><link href="https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi.html" rel="alternate" type="text/html" title="First post: Ark and scp files in kaldi" /><published>2020-10-04T12:40:02+00:00</published><updated>2020-10-04T12:40:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/10/04/First-post-ark-and-scp-files-kaldi.html">&lt;p&gt;Both file types are structured by having keys and a value for each key.&lt;/p&gt;

&lt;h1 id=&quot;scp-files&quot;&gt;Scp files&lt;/h1&gt;

&lt;p&gt;In scp files, usually ending with the “.scp” suffix, the first column (aka field) is the key (usually an utterance ID). The rest of the line is treated as a pointer to data (the pointer is the value). 
What this means is that the scp file never contains any data, it just contains something which points to the data you will eventually use. See for example the wav.scp file which will look something like this usually:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wav-id-1 /path/to/file-1.wav
wav-id-2 /path/to/file-2.wav
wav-id-3 /path/to/file-3.wav
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It can be a file path, but can also be more complex like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wav-id-1 sox /path/to/file-1.wav -r 8k - |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Kaldi’s code is written so that it will recognize and execute the above command (starting from after the key), which results in the wav file being read with a sampling rate of 8kHz. This is a convenient way of doing the resampling on the fly (instead of resampling and having to waste space storing a file somewhere).&lt;/p&gt;

&lt;p&gt;Another file one will see a lot when using kaldi is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats.scp&lt;/code&gt; file which looks like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;utt-id-1 /path/to/file-1.ark:44
utt-id-2 /path/to/file-1.ark:760
utt-id-3 /path/to/file-1.ark:1520
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The number after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt; is a byte offset.&lt;/p&gt;

&lt;h1 id=&quot;ark-files&quot;&gt;Ark files&lt;/h1&gt;

&lt;p&gt;Instead of the value being a pointer to the data, in ark files the value &lt;strong&gt;is&lt;/strong&gt; the data. Usually this means it is in binary format, so you can’t visually inspect it, however kaldi has binaries for converting from binary to text format. So for example take the ark file from the feats.scp example above and call:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats ark:/path/to/file-1.ark ark,t:/path/to/file-1.ark.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark,t&lt;/code&gt; means that the values in the output will be written in textual format, so you can open the file and see what the values for the (example) MFCC coefficients are for each frame.&lt;/p&gt;

&lt;p&gt;Another example of an ark style file, although it doesn’t have the “.ark” suffix, is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; file you will find in kaldi data folders. The first field/column is the utterance ID, the rest of the line is the words belonging to that utterance. The words are the data.&lt;/p&gt;

&lt;p&gt;You can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; to concatenate ark files. And kaldi binaries accepts commands and wildcards in the arguments, so this is valid: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark:gunzip -c lat.*.gz|&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Kaldi has a script for subsetting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; files you may find useful called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/subset_scp.pl&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When you call kaldi binaries you often have to prefix with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark:&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp:&lt;/code&gt; to tell kaldi what type of file to expect. You can then mix types, for example I could use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/subset_scp.pl&lt;/code&gt; to subset &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats.scp&lt;/code&gt; and create a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;feats_subset.scp&lt;/code&gt;, and then copy a subset of the feature data by calling&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats scp:feats_subset.scp ark:copied_feats_subset.ark
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Notice how the input argument is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt;, so kaldi knows the input file is pointing to the data I want to use, and the output is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt;, so kaldi knows to actually write the data pointed to to a new file.&lt;/p&gt;

&lt;p&gt;It’s not possible to create just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; file from an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt; file, you have to create both the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ark&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scp&lt;/code&gt; files. So you have to do something like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;copy-feats scp:/path/to/feats.ark ark,scp:copied_feats.ark,copied_feats.scp 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details check out the official documentation of &lt;a href=&quot;http://kaldi-asr.org/doc/io.html&quot;&gt;kaldi io&lt;/a&gt;. Probably the section “from a command-line perspective” is more relevant to you.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Both file types are structured by having keys and a value for each key.</summary></entry></feed>