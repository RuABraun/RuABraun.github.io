<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://ruabraun.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ruabraun.github.io/" rel="alternate" type="text/html" /><updated>2024-03-04T03:37:00+00:00</updated><id>https://ruabraun.github.io/feed.xml</id><title type="html">Sharings</title><author><name>Rudolf A. Braun</name></author><entry><title type="html">Favourites of 2023</title><link href="https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023.html" rel="alternate" type="text/html" title="Favourites of 2023" /><published>2024-02-26T10:06:02+00:00</published><updated>2024-02-26T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2024/02/26/favourites-of-2023.html">&lt;p&gt;This is a list of stuff I read or saw in 2023 and really liked. I’m going to say a sentence or more about each and maybe include a quote. I’m hoping somebody who stumbles across this finds something they really like that they otherwise wouldn’t have, or someone who knows me might find an interest in common we didn’t know we had!&lt;/p&gt;

&lt;p&gt;I suggest skimming and using the bolded bylines to pick out what might interest.&lt;/p&gt;
&lt;h2 id=&quot;books&quot;&gt;Books&lt;/h2&gt;

&lt;p&gt;This was a (first?) year of mostly non-fiction for me. It was great! Do want to read more fiction this year though.&lt;/p&gt;

&lt;h3 id=&quot;storyworthy&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/37786022&quot;&gt;Storyworthy&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;How to tell a good story.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Your story must reflect change over time. A story cannot simply be a series of remarkable events. You must start out as one version of yourself and end as something new.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I loved the practicality of this. He takes his own stories and breaks them down. He goes into detail and provides tons of examples. 
It made me understand that I usually do not do a good job of telling a story and I appreciated having that realization.&lt;/p&gt;

&lt;p&gt;It also made me think there is an opposition between telling an engaging story and explaining something (like in a paper or technical report) or communicating facts efficiently. With the latter you never want to surprise a listener, everything should be obvious because of what you said previously, if that’s not the case then you missed a step in the logic or made an error which is why the listener is surprised.
But for storytelling surprise is a key tool to engage the listener, so depending on what you’re talking about you really should have a different structure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=U9v0O0oEmpQ&quot;&gt;He’s got videos of his stories on youtube, check it out!&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;path-to-power&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/en/book/show/86524&quot;&gt;Path to Power&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Lyndon B Johnson’s early life and the first roughly ten years of his political career.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In every election in which he ran—not only in college, but thereafter—he displayed a willingness to do whatever was necessary to win: a willingness so complete that even in the generous terms of political morality, it amounted to amorality.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is hands-down one of the most engaging books I’ve ever read that also goes into incredible detail on things that happened a century ago and one might initially think uninteresting. But the narrative works. It is interesting. It helps of course that LBJ is a singular character. His portrayal is at minimum unsympathetic, yet you can’t help but root for him when he goes to the greatest lengths to help his constituents and when he tries to gain more power (with the hindsight knowledge of him enacting so many liberal policies later). I wonder if someone wrote a similarly detailed biography of say, Obama, would it be close to as interesting?&lt;/p&gt;

&lt;h3 id=&quot;how-to-be-the-greatest-improviser-on-earth&quot;&gt;&lt;a href=&quot;https://www.goodreads.com/book/show/30364117-how-to-be-the-greatest-improviser-on-earth&quot;&gt;How To Be The Greatest Improviser On Earth&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Everything you need to know to do improv.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another way to think about the top of the scene is with the Taoist principles of yin and yang. Yin is passive, patient, empathetic, malleable. It is akin to a “yes.” Yang is active, assertive, decisive, altering. It is akin to an “and.” After a series of small moves that simply confirm information without adding a lot, it’s time for a “yang” move: make a decision, add some information. But once someone makes a big “yang” move, then it’s time for a simple “yin” response. Confirm and unpack what was just added. Even a beat of silence is a good idea.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I believe the improv mindset is beneficial for more than just improv. So while a niche book I’m including it here as it really is excellent. This book gives you everything you need to know for improv, literally, and yet it’s so compact! I really felt after reading it I just needed to go practice more, what I should be doing was made clear.&lt;/p&gt;

&lt;p&gt;Something I’ve begun to think as a consequence of doing improv and hanging out with comedy people is how there is a dichotomy between people in that one group likes talking about facts and the other thinks that the most boring conversation possible.&lt;/p&gt;

&lt;h2 id=&quot;articles&quot;&gt;Articles&lt;/h2&gt;

&lt;p&gt;Only started saving these towards the end of the year so the list is short.&lt;/p&gt;
&lt;h3 id=&quot;lessons-from-the-years-of-lyndon-johnson-by-robert-caro&quot;&gt;&lt;a href=&quot;https://www.dwarkeshpatel.com/p/lyndon-johnson&quot;&gt;Lessons from The Years of Lyndon Johnson by Robert Caro&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;A collections of thoughts and lessons from reading Robert Caro’s series on LBJ by Dwarkesh Patel.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For the first 55 years of his life, he convinced men of tremendous intellect and drive of their own that he was the scion of the Southern cause - that the only hope for that cause was to make Lyndon Johnson President. So much so that when Lyndon Johnson was Majority Leader, the Southern Senators, who at the time controlled all the important committees in Congress, allowed Johnson to pass liberal legislation they scorned because they believed it would help him become President. And, they believed, as President, Johnson would govern as a conservative, especially on the issue of race. The tragic irony and betrayal here is hard to over-emphasize. These powerful conservative men - who, while being ignorant and backwards in certain ways, were famously shrewd and intelligent - conceded on issue after issue, and campaigned with their immense political influence, for a man who ended up being the biggest liberal President since FDR.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Great_man_theory&quot;&gt;great man theory&lt;/a&gt; real? The strongest argument pro I’ve ever read is in this article. Lots of other interesting takeaways too.&lt;/p&gt;

&lt;p&gt;Also loved this story from a footnote:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I remember I was visiting India as a teenager. My uncle was talking about his son, who was studying business in the US, in the hopes that he would succeed my uncle as head of the family import/export conglomerate. My dad said something to the effect of, “You must be proud that your son is studying in the US.”
“No,” my uncle responded. “In fact, I’m a little worried.” In America, he explained, a student learns that if you can’t agree on a price with a counterparty, or if a regulator says that you’re out of line, then you sigh, say, “Ah, too bad”, and move on. But one cannot conduct business this way in India. In India, if an apparatchik of the License Raj tells you that you can’t do something, then you make a personal appeal to the man. And if that fails, you offer him cash under-the-table. If that fails, you make such appeals to his boss. And if that fails, you find some way to avoid their authority altogether. You never just accept a no at face value. You couldn’t get anything done in India if you did. Reading about how Lyndon Johnson got what he wanted - how he flattered, threatened, manipulated, and lied to get bills passed - reminded me of this story.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;if-only-i-could-begin-again&quot;&gt;&lt;a href=&quot;https://harpers.org/archive/2020/12/storm-jameson-if-only-i-could-begin-again/&quot;&gt;If only I could begin again&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;A review of the autobiography of an only moderately successful early 20th century English writer.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The speaking voice, throughout the entire performance, has been described in reviews and biographical essays as ruthlessly honest; by which it is meant not that Jameson delivers a tell-all confessional, but rather that the reader can sense her grappling with something recalcitrant in the material that nonetheless draws her on. She realizes, just as we do, that there is much she does not know about this something. But we have no doubt that she is intent on telling us—&lt;em&gt;really&lt;/em&gt; telling us—as much as she does know, and that intent is what counts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;What do you do when your work does not meet the standards you aspire to?
This article reviews the autobiography of Storm Jameson, an early 20th century English writer, who wrote dozens of books to little acclaim but found her metier in her final piece of work.
It might seem strange to highlight a book review as a standalone piece, but the subjects covered through talking about her life - not being as good as one wants to be at something, escaping the shadow of one’s parents, asking oneself the hard questions while also being forgiving and understanding of oneself and others - are relevant to many and therefore, I think, thought-provoking.&lt;/p&gt;
&lt;h3 id=&quot;scott-and-scurvy&quot;&gt;&lt;a href=&quot;https://idlewords.com/2010/03/scott_and_scurvy.htm&quot;&gt;Scott and Scurvy&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;How the cure for scurvy was lost and the impact it had on the British attempt to reach the south pole.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Steam power had shortened travel times considerably from the age of sail, so that it was rare for sailors other than whalers to be months at sea without fresh food. Citrus juice was a legal requirement on all British vessels by 1867, but in practical terms it was becoming superfluous. 
So when the Admiralty began to replace lemon juice with an ineffective substitute in 1860, it took a long time for anyone to notice.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One generally imagines science as always moving forward but as this account shows that is not actually guaranteed.&lt;/p&gt;
&lt;h3 id=&quot;the-humanities-have-sown-the-seeds-of-their-own-destruction&quot;&gt;&lt;a href=&quot;https://web.archive.org/web/20240114171009/https://www.theatlantic.com/ideas/archive/2023/12/humanities-university-conservative-critics/676890/&quot;&gt;The Humanities Have Sown the Seeds of Their Own Destruction&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;A humanities professor laments how the humanities have changed for the worse.&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If the arc of the universe bends toward justice, some would have you believe that it is humanities departments that do the bending.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is so nice to see it acknowledged that the humanities are in some sense useless (though that doesn’t mean it is worthless!). I especially appreciated the call-out of participants writing about race or gender in past novels as attempting to fight a proxy-war for progressive causes. This is the real talk we need!&lt;/p&gt;

&lt;h2 id=&quot;movies-and-series&quot;&gt;Movies and Series&lt;/h2&gt;

&lt;p&gt;Need to still watch a ton of highly rated 2023 movies but here goes:&lt;/p&gt;

&lt;h3 id=&quot;past-lives&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt13238346&quot;&gt;Past Lives&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;As someone who moved between multiple countries as a child this cut deep. I get such a sense of melancholy from the stairs shot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/pastlives_stairs.jpg&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;:/ &lt;/p&gt;

&lt;h3 id=&quot;succession&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt7660850&quot;&gt;Succession&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;What is it that makes Succession such a masterpiece to some and only mildly interesting (“why would I watch a show where everyone is unlikeable”) to others ? I think it’s the originality of the dialogue and being able to recognize how unique it is. Of course the story has interesting twists and turns but I don’t think that is what makes the show stand out. It’s the characters and how they talk to each other. I’ve noticed that people interested in writing tend to belong to the group of Succession superfans and I don’t think that’s an accident.&lt;/p&gt;

&lt;h3 id=&quot;barry&quot;&gt;&lt;a href=&quot;https://www.imdb.com/title/tt5348176&quot;&gt;Barry&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;I was debating about putting this on here as there was a lot of criticism about the last season or two about it essentially changing from a dark comedy to a dark drama, and I think the criticism was warranted. But the uniqueness of Barry makes it still worth elevating in my opinion: The twists, the way comedy is combined with earnestness and the unique characters who also grow over the course of the show (characters growing in a comedy imagine that!).&lt;/p&gt;

&lt;h2 id=&quot;misc&quot;&gt;Misc&lt;/h2&gt;

&lt;p&gt;I loooved this.&lt;/p&gt;
&lt;h3 id=&quot;john-wick-is-so-tired-by-kyra-wilder&quot;&gt;&lt;a href=&quot;https://www.theparisreview.org/poetry/7974/john-wick-is-so-tired-kyra-wilder&quot;&gt;John Wick Is So Tired&lt;/a&gt; by &lt;a href=&quot;https://www.theparisreview.org/authors/34161/kyra-wilder&quot;&gt;Kyra Wilder&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;John Wick is so tired, but he can still throw a hatchet and hit a guy dead in the face&lt;br /&gt;
he can just split other people open with anything, with a pencil&lt;br /&gt;
because he knows what it’s like&lt;br /&gt;
because he’s tired and loves dogs and he’s cracked right open too and&lt;br /&gt;
I want to tell you to&lt;br /&gt;
look at his feet when he runs&lt;br /&gt;
the way they turn so delicately in&lt;br /&gt;
the way they’re listing slightly, his black shoes&lt;br /&gt;
the heels of them&lt;br /&gt;
their heartbreaking glissade hush-hushing across the hotel tiles&lt;br /&gt;
just look at the way he’s slipping&lt;br /&gt;
even before he soaks the floor with other people’s blood&lt;br /&gt;
I want to do push-ups like John Wick does in the morning&lt;br /&gt;
so I won’t just be sad but sad and also ripped, like&lt;br /&gt;
sad with muscles that stand out all obvious in desolate relief&lt;br /&gt;
sad where it looks like I eat clean and have expensive taste&lt;br /&gt;
I want to be sad but with a cut six-pack and&lt;br /&gt;
to drink thimblefuls of espresso out of impeccable cups and&lt;br /&gt;
I want to tell you to wait and be here and look&lt;br /&gt;
at me and also at the way John Wick is leaning&lt;br /&gt;
into those people that he’s stabbing&lt;br /&gt;
how he gets so close to them and just holds them for a second&lt;br /&gt;
how he’s so tired but he knows he has to let them go&lt;br /&gt;
and I wish you would be here and&lt;br /&gt;
we could watch John Wick together&lt;br /&gt;
and we could put our ruthless arms around each other and if we looked&lt;br /&gt;
out the window it would be all California&lt;br /&gt;
and I would lean in close and tell you that John Wick kills women like&lt;br /&gt;
he’s read feminist theory&lt;br /&gt;
which is to say I think he’s familiar with the philosophy of care and&lt;br /&gt;
you would laugh and&lt;br /&gt;
wait, look now, John Wick is riding&lt;br /&gt;
that black horse like he knows just what grief is&lt;br /&gt;
like he knows sometimes it’s killing and killing and&lt;br /&gt;
sometimes it’s just slipping in your shoes and&lt;br /&gt;
I want you to be here and&lt;br /&gt;
wait, now the camera’s right on him, just all cool colors and diaphanous mood and&lt;br /&gt;
it looks like his hand hurts like his knuckles are a little swollen but&lt;br /&gt;
he’s not saying it and&lt;br /&gt;
I want to know what you think&lt;br /&gt;
of all that blue light&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This is a list of stuff I read or saw in 2023 and really liked. I’m going to say a sentence or more about each and maybe include a quote. I’m hoping somebody who stumbles across this finds something they really like that they otherwise wouldn’t have, or someone who knows me might find an interest in common we didn’t know we had!</summary></entry><entry><title type="html">How k2 calculates the transducer loss quickly</title><link href="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html" rel="alternate" type="text/html" title="How k2 calculates the transducer loss quickly" /><published>2022-07-15T10:06:02+00:00</published><updated>2022-07-15T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html">&lt;p&gt;The new ASR toolkit k2/icefall gets great results while training models quickly. This is an explanation of how it does that by efficiently calculating the transducer loss and thereby using much less memory. Code is also shown.&lt;/p&gt;

&lt;p&gt;If you’re new to transducers read this first for an excellent introduction: &lt;a href=&quot;https://lorenlugosch.github.io/posts/2020/11/transducer/&quot;&gt;https://lorenlugosch.github.io/posts/2020/11/transducer/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Consider the training scenario.  With CTC training your model will output a tensor (B,T,V) with B=’batch size’, T=’time steps’, and V=’vocab size’ (vocab size is the number of tokens your model can output). For each sample in a batch you can imagine a matrix where every traversing path represents an alignment (and CTC means you are summing across all of them). The following image shows an example with time on the x-axis, the tokens that should be output on the y. The path shown represents a particular sequence of token (diagonal) and blank (horizontal) that correspond to one alignment. Note that the probability distribution across V (the vocab) is, as long as you are at the same time-step, the same. In other words if you traverse vertically, the probability distribution across V does not change. It only changes if you traverse horizontally (by changing t).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_align.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;This shows a specific alignment which can be repesented as a path through a matrix. Taken from the preprint linked at the bottom. &lt;/p&gt;

&lt;p&gt;With transducer models this is different. Remember that the point of a transducer model is that you model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2..)&lt;/code&gt; (instead of just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt;), which means the token history matters. This means that you need an extra dimension (for the history) in the output tensor, and therefore you need a shape (B,T,U,V) with U = ‘tokens in reference’.&lt;/p&gt;

&lt;p&gt;The way I imagine it is: Instead of a matrix representing different alignments for a single example, imagine stacked matrices (one behind the other). The z-axis goes along the stack and corresponds to the V dimension. Time is still on the x-axis, and the tokens that should be output still on y (this is the U dimension). This means that now for every combination of x,y (T,U) you have a different probability distribution across V.&lt;/p&gt;

&lt;p&gt;This is a much larger tensor than the one CTC outputs! If the last paragraph was not clear, the main point is that the output tensor for transducers has shape (B,T,U,V) which is much larger than the output tensor of a CTC  model which has shape (B,T,V).&lt;/p&gt;

&lt;p&gt;Because it’s large, training is slow and uses a lot of memory. k2/icefall has created an approach for avoiding that!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The summary&lt;/strong&gt; is one trains a simpler model to get bounds on what alignments are possible, and then uses those bounds to decrease the size of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B,T,U,V)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(B,T,S,V)&lt;/code&gt; (with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&amp;lt;&amp;lt;U&lt;/code&gt; ) and thereby efficiently train a model that models &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2..)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now in more detail.&lt;/p&gt;

&lt;p&gt;Let’s first review the normal method for creating the (B,T,U,V) tensor. This happens by  combining the encoder and decoder outputs, which have shape (B,T,C) and (B,U,C). The combination is done by adding the two (with dimension unsqueezing so the result of the addition is (B,T,U,C)), and then projecting to (B,T,U,V).&lt;/p&gt;

&lt;p&gt;Code:
&lt;img src=&quot;https://ruabraun.github.io/images/k2_joiner.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;The joiner that combines the encoder and decoder outputs. Note the dimension unsqueezing of the encoder and decoder outputs is assumed to have already happened (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/joiner.py&quot;&gt;source code&lt;/a&gt;). The first projection you see just projects to a joiner dimension, `output_linear` projects to vocab size V.&lt;/p&gt;

&lt;p&gt;Let’s just consider the case where B=1 (everything that follows holds true with B&amp;gt;1, this is just to simplify things) and we can work with the shape (T,U,V).&lt;/p&gt;

&lt;p&gt;Remember our end-goal is to calculate the logprob of all alignments by summing across all of them. This requires stepping, from start to end, through each combination of time (T) and token history (U) in the (T,U,V) tensor. The first insight is that for training we don’t need to have a distribution across all tokens in V as we have a training transcript so we know at each position (T,U) the token probability that matters: In the first row on the y-axis (equals U axis, see figure at the start) it is the first token in U, in the second row it is the second token in U and so on. These token probabilities govern the vertical transitions, the horizontal transitions are dependent on the blank token probabilities.&lt;/p&gt;

&lt;p&gt;Okay cool, but how do we actually get the logprobs we need for each position in (T,U,) without creating (T,U,V)?&lt;/p&gt;

&lt;p&gt;This is done by initially treating the encoder and decoder as separate models that act as an AM and LM by modeling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|y-1,y-2..)&lt;/code&gt;. First the encoder and decoder outputs are projected to (T,V) and (U,V), then these are matrix-multiplied with V on the inner dimension to get a matrix (T,U) with marginalized values across V.&lt;/p&gt;

&lt;p&gt;This lets us get normalized probabilities for tokens we care about: We can add together the unnormalized token log probabilities from the encoder and decoder, and subtract the marginalized value. In equation form (everything is in logprob here):&lt;/p&gt;

\[\log p(t,u,v)=l_{encoder}(t,v) + l_{decoder}(u,v) - l_{marginalized}(t,u)\]

&lt;p&gt;Because time and token history are independent of each other, we avoid having to create a (T,U,V) matrix.&lt;/p&gt;

&lt;p&gt;Logprobs are used when adding, normal probs when multiplying (the previously mentioned matrix multiply), so the implementation has some exp() and log() calls.&lt;/p&gt;

&lt;p&gt;Using the probabilities we efficiently calculate above, we create a matrix (T,U) containing the token probabilities we care about (those in the reference transcript), and additionally a matrix (T,U) with the blank probabilities (since blank transitions are always possible). We then calculate a &lt;strong&gt;simple&lt;/strong&gt; transducer loss by using both matrices to traverse across (T,U) (we need both since we need non-blank token probs for vertical and blank for horizontal transitions) to find the logprob of all alignments.&lt;/p&gt;

&lt;p&gt;Note you wouldn’t normally want to take this approach because the encoder and decoder outputs don’t get to interact before the token distribution is calculated (they’re added together after the projection to size V), as already mentioned this effectively uses separate AM and LM models (where the AM just sees audio and the LM just text). But the point of a transducer model is that you want an output &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y-1,y-2)&lt;/code&gt;; something that directly conditions on both audio and text!&lt;/p&gt;

&lt;p&gt;The idea here is to use the simple loss so that we can do a pruned version of the normal transducer loss. After some training with the simple loss the model will learn that some alignments (paths in (T,U)) are more or less likely than others.&lt;/p&gt;

&lt;p&gt;This information can be used to set boundaries for each time step in T, which allows doing the proper transducer loss on a subset (T,S,V) with S&amp;lt;&amp;lt;U (because we know that for a given point in time only some tokens are possible, not all in U), which takes much less memory, is faster and trains the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x,y_1)&lt;/code&gt; output. Effectively this means we are not considering all alignments, just those that are “reasonable” (according to the simple loss).&lt;/p&gt;

&lt;p&gt;Let’s look at the high level code (I collapsed whitespace to make things more compact).&lt;/p&gt;

&lt;p&gt;The following image shows all the steps to computing the pruned transducer loss. You can see the separate projection of encoder and decoder outputs to the vocab size, then computing a simple loss and using that (specifically the gradients) to get boundaries (here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ranges&lt;/code&gt;) for creating (B,T,S,V) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logits&lt;/code&gt;. Finally the normal transducer loss is calculated. (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/model.py#L146&quot;&gt;source code&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_losshighlevel.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Looking slightly deeper into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.rnnt_loss_smoothed&lt;/code&gt; we can see there are two stages: First calculating the matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;px&lt;/code&gt; (with shape (B,T,U,) of reference tokens) and matrix &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;py&lt;/code&gt; (shape (B,T,U,) of blank token), then in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mutual_information_recursion&lt;/code&gt; calculating the total logprob across all alignments ( &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/k2/rnnt_loss.py#L1152&quot;&gt;source code&lt;/a&gt; ). Despite the intimidating name the implementation of the latter is quite straightforward (for CPU at least) and just involves doing the standard dynammic programming triple for-loop (for the batch, time, token dimensions), see &lt;a href=&quot;https://github.com/k2-fsa/k2/blob/master/k2/python/csrc/torch/mutual_information_cpu.cu#L89&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/k2_smoothloss.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Thank you for reading, hope it is helpful, and please send an email / DM if something is unclear! :)&lt;/p&gt;

&lt;p&gt;A preprint is available with nice results and additional details: &lt;a href=&quot;https://arxiv.org/abs/2206.13236&quot;&gt;https://arxiv.org/abs/2206.13236&lt;/a&gt;&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">The new ASR toolkit k2/icefall gets great results while training models quickly. This is an explanation of how it does that by efficiently calculating the transducer loss and thereby using much less memory. Code is also shown.</summary></entry><entry><title type="html">A comparison of fairseq, speechbrain, k2 for ASR</title><link href="https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html" rel="alternate" type="text/html" title="A comparison of fairseq, speechbrain, k2 for ASR" /><published>2022-06-26T10:06:02+00:00</published><updated>2022-06-26T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html">&lt;p&gt;This gives a high level overview of fairseq, speechbrain and k2. We will go over the code base structure, what the training loop looks like, the procedure for training a model, and I will name stuff I liked or disliked.&lt;/p&gt;

&lt;p&gt;Whenever I refer to files, I assume you are starting from the root folder of the repo.&lt;/p&gt;

&lt;h1 id=&quot;fairseq&quot;&gt;fairseq&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/fairseq&quot;&gt;Fairseq&lt;/a&gt; is not just meant for ASR. It’s for many different modeling tasks, including language modeling, translation, masked LM pretraining, summarization and text to speech.&lt;/p&gt;

&lt;p&gt;Training models and doing inference is done via command-line scripts (found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq_cli/&lt;/code&gt;). The training loop and data processing code is unified when possible. As a consequence the code is generally a bit more abstract, you cannot look at just one file if you want to read and understand the training loop.
The codebase can be seen as split into four different parts.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq_cli/&lt;/code&gt; contains CLI scripts for data processing, model training and evaluation.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/&lt;/code&gt; (except for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/tasks/&lt;/code&gt;) is like a library of common models, data processing or utility classes and functions&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/tasks/&lt;/code&gt; has files which contain code specific to a modeling task&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/&lt;/code&gt; has the documentation and scripts for training and evaluating models related to facebook’s papers and corpuses&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When training you pass a “task” argument (when calling the CLI scripts) which ensures the appropriate code is run for whatever task you are doing. Many modifications to a task, like using a different model, can done by changing a command-line argument or using a config file to redefine a default (the config is based on hydra+omegaconf). Models defined in fairseq (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/models/&lt;/code&gt;) will have a decorator enabling you to specify it using one keyword (making it easy to change).&lt;/p&gt;

&lt;p&gt;There are several tasks related to speech recognition: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech_to_text&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech_recognition&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio_pretraining&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;audio_finetuning&lt;/code&gt;. The latter two are for wav2vec[2]. It seems they are planning to unify all the previously mentioned ASR related ones to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speech_to_text&lt;/code&gt;, but this is still WIP.&lt;/p&gt;

&lt;p&gt;The data format depends on the task and tends to favor simplicity. For example for wav2vec pretraining you just need a .tsv file which has on the first line a root path, and every line afterwards has a path to an audio file and its sample count. That’s it. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;examples/*&lt;/code&gt; there always is an explanation for how to get to the data format needed.&lt;/p&gt;

&lt;p&gt;I find fairseq to be a good toolkit for reproducing results from facebook’s papers - I was surprised how easy it was to do the pretraining for wav2vec models - and convenient if you want to toy around with a research idea that is slightly different from what already exists. A lot of small changes that you might want to do can be done fairly easily. If you’re doing something significantly different from what already exists that could get a lot harder and you will likely have to end up learning about how the entire fairseq repo ties together to achieve that. But I do think once you get it it can be quite nice to work with. One downside is you can’t rely much on support from the maintainers; github issues are rarely responded to. It also is not very suited for using in production (but to be fair that is not what it’s made for).&lt;/p&gt;

&lt;p&gt;Let’s skim over the training loop, we start in &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/train.py#L180&quot;&gt;fairseq_cli/train.py&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while epoch_itr.next_epoch_idx &amp;lt;= max_epoch:
    [..]
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;With train defined in the same file and then calling &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/train.py#L312&quot;&gt;trainer.train_step()&lt;/a&gt;, the most important of which looks like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;logger.info(&quot;Start iterating over samples&quot;)
for i, samples in enumerate(progress):
     [..]
	 log_output = trainer.train_step(samples)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The dataloader is wrapped inside of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;progress&lt;/code&gt;.  The trainer object is defined in &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/trainer.py&quot;&gt;fairseq/trainer.py&lt;/a&gt; and its &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/trainer.py#L824&quot;&gt;train_step()&lt;/a&gt; calls the task specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_step&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i, sample in enumerate(samples):  # delayed update loop
    [..]
	loss, sample_size_i, logging_output = self.task.train_step(
		sample=sample,
		model=self.model,
		criterion=self.criterion,
		optimizer=self.optimizer,
		update_num=self.get_num_updates(),
		ignore_grad=is_dummy_batch,
		**extra_kwargs,
	)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;samples&lt;/code&gt; will have a length bigger 1 only if gradient accumulation is used.&lt;/p&gt;

&lt;p&gt;A lot of tasks actually don’t have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_step()&lt;/code&gt; defined and just use the generic one defined in &lt;a href=&quot;https://github.com/facebookresearch/fairseq/blob/main/fairseq/tasks/fairseq_task.py#L490&quot;&gt;fairseq/tasks/fairseq_task.py&lt;/a&gt; (the parent class from which all other tasks inherit).&lt;/p&gt;

&lt;p&gt;One uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq-hydra-train&lt;/code&gt; to train a model when one has a config and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq-train&lt;/code&gt; if you’re willing to specify all the arguments on the commandline. This is an example call to finetune a wav2vec model:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fairseq-hydra-train task.data=/work2/rudolf/wav2vec/ch-de/22-08-05/manifest/ model.w2v_path=/work1/rudolf/wav2vec_model.pt checkpoint.save_dir=/work2/rudolf/wav2vec/ch-de/22-08-05/model-a --config-dir $PWD --config-name finetuning.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Most of the options are defined inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;finetuning.yaml&lt;/code&gt; , for example the task, this looks like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;task:
  _name: audio_finetuning
  data: ???
  normalize: true 
  labels: bpe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The name specifies which task defined in  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/tasks/&lt;/code&gt; will be used, all other options are specific to that. In this case they specify that bpe labels will be used for training and the audio files will be normalized after loading. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;???&lt;/code&gt; means the user should specify this, which we did in the above commandline with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task.data=...&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fairseq/examples/&lt;/code&gt; one can always find the commandline necessary to train a recipe.&lt;/p&gt;

&lt;h1 id=&quot;speechbrain&quot;&gt;speechbrain&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/speechbrain/speechbrain&quot;&gt;Speechbrain&lt;/a&gt; is specifically for speech processing tasks, they have an impressive array of examples where they get good results (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;recipes/&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;The recipe implementations are less unified than those in fairseq, but there is still a lot of shared code. The core training loop is unified in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; class (found in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speechbrain/core.py&lt;/code&gt;), the data processing code is specified in each recipe.&lt;/p&gt;

&lt;p&gt;Speechbrain uses a powerful yaml config (&lt;a href=&quot;https://github.com/speechbrain/HyperPyYAML&quot;&gt;HyperPyYaml&lt;/a&gt;) which is used not only to define hyperparameters but also to define data processing related things like the data loading sampler, the loss, the optimizer and the model. A simple one like an 3-layer MLP you could define right in the config like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;enc: !new:speechbrain.lobes.models.VanillaNN.VanillaNN
   input_shape: [null, null, 1024]
   activation: torch.nn.LeakyRelu
   dnn_blocks: 2
   dnn_neurons: 1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A complicated model would be defined by refering to the speechbrain implementation (for example a transformer) for example:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;encoder: !new:speechbrain.lobes.models.transformer.Transformer.TransformerEncoder
	d_model: 768
	num_layers: 12
	nhead: 8
	d_ffn: 3072
	dropout: 0.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note that the class will be instantiated when the config is loaded, so you don’t have to write any code to do that. To use a different model you can just modify the config.&lt;/p&gt;

&lt;p&gt;Sometimes speechbrain’s models (e.g. the VanillaNN above) use an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input_shape&lt;/code&gt; to allow them to infer the output shape (by running the model). This is implemented by having those models inherit from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speechbrain.nnet.containers.Sequential&lt;/code&gt; (&lt;a href=&quot;https://github.com/speechbrain/speechbrain/blob/424e7921531b0ea6523557ef0fd6ca249936bd26/speechbrain/nnet/containers.py#L18&quot;&gt;code link&lt;/a&gt;) which has an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;append&lt;/code&gt; method which you use when successively adding layers (this will then pass the input shape and check it works). If this doesn’t make sense it’s okay, it’s a detail.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; class is initialized with this yaml config, and will hold a reference to the model, the loss, the optimizer, but not the dataset/dataloaders. In contrast to other toolkits speechbrain defines much more in the config.&lt;/p&gt;

&lt;p&gt;How the model is used is specified by overriding two methods (of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; class) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_forward&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_objectives&lt;/code&gt;. The former defines how to get from the data to the outputs you need to compute the loss. You then use the latter to define how to compute the loss from those outputs (it’s called automatically after the former). Sometimes the implementation will be extremely simple: In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_forward&lt;/code&gt; you call the model and in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_objectives&lt;/code&gt; you call the loss defined in your config.&lt;/p&gt;

&lt;p&gt;The data is typically stored in CSV format (with filepaths pointing to audio data) which you load to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DynamicItemDataset&lt;/code&gt; like (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sb&lt;/code&gt;  is  the speechbrain package):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(csv_path=hparams[&quot;train_csv&quot;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;in your recipe you then specify (via a custom “pipeline” construct) how to go from the data in the CSV to the data you want available in a batch. This can look like this (&lt;a href=&quot;https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/ASR/CTC/train_with_wav2vec.py#L234&quot;&gt;code link&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@sb.utils.data_pipeline.takes(&quot;wav&quot;)
@sb.utils.data_pipeline.provides(&quot;sig&quot;)
def audio_pipeline(wav):
	sig = sb.dataio.dataio.read_audio(wav)
	return sig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 3. Define text pipeline:
@sb.utils.data_pipeline.takes(&quot;wrd&quot;)
@sb.utils.data_pipeline.provides(&quot;wrd&quot;, &quot;tokens_list&quot;, &quot;tokens_bos&quot;)
def text_pipeline(wrd):
	yield wrd
	tokens_list = tokenizer.encode_as_ids(wrd)
	yield tokens_list
	tokens_bos = torch.LongTensor([hparams[&quot;bos_index&quot;]] + (tokens_list))
	yield tokens_bos
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You attach it to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DynamicItemDataset&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sb.dataio.dataset.add_dynamic_item(train_data, text_pipeline)
sb.dataio.dataset.add_dynamic_item(train_data, audio_pipeline)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The “takes” are columns available in the CSV file. The “provides” you can access in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_forward&lt;/code&gt;  like this (&lt;a href=&quot;https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/ASR/CTC/train_with_wav2vec.py#L35&quot;&gt;code link&lt;/a&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def compute_forward(self, batch, stage):
	batch = batch.to(self.device)
	wavs, wav_lens = batch.sig
	tokens_bos, _ = batch.tokens_bos
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Some magic that happens is when the batch is created padding is automatically done and therefore &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch.sig&lt;/code&gt; also returns a second tensor which specifies the relative length of each sample (relative to the longest sample, which will have the relative length of 1.0).&lt;/p&gt;

&lt;p&gt;I like the config and how powerful it is.  I think they’ve made some good decisions about how to allow people to make the changes they need to write code specific to their problem. I have though sometimes been bitten by behaviour that I did not expect. It definitely is a framework, something that expects you to do things in a certain way. I have also come across a number of bugs and the code is very object oriented. I think it’s kind of similar to pytorch lightning (the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain&lt;/code&gt; class is similar to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LightningModule&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Everything under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speechbrain/&lt;/code&gt; is like a library of everything you could need, recipes will always use models, dataloaders and utility functions from there.&lt;/p&gt;

&lt;p&gt;To train a model you call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Brain.fit&lt;/code&gt;, which looks like this (&lt;a href=&quot;https://github.com/speechbrain/speechbrain/blob/b1934fa38d9a073eb105e6ec9ffa2119cd4142bf/speechbrain/core.py#L1075&quot;&gt;code link&lt;/a&gt;) (all code snippets are shortened):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if not (
	isinstance(train_set, DataLoader)
	or isinstance(train_set, LoopedLoader)
):
	train_set = self.make_dataloader(train_set, stage=sb.Stage.TRAIN, 
         **train_loader_kwargs)
[..]
self.on_fit_start()
[..]
for epoch in epoch_counter:
	self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
	self._fit_valid(valid_set=valid_set, epoch=epoch, enable=enable)
[..]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;There is some logic for creating the dataloader if necessary and then the training loop is started. 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_fit_train&lt;/code&gt; (&lt;a href=&quot;https://github.com/speechbrain/speechbrain/blob/b1934fa38d9a073eb105e6ec9ffa2119cd4142bf/speechbrain/core.py#L982&quot;&gt;code link&lt;/a&gt;) encapsulates training for one epoch:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[..]
self.on_stage_start(Stage.TRAIN, epoch)
[..]
with tqdm(train_set, initial=self.step, dynamic_ncols=True, disable=not enable) as t:
	for batch in t:
		[..]
		loss = self.fit_batch(batch)
		self.avg_train_loss = self.update_average(loss, self.avg_train_loss)
		[..]
self.on_stage_end(Stage.TRAIN, self.avg_train_loss, epoch)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fit_batch&lt;/code&gt; (&lt;a href=&quot;https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/core.py#L842&quot;&gt;code link&lt;/a&gt;), it’s here that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_forward&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_objectives&lt;/code&gt; are used and the optimization step is taken:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;should_step = self.step % self.grad_accumulation_factor == 0
# Managing automatic mixed precision
if self.auto_mix_prec:
	[..]
else:
	outputs = self.compute_forward(batch, Stage.TRAIN)
	loss = self.compute_objectives(outputs, batch, Stage.TRAIN)
	with self.no_sync(not should_step):
		(loss / self.grad_accumulation_factor).backward()

	if should_step:
		if self.check_gradients(loss):
			self.optimizer.step()
		self.optimizer.zero_grad()
		self.optimizer_step += 1
self.on_fit_batch_end(batch, outputs, loss, should_step)
return loss.detach().cpu()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each recipe in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;speechbrain/recipes/&lt;/code&gt; contains a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train*.py&lt;/code&gt; that you use for calling training (there is no single binary like with fairseq). A training command will look like (from the recipe dir):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train_with_wav2vec.py hparams/train_with_wav2vec.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train_speaker_embeddings.py hparams/train_x_vectors.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;k2icefalllhotse&quot;&gt;k2/icefall/lhotse&lt;/h1&gt;

&lt;p&gt;This is by far the newest of the three tools I’m comparing here. I wrote k2/icefall/lhotse because there are three different repositories you would make use of: &lt;a href=&quot;https://github.com/k2-fsa/k2&quot;&gt;k2&lt;/a&gt; for ragged tensors, lattice related operations and loss functions; &lt;a href=&quot;https://github.com/lhotse-speech/lhotse&quot;&gt;lhotse&lt;/a&gt; for everything related to data such as downloading a corpus, feature extraction and dynamic batching; &lt;a href=&quot;https://github.com/k2-fsa/icefall&quot;&gt;icefall&lt;/a&gt; for the recipes (for example for transducer models trained on LibriSpeech) which use lhotse and k2.&lt;/p&gt;

&lt;p&gt;There is no unified training loop, each recipe will have its own straight-forward implementation making use of lhotse and k2 to do training and inference.&lt;/p&gt;

&lt;p&gt;lhotse has a commandline interface which is quite powerful. It lets you extract features, get duration statistics and convert kaldi style data dirs to lhotse’s format among other things. See &lt;a href=&quot;https://lhotse.readthedocs.io/en/latest/cuts.html&quot;&gt;lhotse documentation&lt;/a&gt; for an explanation on lhotse’s data constructs like Manifests, Cuts (basically an utterance) and CutSets. An example for getting duration statistics of a CutSet manifest:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ lhotse cut describe cuts_example.jsonl.gz
Cuts count: 16028
Total duration (hh:mm:ss): 17:53:29
Speech duration (hh:mm:ss): 17:53:29 (100.0%)
Duration statistics (seconds):
mean    4.0
std     4.1
min     0.2
25%     1.4
50%     2.6
75%     5.1
99%     20.5
99.5%   23.3
99.9%   27.6
max     30.5
Recordings available: 16028
Features available: 16028
Supervisions available: 16028
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also do all this via code of course. An example, this:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Converts from a kaldi style datadir to a recording and supervision set (lhotse constructs)&lt;/li&gt;
  &lt;li&gt;Creates a CutSet object&lt;/li&gt;
  &lt;li&gt;Trims the recordings to when the supervisions (transcripts) are (the utterance transcripts have timestamps associated) (now equivalent to a set of utterances)&lt;/li&gt;
  &lt;li&gt;Extracts features&lt;/li&gt;
  &lt;li&gt;Writes a manifest to a file (lhotse’s data format on disk):
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;recording_set, supervision_set, _ = load_kaldi_data_dir(kdata, 16000, num_jobs=4)
cuts = CutSet.from_manifests(recordings=recording_set, supervisions=supervision_set)
cuts = cuts.trim_to_supervisions()
cuts = cuts.compute_and_store_features(Fbank(), storage_path=outf + '-feats', num_jobs=6, storage_type=LilcomChunkyWriter)
cuts.to_file(outf + '.jsonl.gz')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think lhotse has a very nice interface and definitely plan on basing all my speech recipes around it. I will say though the lhotse codebase has a lot of indirection going on (decorators everywhere) making it hard to reason about the code and debugging sometimes a pain.&lt;/p&gt;

&lt;p&gt;k2 is more low-level. It’s for example used for &lt;a href=&quot;https://k2-fsa.github.io/k2/core_concepts/index.html#ragged-arrays&quot;&gt;RaggedTensors&lt;/a&gt; like this where y is a list of lists:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;y = k2.RaggedTensor(y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And for FSTs. k2 saves FSTs in pytorch’s (actually pickling) format so you load them from disk like this:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dct = torch.load(args.decode_graph, map_location=device)
decoding_graph = k2.Fsa.from_dict(dct)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Otherwise it contains many constructs for loss functions, FST operations and decoders: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.ctc_loss&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.rnnt_loss&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.rnnt_loss_pruned&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.intersect&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.top_sort&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k2.RnntDecodingStream&lt;/code&gt;  are some examples. You will only use it yourself if you want to do something with the loss function, the decoder or an FST. It’s not necessary to understand if you just want to run recipes and change the data and/or model.&lt;/p&gt;

&lt;p&gt;Now about icefall. Regarding the repo layout, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;egs/&lt;/code&gt; is for the recipes (don’t understand why they didn’t move away from the opaque term kaldi that used) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icefall/&lt;/code&gt; basically contains utility functions. Note, unlike in speechbrain and fairseq, this dir &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icefall/&lt;/code&gt; (the dir where all the python library code goes) contains not much code as the majority is kept recipe specific, for example the training loop. This makes the recipes easy to read and modify as there is no framework involved (it’s just normal pytorch) and minimal coupling: You don’t have to jump to ten different places to follow the logic.&lt;/p&gt;

&lt;p&gt;So for example &lt;a href=&quot;https://github.com/k2-fsa/icefall/tree/master/egs/librispeech/ASR/pruned_transducer_stateless2&quot;&gt;egs/librispeech/ASR/pruned_transducer_stateless2&lt;/a&gt; contains the files &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;beam_search.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conformer.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decode.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;decoder.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;joiner.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optim.py&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaling.py&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt;. Basically everything is in there, and only the “boring” stuff (like word symbol tables, checkpointing, logging) is relegated to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icefall/&lt;/code&gt; library.&lt;/p&gt;

&lt;p&gt;Sidenote, there’s some interesting new ideas regarding the basics of DNNs in icefall. The main one is that instead of using LayerNorm/BatchNorm it makes sense to have a differentiable scale parameter for the weight matrices. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaling.py&lt;/code&gt; file contains new implementations of all basic modules that follow this new approach and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optim.py&lt;/code&gt; has a custom optimizer for these new modules.&lt;/p&gt;

&lt;p&gt;Back to the main thread: Everything important is inside the recipe folder. The training loop could not look more normal (that’s good!). After the usual setup (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/train.py#L987&quot;&gt;code link&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[..]
for epoch in range(params.start_epoch, params.num_epochs):
	[..]
	train_one_epoch(params=params, model=model, optimizer=optimizer, 
        scheduler=scheduler, sp=sp, train_dl=train_dl, valid_dl=valid_dl, 
        scaler=scaler,
tb_writer=tb_writer, world_size=world_size, rank=rank)
	[..]
	save_checkpoint(params=params, model=model, optimizer=optimizer, 
        scheduler=scheduler, sampler=train_dl.sampler, scaler=scaler, rank=rank)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train_one_epoch&lt;/code&gt; looks like (&lt;a href=&quot;https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/train.py#L760&quot;&gt;code link&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for batch_idx, batch in enumerate(train_dl):
	[..]
	loss, loss_info = compute_loss(params=params, model=model, sp=sp, batch=batch,
is_training=True, warmup=(params.batch_idx_train / params.model_warm_step))
	[..]
	if batch_idx &amp;gt; 0 and batch_idx % params.valid_interval == 0:
		valid_loss = compute_validation_loss(params=params, model=model, sp=sp,
			valid_dl=valid_dl, world_size=world_size)
	[..]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One neat feature is that if a training crashes or you ctrl+c it, the current batch will automatically be dumped to disk for you to inspect it.&lt;/p&gt;

&lt;p&gt;I like the overall structure, and appreciate the focus on delivering something first and thinking about unifying it later. The results are very good.
However, because this is all very new it’s in a state of flux. Things change and sometimes there are bugs. On the bright side everyone involved is very responsive and if something doesn’t work one can get help very quickly.&lt;/p&gt;

&lt;p&gt;Training is similar to speechbrain with the recipe specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; files. Unfortunately there are not many READMEs, but the top of a file usually has documentation. For “egs/librispeech/ASR/pruned_transducer_stateless2” the training command looks like:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PYTHONPATH=/path/to/icefall ./pruned_transducer_stateless2/train.py \
--world-size 4 \
--num-epochs 30 \
--start-epoch 0 \
--use-fp16 1 \
--exp-dir pruned_transducer_stateless2/exp \
--full-libri 1 \
--max-duration 550
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note setting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYTHONPATH&lt;/code&gt; is necessary as icefall is not pip-installable, and instead one uses the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PYTHONPATH&lt;/code&gt; so that the imports (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from icefall.utils import bla&lt;/code&gt;) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; work.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This gives a high level overview of fairseq, speechbrain and k2. We will go over the code base structure, what the training loop looks like, the procedure for training a model, and I will name stuff I liked or disliked.</summary></entry><entry><title type="html">Why I don’t like the black code formatter</title><link href="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html" rel="alternate" type="text/html" title="Why I don’t like the black code formatter" /><published>2021-11-18T10:06:02+00:00</published><updated>2021-11-18T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/11/18/black-is-disgusting.html">&lt;p&gt;First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.&lt;/p&gt;

&lt;p&gt;However:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;’ is cleaner than “&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To me, ‘ has less visual noise than “. It’s a single stroke rather than two. Additionally, on US keyboards ‘ does not require pressing shift, “ does.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Black loves newlines, this leads to too much whitespace and makes skimming code harder&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Take a look at this image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/blackbloat_args.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This could take half the space, and then I could view twice as many arguments in one go.&lt;/p&gt;

&lt;p&gt;Black favoring to break functions with arguments up like this&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function(
    argument
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;generally leads to one seeing much less code on one’s screen than originally. This worsens readability. I can see the argument for doing this
for a function with ten arguments, but one? Nah.&lt;/p&gt;

&lt;p&gt;I might think of more issues this is it for now.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">First off I understand the need for a tool to avoid teammates bickering with each other, and if I joined a team using black I would follow their rules.</summary></entry><entry><title type="html">Why the Temperature Matters for Contrastive Loss</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html" rel="alternate" type="text/html" title="Why the Temperature Matters for Contrastive Loss" /><published>2021-04-30T10:06:02+00:00</published><updated>2021-04-30T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/30/Why-The-Temperature-Matters-For-Contrastive-Loss.html">&lt;p&gt;Contrastive learning has become very popular recently, see &lt;a href=&quot;https://github.com/HobbitLong/PyContrast/blob/master/AWESOME_CONTRASTIVE_LEARNING.md&quot;&gt;here&lt;/a&gt; for a good overview of recent papers.&lt;/p&gt;

&lt;p&gt;However, one thing which they all use but is not well motivated is the use of a temperature parameter in the softmax that the contrastive loss uses. I want to share why I think it matters.&lt;/p&gt;

&lt;p&gt;As a reminder this is what the equation looks like:&lt;/p&gt;

\[L=-\log \frac{e^{x_a \cdot x_p \over \tau }}{\sum_i e^{x_a \cdot x_i \over \tau}}\]

&lt;p&gt;Where the numerator contains the comparison between the positive pair \(x_a\) and \(x_p\), and the denominator has all the comparisons (all negative pairs except one). Minimizing the loss means maximizing the value in the numerator (maximizing \(x_a \cdot x_p\)) and minimizing the denominator (minimizing all \(x_a \cdot x_i\)).&lt;/p&gt;

&lt;p&gt;Now imagine \(\tau=1\). Remember the similarity function is the cosine distance, which with normalized vectors goes from -1 to 1. After applying \(exp\) the range goes from 1/e to e.&lt;/p&gt;

&lt;p&gt;Note how small that range is, and note that if the vectors were orthogonal to each other then the similarity is 0 and therefore \(e^{0}=1\).&lt;/p&gt;

&lt;p&gt;So making negative pairs orthogonal to each other is not enough to minimize the loss, because the numerator can at most be \(e\) while the denominator would be (if all pairs were orthogonal) \(\sum_i 1\). What the model would have to try and do is make the negative pairs all be antiparallel to each other. That’s obviously not ideal, if two vectors are orthogonal that should be enough of a separation.&lt;/p&gt;

&lt;p&gt;Now set the \(\tau=0.1\). Now the range goes from \(e^{-0.1}=4e-5\) to \(e^{10}=22000\) ! Incase of orthogonality the similarity after exp is still 1 of course. But now making the vectors of a negative pair orthogonal (rather than close to parallel) will result in a much larger decrease in loss, as the numerator value can be so much larger (i.e. \(22000 &amp;gt;&amp;gt; \sum_i 1\)).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Contrastive learning has become very popular recently, see here for a good overview of recent papers.</summary></entry><entry><title type="html">Changing My Mind On E2E ASR</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html" rel="alternate" type="text/html" title="Changing My Mind On E2E ASR" /><published>2021-04-12T10:06:02+00:00</published><updated>2021-04-12T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/12/Why-Im-Changing-My-Mind-On-E2E-ASR.html">&lt;p&gt;I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).&lt;/p&gt;

&lt;p&gt;First let’s clarify what E2E ASR means for me: No phones, the model outputs are letters or subword units. No alignment needed and the model learns an internal LM.&lt;/p&gt;

&lt;p&gt;Particularly not using phones bothered me knowing how inconsistent pronunciations of English words are, it just seemed suboptimal to force the network to memorize that.&lt;/p&gt;

&lt;p&gt;However, I’ve had a bit of a change in thinking recently. This has come not from realizing the existance of some technical fact, but rather thinking about what the point of doing speech recognition actually is from the perspective of a consumer. There are many different use cases of course but in by far the majority of them what the end-user wants is a clean transcript. It should be easy to read, free of disfluences, filler words and repetitions.&lt;br /&gt;
I believe traditional ASR is flawed for achieving this, and E2E ASR is not.&lt;/p&gt;

&lt;p&gt;Traditional ASR is split up into at least two components, the AM and LM. The AM is tasked with modeling the acoustics by recognizing phones. Training it means tuning the model so that it makes a decision to which phone each frame (25ms of audio) belongs to. But outputting clean transcripts means ignoring parts of the audio. So this design decision of having one model which classifies phones makes it hard for the model to learn to output clean transcripts, since the better the AM gets at modeling phones the harder it is to ignore certain sounds in the input audio.&lt;/p&gt;

&lt;p&gt;Of course, people with some ASR experience will know that in reality traditional ASR does not have such a problem with outputting clean transcripts. The LM alone should make sure only reasonable word sequences are output. And in practice the AM learns from the training data which (often) has no disfluencies, so the AM learns to map those sounds, which really correspond to some phone, to silence.&lt;/p&gt;

&lt;p&gt;Another issue is that people will regularly pronounce words differently than what you would expect, so during training the model will have to learn to map from one phone to another simply because the lexicon entry for a word will not always correspond exactly how people actually pronounce it.&lt;/p&gt;

&lt;p&gt;So if our AM is not actually modeling sounds, and we’re already forcing it do some more complicated memorization, well then why not just make it model letters?&lt;/p&gt;

&lt;p&gt;While fillers like “um” are never wanted, it’s pretty common to have “you know” missing in a transcript and those words are definitely something the model should not learn to ignore. How is a traditional ASR system supposed to deal with these sorts of errors? &lt;br /&gt;
It cannot. But a transformer-based E2E ASR system can. Thanks to the fact that it looks at much more context (basically seeing the entire input), and that it models letters/subwords directly, it can (for example) learn that a certain sound sequence said quickly at the end of certain sentences (“I really like him yaknow”) can be ignored. This is much harder for a traditional ASR system to learn because the AM does not have so much context it can look at, and even if it did the internal representations would have little to do with words - so it doesn’t know what sentence is being said - as its task is discriminating phones.&lt;/p&gt;

&lt;p&gt;So there’s two points I’m making here: (1) With the training data we have we force the AM to learn an ill-defined task (the task is actually more complicated than just learning to classify phones). (2) E2E systems that use lots of context are better suited to outputting clean transcripts because they combine the AM and LM, so they can learn to ignore sounds because of words that were said before and/or after.&lt;/p&gt;

&lt;p&gt;Clean transcripts aren’t just better for a human reader, it also makes post processing easier for any downstream ML system. And in my experience speech recognition by itself does not have that much value (from a commercial perspective), it’s by adding an NLU system on top that a lot more possibilities for use cases open up.&lt;/p&gt;

&lt;p&gt;I feel like some people have spent so much time thinking about how to model phones that they’ve forgotten that we don’t actually care about phones at all. It could be an interesting question for linguists to study, what phones do people use etc., but if your task is speech recognition classifying which sounds were in the audio should only be a means to an end. A more general conclusion is: &lt;strong&gt;Speech recognition is actually not about what a person said, rather it’s about what they meant to say.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Of course, it’s not very satisfying to just cross your fingers and train a NN to do ASR. It would be nice to somehow give the model a good prior. But with wav2vec2 I feel a good solution has been found as the performance is very good! This combined with the above perspective has changed my mind on E2E-ASR: I believe it is the way forward.&lt;/p&gt;

&lt;p&gt;edit: In hindsight one thing I want to clarify, although I keep mentioning transformer models those are not really required. What really matters is just that the model is bidirectional, therefore can see into both the past and future, and of course that the outputs are character based and the model learns an internal LM.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I used to be quite skeptical of E2E ASR. I thought that yes, the approach was interesting and worth investigating, but it felt like it was putting too much responsibility on the shoulders of a single system (the neural network) with no priors attached. It did not feel like there was an advantage to it other than simplicity (which by itself will not help performance).</summary></entry><entry><title type="html">Why you need a billion words to get a good language model</title><link href="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" rel="alternate" type="text/html" title="Why you need a billion words to get a good language model" /><published>2021-04-01T10:06:02+00:00</published><updated>2021-04-01T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html">&lt;p&gt;I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;199989 krisenbewältigungen 2
199999 gendersensitiv 2
200002 umgehbar 2
200005 widersinnigen 2
200016 ausmehrungen 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It is kind of expected for this to happen, since it is well known that word counts follow zipf’s law, which put simply states that as you go down a table of words sorted by count, their counts decrease very rapidly, meaning a very large amount of the probability mass is covered by the top words. Here is an image (forgive the lack of axis labels please, y-axis is count in millions x-axis rank of word):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ruabraun.github.io/images/words.png&quot; style=&quot;display: block; margin: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at it! (yes I know log scale bla bla, shush pedants) The counts drop extremely quickly to almost nothing.&lt;/p&gt;

&lt;p&gt;So the main point is that you need to have seen a word a couple times to know how it is used. But because in language there is a looong tail of words that are used infrequently (but are still normal enough that you do want to estimate them!) you need a &lt;strong&gt;lot&lt;/strong&gt; of text to get the counts to a reasonable level.&lt;/p&gt;

&lt;p&gt;The additional thing to realise is that because the counts are so low there will be a &lt;strong&gt;ton&lt;/strong&gt; of noise in the results. Depending on the corpus some words whose “true” probability is higher will be much lower and vice versa. This is bad.&lt;/p&gt;

&lt;p&gt;This is why you need to train on billion word corpuses to get good results. Then all those 2s will turn into 20s, and then the model can learn something.&lt;/p&gt;

&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The total count is around 400k by the way (a good chunk of them are rubbish).&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position. 199989 krisenbewältigungen 2 199999 gendersensitiv 2 200002 umgehbar 2 200005 widersinnigen 2 200016 ausmehrungen 2 The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!1</summary></entry><entry><title type="html">Why does BPE work?</title><link href="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html" rel="alternate" type="text/html" title="Why does BPE work?" /><published>2021-02-09T10:06:02+00:00</published><updated>2021-02-09T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2021/02/09/Why-does-BPE-work.html">&lt;p&gt;BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?&lt;/p&gt;

&lt;p&gt;Let’s create a simple ‘corpus’* (word counts are what actually matter):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;low 5
lowest 2
newer 6
wider 3
new 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Including the separator (_, appears implicitly at the end of a word) there are 11 characters. Let’s pick the 20 most common ngrams, they turn out to be (in order, but starting with unigrams):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'r_', 'we', 'ne', 'new', 'ew', 'lo', 'low']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is what BPE outputs (single character tokens are not ordered):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['l', 'o', 'w', '_', 'e', 's', 't', 'n', 'r', 'i', 'd', 'er', 'er_', 'ne', 'new', 'lo', 'low', 'newer_', 'low_', 'wi']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first not-unigram is the same for both (“er”), the second as well, but then the methods diverge. Note how the ngram counter picks up “we” while BPE never does. Why is that? Normally “we” would of course be a reasonable token, but for this corpus it makes more sense intuitively, considering the counts, to keep it seperate so that low + est and new + er are valid tokenisations.&lt;/p&gt;

&lt;p&gt;Because BPE redoes the count each time two tokens are merged (from which a new one is created), and only the merged token is counted - instead of also the previous two - the counts of some tokens can go down after a recount. So when “er” is created the count of “we” drops from 8 to 2! The recounting makes sense in hindsight, the counts of a token should be based on when that token could actually be used, and after choosing the “er” token, the token “we” is not possible as often as before.&lt;/p&gt;

&lt;p&gt;What if we change the k most frequent method to iteratively merge with recounts inbetween, and when recounting if two tokens were merged then we only count the merged token (not the previously split tokens)?&lt;/p&gt;

&lt;p&gt;We would have to start with some set of tokens, those could be the unigrams. Then we count ngrams that are not already in our token set, merge the most frequent, repeat. This is basically BPE, except we’re still doing a bunch of wasted computation by counting ngrams: It should be, with a little thinking, obvious that a bigram will always be at least as likely as a trigram. So rather than count all ngrams that are not already in the token set, we just count bigrams of tokens AKA pairs of tokens. This is BPE.&lt;/p&gt;

&lt;p&gt;Now the motivation for the algorithm of BPE is clear. It recounts after each merge to make sure the statistics used are up-to-date, and by considering pairs of tokens it saves a lot of computation.&lt;/p&gt;

&lt;p&gt;* Taken from Jurafsky’s chapter on BPE.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">BPE is a remarkably effective algorithm for finding a set of subwords. Just count pairs of tokens, merge the most frequent one, repeat until you have the desired number of subwords. Why does this work, and why would just picking the k most frequent ngrams not?</summary></entry><entry><title type="html">On WER in ASR</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html" rel="alternate" type="text/html" title="On WER in ASR" /><published>2020-11-27T10:06:02+00:00</published><updated>2020-11-27T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/27/On-word-error-rates.html">&lt;p&gt;This post will be about the python-based &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;tool (“texterrors”)&lt;/a&gt; I created for getting error metrics (relevant for ASR). It is split in two parts: 
First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking &lt;a href=&quot;#newtool&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;wer-calculation-recap&quot;&gt;WER calculation recap&lt;/h2&gt;

&lt;p&gt;Given a hypothesized sentence the Word-Error-Rate is defined as the number of insertion, deletion and substitution errors with the respect to a reference sentence, divided by the count of words in the reference. Insertion/Deletion is defined from the perspective of the model, so for example if the model outputs a word when it shouldn’t, that’s an insertion error.&lt;/p&gt;

&lt;p&gt;To find out the types of errors one has to align the hypothesis to the reference, this is typically done by creating a cost matrix (where the cost of a cell depends on the transition cost plus the lowest cost from the left, top or diagonal cells) and backtracing from the end (bottom right) to the start (top left) to find the alignment. Example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;third&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;second&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;third&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Taking a horizontal or vertical (deletion/insertion) transition costs 1. Taking a diagonal to position &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i, j&lt;/code&gt; costs 1 if word i != word j, else 0. “second” is not recognized by the model (a deletion error) which is why at the end the cost in the bottom right is 1. Notice that if all we want to know is the WER, you can actually just take that value (1) and divide by the count of reference words (3) to get the WER (33.3%).&lt;/p&gt;

&lt;p&gt;However, usually one wants to know how many errors of each type there are, and to do that one needs to get the alignment to then count them. This requires backtracing which is done by finding the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; so that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transition_cost&lt;/code&gt; = &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt;, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cell_x&lt;/code&gt; is either to the left, diagonal or above the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;current_cell&lt;/code&gt; (then repeat the process until the start is reached).&lt;br /&gt;
This can be ambiguous, for example consider two sentences “first word in sentence” and “first ward sentence”. There are different ways to align this:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;word&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;in&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ward&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Clearly the pair “word”/”ward” is a more likely substitution error than “in”/”ward”, but this alignment method has no way of identifying that since the cumulative costs are the same, see cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This has no impact on the WER, as in both cases there are two (one insertion/deletion depending on which sentence is considered the reference, one substitution), but from the perspective of analysing what sorts of errors a model is making - which sorts of words the model is failing to recognize (deletions), which words are confused with each other (substitutions) - having a different alignment will change the result.&lt;/p&gt;

&lt;h2 id=&quot;getting-better-alignments-with-texterrors-&quot;&gt;Getting better alignments with “texterrors” &lt;a name=&quot;newtool&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;As just mentioned, the traditional method of alignment (which we need to do to get statistics for the different error types) leads to ambiguous alignments with no sensible way of resolving them. &lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;“texterrors”&lt;/a&gt; is meant to be a tool for getting detailed error metrics. As these are sensitive to suboptimal alignments it uses a smarter method: Instead of having a cost of 1 for the substitution cost (in the cost matrix), it incorporates the character edit distance between the words compared.&lt;/p&gt;

&lt;p&gt;Concretely, the substitution cost is set to the edit distance between two words divided by the maximum edit distance possible (length of the longer word), so it is a value between 0 and 1 (slightly more complicated in practice, you’ll see later why). That way alignments will be favoured when words which are similar to each other are substitution errors instead of deletion/insertion errors.&lt;/p&gt;

&lt;p&gt;Example cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;first&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ward&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;first&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;word&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;in&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.25&lt;/td&gt;
      &lt;td&gt;1.125&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;sentence&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.25&lt;/td&gt;
      &lt;td&gt;1.25&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As one can see here, the best (lowest) cumulative cost is achieved by pairing “word”/”ward”.&lt;/p&gt;

&lt;p&gt;One should be aware, this method can result in a higher WER. 
In the below example a normal WER calculation would do a one-to-one mapping and arrive at a WER of 66.67\%.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;But character aware alignment would result in the following alignment:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;test&lt;/th&gt;
      &lt;th&gt;-&lt;/th&gt;
      &lt;th&gt;sentence&lt;/th&gt;
      &lt;th&gt;okay&lt;/th&gt;
      &lt;th&gt;words&lt;/th&gt;
      &lt;th&gt;ending&lt;/th&gt;
      &lt;th&gt;now&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;test&lt;/td&gt;
      &lt;td&gt;a&lt;/td&gt;
      &lt;td&gt;sentenc&lt;/td&gt;
      &lt;td&gt;ok&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;endin&lt;/td&gt;
      &lt;td&gt;now&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This results in a WER of 83.3\% because of the extra insertion and deletion. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; has an option to turn character-aware alignment off (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-no-chardiff&lt;/code&gt;) to get identical results with kaldi. But the difference is small for a normal sized test set, and obviously without the feature the alignments and therefore statistics like the most frequent substitution error will not be as accurate!&lt;/p&gt;

&lt;p&gt;There is still one last issue to deal with, see this example:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.67&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.78&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.78&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The alignment will end up being&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This happens here because using the character edit distance leads to the substitution cost often being smaller than the insertion/deletion cost and therefore alignments with more substitutions are favored.&lt;br /&gt;
This sort of bad alignment can also happen with normal costs of 1/1/1 for ins/del/sub (consider the above example, as the costs are the same for different errors it depends on the implementation which alignment is chosen, you can think of it as random). That’s why such tools, when meant to be used for getting detailed error metrics, will increase the substitution cost to improve alignments. We can do the same: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; will after the previously mentioned calculation times the cost by 1.5. This will lead to the following cost matrix:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;hello&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;eight&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;And the following (obviously superior) alignment.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;-&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;speedbird&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;eight&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;six&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;two&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;hello&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;speedbird&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;six&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;two&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;texterrors&lt;/code&gt; also supports ctm files where the time stamps for reference and hypothesis words will be used for alignment. If you &lt;em&gt;really&lt;/em&gt; care about having the most accurate alignment possible, that is what you should use! But it won’t make a big difference. :)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/RuABraun/texterrors&quot;&gt;Here is a link to it.&lt;/a&gt; Thank you for reading.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">This post will be about the python-based tool (“texterrors”) I created for getting error metrics (relevant for ASR). It is split in two parts: First a refresher on standard WER calculation and an illustration of how this can be suboptimal when interested in analysing errors. Then an introduction to the approach I use which fixes the problems mentioned. You can skip to the second part by clicking here.</summary></entry><entry><title type="html">Doing non-standard stuff with kaldi decoding</title><link href="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html" rel="alternate" type="text/html" title="Doing non-standard stuff with kaldi decoding" /><published>2020-11-06T10:06:02+00:00</published><updated>2020-11-06T10:06:02+00:00</updated><id>https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG</id><content type="html" xml:base="https://ruabraun.github.io/jekyll/update/2020/11/06/Kaldi-decoding-with-custom-LMs-and-no-HCLG.html">&lt;p&gt;Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found &lt;a href=&quot;https://github.com/RuABraun/fst-util&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;adding-words-to-hclg&quot;&gt;Adding words to HCLG&lt;/h2&gt;

&lt;p&gt;Some scripts you will need can be found &lt;a href=&quot;https://github.com/idiap/icassp-oov-recognition&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This method requires you to use a monophone model. Additionally, your language model needs to have been trained with pocolm, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--limit-unk-history&lt;/code&gt; option, and there should have been some OOVs in the training text.&lt;/p&gt;

&lt;p&gt;For simplicity, the modification is done on a graph without self-loops. So you need to modify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;utils/mkgraph.sh&lt;/code&gt; and comment L167: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm $dir/HCLGa.fst $dir/Ha.fst 2&amp;gt;/dev/null || true&lt;/code&gt; because we will use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Inside the graph dir where the HCLG is there is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt;. You need to assign IDs to the new words you’re adding and append these to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words.txt&lt;/code&gt; file (these should be larger than the existing ones obviously).&lt;/p&gt;

&lt;p&gt;Assuming all this is ready you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;script/compose_hcl.sh&lt;/code&gt; to create the HCL from a lexicon of the OOV words you want to add. Check the script for the input arguments, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt; is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;final.mdl&lt;/code&gt;, isym is phones osym words. Notice it uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_lfst.py&lt;/code&gt; so you need the fst wrapper installed. There is one hardcoded parameter on L25, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;303&lt;/code&gt;, see &lt;a href=&quot;https://groups.google.com/g/kaldi-help/c/jL8VnwKGRWs/m/-Pe29-G9AgAJ&quot;&gt;here&lt;/a&gt; for what’s about. You can set it to any number larger than the existing phone IDs.&lt;/p&gt;

&lt;p&gt;After calling the script and creating the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCL.fst&lt;/code&gt; you use the fst wrapper to modify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLGa.fst&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from wrappedfst import WrappedFst
fst = WrappedFst('HCLGa.fst')
ifst = WrappedFst('HCL.fst')
unk_id =  # unk symbol
fst.replace_single(unk_id, ifst)
fst.write('HCLGa_new.fst')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then add the self-loops (check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkgraph.sh&lt;/code&gt; for how to do that) and you are done. Replace an existing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCLG.fst&lt;/code&gt; with the new version and you can run decoding as you would normally.&lt;/p&gt;

&lt;h2 id=&quot;no-hclg-just-g&quot;&gt;No HCLG just G&lt;/h2&gt;

&lt;p&gt;Imagine you have an acoustic model that you created via pytorch, and you want to then evaluate how well it does at recognizing phonemes (TIMIT for example). You have a phone LM that you want to incorporate. I’m going to show you how you can do that with kaldi.&lt;/p&gt;

&lt;p&gt;Usually kaldi assumes that you have an HMM graph (the H in HCLG) and possibly context dependency as well (the C), then there’s also the lexicon etc. If you had a pytorch AM that you had trained on the PDF-ids (the targets you get by calling ali-to-pdf on the alignments), you would then use that by saving the loglikelihood output of your NN to a file, then passing that file and the final.mdl, HCLG.fst to latgen-faster-mapped, which will do decoding and generate a lattice (from which you could then take the best path for example).&lt;/p&gt;

&lt;p&gt;But if you want to just model phones, you don’t care about the HCL at all. You really just want to use a G that will act as the phone LM.&lt;/p&gt;

&lt;p&gt;So the first issue to get around is to avoid doing any mapping from transition IDs (the typical input symbols of the HCLG) to PDF-IDs. We can do that by creating a file latgen-faster.cc with the only difference being we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaled&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DecodableMatrixScaledMapped&lt;/code&gt; in the code.&lt;/p&gt;

&lt;p&gt;Then you take the arpa LM that you have trained, convert it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arpa2fst&lt;/code&gt;, convert the #0 arcs to &lt;eps&gt; and add self-loops to states that have incoming arcs with input labels not equal 0. The point of that is when the AM predicts a sequence &quot;ae ae ae b b&quot; over five frames, the LM should not care about repetitions, we want it to only score the transition &quot;ae&quot; to &quot;b&quot;, and of course we don't want repetitions in the output transcript. Having self-loops fixes that.&lt;/eps&gt;&lt;/p&gt;

&lt;p&gt;Then you call&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;latgen-faster --acoustic-scale=1.0 --beam=13 --determinize-lattice=false G.fst ark,t:loglikelihoods-file&amp;gt; ark:- | lattice-best-path etc.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to get a hypothesis that you can compare to the reference to get the PER!&lt;/p&gt;

&lt;p&gt;If you wanted to recognize words, all you’d need to do is create a L.fst, compose that with the G.fst (which is then a word LM) and use the LG.fst.&lt;/p&gt;

&lt;h2 id=&quot;g-with-keyword-list&quot;&gt;G with keyword list&lt;/h2&gt;

&lt;p&gt;Imagine you just want to recognize a small number of words and you don’t want to have any sort of prior regarding which one is more likely.&lt;/p&gt;

&lt;p&gt;To do that you just have to create an FST with a single state, and self-loops where each self-loop has a keyword as the input and output symbol.&lt;/p&gt;

&lt;p&gt;In pseudo-code&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fst = Fst()
state = fst.add_state()
fst.set_start(state)
fst.set_final(state)
for keyword in keywords
    fst.add_arc(state, state, keyword, keyword, cost)
fst.write(...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The self-loops should have costs, what these should be you will have to find out experimentally.&lt;/p&gt;

&lt;p&gt;You probably will have to create the lang/ folder again. To do that just call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_lang.sh&lt;/code&gt; with the dict/ of keywords and with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--phone-symbol-table&lt;/code&gt; option set to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;phones.txt&lt;/code&gt; that you trained the AM model with.&lt;/p&gt;</content><author><name>Rudolf A. Braun</name></author><category term="jekyll" /><category term="update" /><summary type="html">Here I’m going to describe methods for using kaldi for decoding when you want to do something a bit custom. I will use an OpenFST wrapper and scripts using it which can be found here.</summary></entry></feed>