<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link type="application/atom+xml" rel="alternate" href="https://ruabraun.github.io/feed.xml" title="Sharings" />
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>k2 simple rnnt loss | Sharings</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="k2 simple rnnt loss" />
<meta name="author" content="Rudolf A. Braun" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is a summary of how k2/icefall makes the transducer loss faster and take less memory, see here for an excellent introduction to transducers: https://lorenlugosch.github.io/posts/2020/11/transducer/" />
<meta property="og:description" content="This is a summary of how k2/icefall makes the transducer loss faster and take less memory, see here for an excellent introduction to transducers: https://lorenlugosch.github.io/posts/2020/11/transducer/" />
<link rel="canonical" href="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html" />
<meta property="og:url" content="https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html" />
<meta property="og:site_name" content="Sharings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-15T10:06:02+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="k2 simple rnnt loss" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"k2 simple rnnt loss","dateModified":"2022-07-15T10:06:02+00:00","datePublished":"2022-07-15T10:06:02+00:00","author":{"@type":"Person","name":"Rudolf A. Braun"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html"},"description":"This is a summary of how k2/icefall makes the transducer loss faster and take less memory, see here for an excellent introduction to transducers: https://lorenlugosch.github.io/posts/2020/11/transducer/","url":"https://ruabraun.github.io/jekyll/update/2022/07/15/k2-rnnt-loss.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/pure-min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/grids-responsive-min.css">
  <link rel="stylesheet" href="/assets/css/open-color.css">
  <link rel="stylesheet" href="/assets/css/hydure.css">

  <script async src="https://use.fontawesome.com/releases/v5.12.0/js/all.js"></script>

  

</head>


  <body>
    <div id="layout" class="pure-g">
      <div class="sidebar pure-u-1 pure-u-md-1-4" style="background-color: #202421">
        <div class="sidebar-shield">
          <header class="header">
  <img src="https://ruabraun.github.io/images/profpic.jpg" alt="Me!" width="40%" height="40%" class="profile">
  <a class="brand-title" href="/">Sharings</a>
  <p class="brand-name">Rudolf A. Braun</p>
  <p class="brand-tagline">On speech recognition and maybe other things</p>

  
    <nav class="nav pure-menu">
      <ul class="pure-menu-list">
      
        <li class="nav-item pure-menu-item">
          <a href="/" class="pure-menu-link ">
            Home
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/about/" class="pure-menu-link ">
            About
          </a>
        </li>
      
      </ul>
    </nav>
  

  
    <div class="social pure-menu pure-menu-horizontal">
      <ul class="social-list pure-menu-list">
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="mailto:rab014@gmail.com" target="_blank">
              <i class="fas fa-envelope" title="Email"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://twitter.com/fasttosmile" target="_blank">
              <i class="fab fa-twitter" title="Twitter"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://github.com/RuABraun/" target="_blank">
              <i class="fab fa-github" title="GitHub"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://www.linkedin.com/in/rudolf-a-braun-speech/" target="_blank">
              <i class="fab fa-linkedin" title="LinkedIn"></i>
            </a>
          </li>
        
      </ul>
    </div>
  
</header>

        </div>
      </div>
      <div class="content pure-u-1 pure-u-md-3-4">
        <div class="main">
          <article class="post">
  
    <div class="post-meta">
      <ul class="post-categories"><li>
            <span class="post-category">jekyll</span></li><li>
            <span class="post-category">update</span></li></ul>
    </div>
  
  <h1 class="post-title">k2 simple rnnt loss</h1>
  <div class="post-meta">
    <time datetime="2022-07-15T10:06:02+00:00" itemprop="datePublished">
      15 Jul 2022
    </time></div>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
  <p>This is a summary of how k2/icefall makes the transducer loss faster and take less memory, see here for an excellent introduction to transducers: <a href="https://arxiv.org/abs/2206.13236">https://lorenlugosch.github.io/posts/2020/11/transducer/</a></p>

<p>The normal rnnt loss requires creating a matrix (B,T,U,V) where B=batch size; T=time steps; U=symbols in reference and V=vocab size.</p>

<p>This matrix is created by combining the encoder and decoder outputs, which have shape (B,T,C) and (B,U,C). The combination is done by adding the two (with dimension unsqueezing so the result of the addition is (B,T,U,C)), and then projecting to (B,T,U,V). (This means time and symbol states are independent in the sense of p(X,Y) = p(X)p(Y))</p>

<p>The simple rnnt loss (from k2) avoids creating the very large (B,T,U,V) matrix. It does so by recognizing that for training we don’t need to have a distribution across all tokens in V as we have a training transcript so we know at each position (T,U) the token probability we care about. Since we also care about the blank transition, the simple rnnt loss calculates two matrices (B,T,U) which contain the appropriate symbol and blank probabilities respectively.</p>

<p>The calculation is done by first projecting encoder and decoder outputs to (B,T,V) and (B,U,V), doing matrix-multiply of (B,T,V), (B,V,U) to get a normalization matrix (B,T,U), and then picking out the unnormalized token log probabilities we care about from (B,T,V) plus (B,U,V) and subtracting the normalization value (to get a normalized value). Logprobs are used when adding, normal probs wwhen multiplying (for the matrix multiply) normal probs are used, so the implemenation has some exp() and log() calls.</p>

<p>Note you wouldn’t normally want to take this approach because the encoder and decoder outputs don’t get to interact before the token distribution is calculated (they’re added together after the projection to size V), it’s as if you had separate AM and LM models and you were just adding together <code class="language-plaintext highlighter-rouge">P(y|x)</code> and <code class="language-plaintext highlighter-rouge">P(y|y-1)</code> (the AM just seeing audio and the LM just text). The point of RnnT is that you want an output <code class="language-plaintext highlighter-rouge">P(y|x,y_1)</code>, something that directly conditions on both audio and text.</p>

<p>After some training this approach will tell us which paths in (T,U) are more or less likely, because the simple loss will calculate its own version of (T,U,) and many paths in it will have low probability. This information can be used to set boundaries for each time step in T, which allows doing the proper RNNT loss on a subset (B,T,S,V) with S&lt;&lt;U (because we know that for a given point in time only some tokens are possible, not all in U), which takes much less memory, is faster and trains the <code class="language-plaintext highlighter-rouge">P(y|x,y_1)</code> output.</p>

<p>Let’s look at some code (I collapsed whitespace to make things more compact). Here’s the joiner that creates (B,T,U,V). Note the dimension unsqueezing of the encoder and decoder outputs is assumed to have already happened (code is from a recipe specific file <code class="language-plaintext highlighter-rouge">joiner.py</code>).</p>

<p><img src="https://ruabraun.github.io/images/k2_joiner.png" style="display: block; margin: auto;" /></p>

<p>In the following image you can see the separate projection of encoder and decoder outputs to the vocab size, then computing a simple loss and using that (specifically the gradients) to get boundaries for creating (B,T,S,V). Directly after the last line shown the normal rnnt loss is calculated (code is from a recipe specific file <code class="language-plaintext highlighter-rouge">model.py</code>).</p>

<p><img src="https://ruabraun.github.io/images/k2_losshighlevel.png" style="display: block; margin: auto;" /></p>

<p>Looking slightly deeper into <code class="language-plaintext highlighter-rouge">k2.rnnt_loss_smoothed</code> we can see there are two stages: First calculating px ((T,U,) of reference symbols) and py ((T,U,) of blank symbol), then doing the standard dynammic programming triple for-loop (for the batch, time, symbol dimensions) in <code class="language-plaintext highlighter-rouge">mutual_information_recursion</code> to calculate the total logprob across all alignments. Despite the intimidating name the implementation of the latter is quite straightforward (for CPU at least), see <a href="https://github.com/k2-fsa/k2/blob/master/k2/python/csrc/torch/mutual_information_cpu.cu">here</a>.</p>

<p><img src="https://ruabraun.github.io/images/k2_smoothloss.png" style="display: block; margin: auto;" /></p>

<p>I hope this helps give an overview and basic understanding of how the rnnt loss is calculated in k2/icefall.</p>

<p>A preprint is available with nice results: <a href="https://arxiv.org/abs/2206.13236">https://arxiv.org/abs/2206.13236</a></p>


  

  
</article>

        </div>
        <footer class="footer pure-g">
  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      &copy;&nbsp;<time datetime="2020-10-04T12:40:02+00:00">2020</time>-<time datetime="2022-07-28T19:23:22+00:00">2022</time>&nbsp;<a href="" target="_blank">Rudolf A. Braun</a>. All right reserved.
    </small>
  </div>

  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> & <a href="https://github.com/zivong/jekyll-theme-hydure" target="_blank">Hydure</a>
    </small>
  </div>
</footer>

      </div>
    </div>

    

    
  </body>
</html>
