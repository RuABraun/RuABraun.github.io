<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link type="application/atom+xml" rel="alternate" href="https://ruabraun.github.io/feed.xml" title="Sharings" />
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>A comparison of fairseq, speechbrain, k2 for ASR | Sharings</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="A comparison of fairseq, speechbrain, k2 for ASR" />
<meta name="author" content="Rudolf A. Braun" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This gives a high level overview of fairseq, speechbrain and k2. We will go over the code base structure, what the training loop looks like, the procedure for training a model, and I will name stuff I liked or disliked." />
<meta property="og:description" content="This gives a high level overview of fairseq, speechbrain and k2. We will go over the code base structure, what the training loop looks like, the procedure for training a model, and I will name stuff I liked or disliked." />
<link rel="canonical" href="https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html" />
<meta property="og:url" content="https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html" />
<meta property="og:site_name" content="Sharings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-26T10:06:02+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="A comparison of fairseq, speechbrain, k2 for ASR" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"A comparison of fairseq, speechbrain, k2 for ASR","dateModified":"2022-06-26T10:06:02+00:00","datePublished":"2022-06-26T10:06:02+00:00","author":{"@type":"Person","name":"Rudolf A. Braun"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html"},"description":"This gives a high level overview of fairseq, speechbrain and k2. We will go over the code base structure, what the training loop looks like, the procedure for training a model, and I will name stuff I liked or disliked.","url":"https://ruabraun.github.io/jekyll/update/2022/06/26/ASR-Toolkits.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/pure-min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/grids-responsive-min.css">
  <link rel="stylesheet" href="/assets/css/open-color.css">
  <link rel="stylesheet" href="/assets/css/hydure.css">

  <script async src="https://use.fontawesome.com/releases/v5.12.0/js/all.js"></script>

  

</head>


  <body>
    <div id="layout" class="pure-g">
      <div class="sidebar pure-u-1 pure-u-md-1-4" style="background-color: #202421">
        <div class="sidebar-shield">
          <header class="header">
  <img src="https://ruabraun.github.io/images/profpic.jpg" alt="Me!" width="40%" height="40%" class="profile">
  <a class="brand-title" href="/">Sharings</a>
  <p class="brand-name">Rudolf A. Braun</p>
  <p class="brand-tagline">On speech recognition and maybe other things</p>

  
    <nav class="nav pure-menu">
      <ul class="pure-menu-list">
      
        <li class="nav-item pure-menu-item">
          <a href="/" class="pure-menu-link ">
            Home
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/about/" class="pure-menu-link ">
            About
          </a>
        </li>
      
      </ul>
    </nav>
  

  
    <div class="social pure-menu pure-menu-horizontal">
      <ul class="social-list pure-menu-list">
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="mailto:rab014@gmail.com" target="_blank">
              <i class="fas fa-envelope" title="Email"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://twitter.com/fasttosmile" target="_blank">
              <i class="fab fa-twitter" title="Twitter"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://github.com/RuABraun/" target="_blank">
              <i class="fab fa-github" title="GitHub"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://www.linkedin.com/in/rudolf-a-braun-speech/" target="_blank">
              <i class="fab fa-linkedin" title="LinkedIn"></i>
            </a>
          </li>
        
      </ul>
    </div>
  
</header>

        </div>
      </div>
      <div class="content pure-u-1 pure-u-md-3-4">
        <div class="main">
          <article class="post">
  
    <div class="post-meta">
      <ul class="post-categories"><li>
            <span class="post-category">jekyll</span></li><li>
            <span class="post-category">update</span></li></ul>
    </div>
  
  <h1 class="post-title">A comparison of fairseq, speechbrain, k2 for ASR</h1>
  <div class="post-meta">
    <time datetime="2022-06-26T10:06:02+00:00" itemprop="datePublished">
      26 Jun 2022
    </time></div>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
  <p>This gives a high level overview of fairseq, speechbrain and k2. We will go over the code base structure, what the training loop looks like, the procedure for training a model, and I will name stuff I liked or disliked.</p>

<p>Whenever I refer to files, I assume you are starting from the root folder of the repo.</p>

<h1 id="fairseq">fairseq</h1>
<p><a href="https://github.com/facebookresearch/fairseq">Fairseq</a> is not just meant for ASR. It’s for many different modeling tasks, including language modeling, translation, masked LM pretraining, summarization and text to speech.</p>

<p>Training models and doing inference is done via command-line scripts (found in <code class="language-plaintext highlighter-rouge">fairseq_cli/</code>). The training loop and data processing code is unified when possible. As a consequence the code is generally a bit more abstract, you cannot look at just one file if you want to read and understand the training loop.
The codebase can be seen as split into four different parts.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">fairseq_cli/</code> contains CLI scripts for data processing, model training and evaluation.</li>
  <li><code class="language-plaintext highlighter-rouge">fairseq/</code> (except for <code class="language-plaintext highlighter-rouge">/tasks/</code>) is like a library of common models, data processing or utility classes and functions</li>
  <li><code class="language-plaintext highlighter-rouge">fairseq/tasks/</code> has files which contain code specific to a modeling task</li>
  <li><code class="language-plaintext highlighter-rouge">examples/</code> has the documentation and scripts for training and evaluating models related to facebook’s papers and corpuses</li>
</ul>

<p>When training you pass a “task” argument (when calling the CLI scripts) which ensures the appropriate code is run for whatever task you are doing. Many modifications to a task, like using a different model, can done by changing a command-line argument or using a config file to redefine a default (the config is based on hydra+omegaconf). Models defined in fairseq (<code class="language-plaintext highlighter-rouge">fairseq/models/</code>) will have a decorator enabling you to specify it using one keyword (making it easy to change).</p>

<p>There are several tasks related to speech recognition: <code class="language-plaintext highlighter-rouge">speech_to_text</code>, <code class="language-plaintext highlighter-rouge">speech_recognition</code>, <code class="language-plaintext highlighter-rouge">audio_pretraining</code> and <code class="language-plaintext highlighter-rouge">audio_finetuning</code>. The latter two are for wav2vec[2]. It seems they are planning to unify all the previously mentioned ASR related ones to <code class="language-plaintext highlighter-rouge">speech_to_text</code>, but this is still WIP.</p>

<p>The data format depends on the task and tends to favor simplicity. For example for wav2vec pretraining you just need a .tsv file which has on the first line a root path, and every line afterwards has a path to an audio file and its sample count. That’s it. In <code class="language-plaintext highlighter-rouge">examples/*</code> there always is an explanation for how to get to the data format needed.</p>

<p>I find fairseq to be a good toolkit for reproducing results from facebook’s papers - I was surprised how easy it was to do the pretraining for wav2vec models - and convenient if you want to toy around with a research idea that is slightly different from what already exists. A lot of small changes that you might want to do can be done fairly easily. If you’re doing something significantly different from what already exists that could get a lot harder and you will likely have to end up learning about how the entire fairseq repo ties together to achieve that. But I do think once you get it it can be quite nice to work with. One downside is you can’t rely much on support from the maintainers; github issues are rarely responded to. It also is not very suited for using in production (but to be fair that is not what it’s made for).</p>

<p>Let’s skim over the training loop, we start in <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/train.py#L180">fairseq_cli/train.py</a>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>while epoch_itr.next_epoch_idx &lt;= max_epoch:
    [..]
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
</code></pre></div></div>
<p>With train defined in the same file and then calling <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq_cli/train.py#L312">trainer.train_step()</a>, the most important of which looks like:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>logger.info("Start iterating over samples")
for i, samples in enumerate(progress):
     [..]
	 log_output = trainer.train_step(samples)
</code></pre></div></div>
<p>The dataloader is wrapped inside of <code class="language-plaintext highlighter-rouge">progress</code>.  The trainer object is defined in <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq/trainer.py">fairseq/trainer.py</a> and its <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq/trainer.py#L824">train_step()</a> calls the task specific <code class="language-plaintext highlighter-rouge">train_step</code>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i, sample in enumerate(samples):  # delayed update loop
    [..]
	loss, sample_size_i, logging_output = self.task.train_step(
		sample=sample,
		model=self.model,
		criterion=self.criterion,
		optimizer=self.optimizer,
		update_num=self.get_num_updates(),
		ignore_grad=is_dummy_batch,
		**extra_kwargs,
	)
</code></pre></div></div>
<p>Note that <code class="language-plaintext highlighter-rouge">samples</code> will have a length bigger 1 only if gradient accumulation is used.</p>

<p>A lot of tasks actually don’t have a <code class="language-plaintext highlighter-rouge">train_step()</code> defined and just use the generic one defined in <a href="https://github.com/facebookresearch/fairseq/blob/main/fairseq/tasks/fairseq_task.py#L490">fairseq/tasks/fairseq_task.py</a> (the parent class from which all other tasks inherit).</p>

<p>One uses <code class="language-plaintext highlighter-rouge">fairseq-hydra-train</code> to train a model when one has a config and <code class="language-plaintext highlighter-rouge">fairseq-train</code> if you’re willing to specify all the arguments on the commandline. This is an example call to finetune a wav2vec model:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fairseq-hydra-train task.data=/work2/rudolf/wav2vec/ch-de/22-08-05/manifest/ model.w2v_path=/work1/rudolf/wav2vec_model.pt checkpoint.save_dir=/work2/rudolf/wav2vec/ch-de/22-08-05/model-a --config-dir $PWD --config-name finetuning.yaml
</code></pre></div></div>
<p>Most of the options are defined inside <code class="language-plaintext highlighter-rouge">finetuning.yaml</code> , for example the task, this looks like:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>task:
  _name: audio_finetuning
  data: ???
  normalize: true 
  labels: bpe
</code></pre></div></div>
<p>The name specifies which task defined in  <code class="language-plaintext highlighter-rouge">fairseq/tasks/</code> will be used, all other options are specific to that. In this case they specify that bpe labels will be used for training and the audio files will be normalized after loading. <code class="language-plaintext highlighter-rouge">???</code> means the user should specify this, which we did in the above commandline with <code class="language-plaintext highlighter-rouge">task.data=...</code>.</p>

<p>In <code class="language-plaintext highlighter-rouge">fairseq/examples/</code> one can always find the commandline necessary to train a recipe.</p>

<h1 id="speechbrain">speechbrain</h1>

<p><a href="https://github.com/speechbrain/speechbrain">Speechbrain</a> is specifically for speech processing tasks, they have an impressive array of examples where they get good results (<code class="language-plaintext highlighter-rouge">recipes/</code>).</p>

<p>The recipe implementations are less unified than those in fairseq, but there is still a lot of shared code. The core training loop is unified in a <code class="language-plaintext highlighter-rouge">Brain</code> class (found in <code class="language-plaintext highlighter-rouge">speechbrain/core.py</code>), the data processing code is specified in each recipe.</p>

<p>Speechbrain uses a powerful yaml config (<a href="https://github.com/speechbrain/HyperPyYAML">HyperPyYaml</a>) which is used not only to define hyperparameters but also to define data processing related things like the data loading sampler, the loss, the optimizer and the model. A simple one like an 3-layer MLP you could define right in the config like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>enc: !new:speechbrain.lobes.models.VanillaNN.VanillaNN
   input_shape: [null, null, 1024]
   activation: torch.nn.LeakyRelu
   dnn_blocks: 2
   dnn_neurons: 1024
</code></pre></div></div>
<p>A complicated model would be defined by refering to the speechbrain implementation (for example a transformer) for example:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>encoder: !new:speechbrain.lobes.models.transformer.Transformer.TransformerEncoder
	d_model: 768
	num_layers: 12
	nhead: 8
	d_ffn: 3072
	dropout: 0.1
</code></pre></div></div>
<p>Note that the class will be instantiated when the config is loaded, so you don’t have to write any code to do that. To use a different model you can just modify the config.</p>

<p>Sometimes speechbrain’s models (e.g. the VanillaNN above) use an <code class="language-plaintext highlighter-rouge">input_shape</code> to allow them to infer the output shape (by running the model). This is implemented by having those models inherit from <code class="language-plaintext highlighter-rouge">speechbrain.nnet.containers.Sequential</code> (<a href="https://github.com/speechbrain/speechbrain/blob/424e7921531b0ea6523557ef0fd6ca249936bd26/speechbrain/nnet/containers.py#L18">code link</a>) which has an <code class="language-plaintext highlighter-rouge">append</code> method which you use when successively adding layers (this will then pass the input shape and check it works). If this doesn’t make sense it’s okay, it’s a detail.</p>

<p>The <code class="language-plaintext highlighter-rouge">Brain</code> class is initialized with this yaml config, and will hold a reference to the model, the loss, the optimizer, but not the dataset/dataloaders. In contrast to other toolkits speechbrain defines much more in the config.</p>

<p>How the model is used is specified by overriding two methods (of the <code class="language-plaintext highlighter-rouge">Brain</code> class) <code class="language-plaintext highlighter-rouge">compute_forward</code> and <code class="language-plaintext highlighter-rouge">compute_objectives</code>. The former defines how to get from the data to the outputs you need to compute the loss. You then use the latter to define how to compute the loss from those outputs (it’s called automatically after the former). Sometimes the implementation will be extremely simple: In <code class="language-plaintext highlighter-rouge">compute_forward</code> you call the model and in <code class="language-plaintext highlighter-rouge">compute_objectives</code> you call the loss defined in your config.</p>

<p>The data is typically stored in CSV format (with filepaths pointing to audio data) which you load to a <code class="language-plaintext highlighter-rouge">DynamicItemDataset</code> like (<code class="language-plaintext highlighter-rouge">sb</code>  is  the speechbrain package):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(csv_path=hparams["train_csv"])
</code></pre></div></div>
<p>in your recipe you then specify (via a custom “pipeline” construct) how to go from the data in the CSV to the data you want available in a batch. This can look like this (<a href="https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/ASR/CTC/train_with_wav2vec.py#L234">code link</a>):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@sb.utils.data_pipeline.takes("wav")
@sb.utils.data_pipeline.provides("sig")
def audio_pipeline(wav):
	sig = sb.dataio.dataio.read_audio(wav)
	return sig
</code></pre></div></div>
<p>or this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 3. Define text pipeline:
@sb.utils.data_pipeline.takes("wrd")
@sb.utils.data_pipeline.provides("wrd", "tokens_list", "tokens_bos")
def text_pipeline(wrd):
	yield wrd
	tokens_list = tokenizer.encode_as_ids(wrd)
	yield tokens_list
	tokens_bos = torch.LongTensor([hparams["bos_index"]] + (tokens_list))
	yield tokens_bos
</code></pre></div></div>

<p>You attach it to the <code class="language-plaintext highlighter-rouge">DynamicItemDataset</code>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sb.dataio.dataset.add_dynamic_item(train_data, text_pipeline)
sb.dataio.dataset.add_dynamic_item(train_data, audio_pipeline)
</code></pre></div></div>
<p>The “takes” are columns available in the CSV file. The “provides” you can access in <code class="language-plaintext highlighter-rouge">compute_forward</code>  like this (<a href="https://github.com/speechbrain/speechbrain/blob/develop/recipes/LibriSpeech/ASR/CTC/train_with_wav2vec.py#L35">code link</a>):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def compute_forward(self, batch, stage):
	batch = batch.to(self.device)
	wavs, wav_lens = batch.sig
	tokens_bos, _ = batch.tokens_bos
</code></pre></div></div>
<p>Some magic that happens is when the batch is created padding is automatically done and therefore <code class="language-plaintext highlighter-rouge">batch.sig</code> also returns a second tensor which specifies the relative length of each sample (relative to the longest sample, which will have the relative length of 1.0).</p>

<p>I like the config and how powerful it is.  I think they’ve made some good decisions about how to allow people to make the changes they need to write code specific to their problem. I have though sometimes been bitten by behaviour that I did not expect. It definitely is a framework, something that expects you to do things in a certain way. I have also come across a number of bugs and the code is very object oriented. I think it’s kind of similar to pytorch lightning (the <code class="language-plaintext highlighter-rouge">Brain</code> class is similar to the <code class="language-plaintext highlighter-rouge">LightningModule</code>).</p>

<p>Everything under <code class="language-plaintext highlighter-rouge">speechbrain/</code> is like a library of everything you could need, recipes will always use models, dataloaders and utility functions from there.</p>

<p>To train a model you call <code class="language-plaintext highlighter-rouge">Brain.fit</code>, which looks like this (<a href="https://github.com/speechbrain/speechbrain/blob/b1934fa38d9a073eb105e6ec9ffa2119cd4142bf/speechbrain/core.py#L1075">code link</a>) (all code snippets are shortened):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if not (
	isinstance(train_set, DataLoader)
	or isinstance(train_set, LoopedLoader)
):
	train_set = self.make_dataloader(train_set, stage=sb.Stage.TRAIN, 
         **train_loader_kwargs)
[..]
self.on_fit_start()
[..]
for epoch in epoch_counter:
	self._fit_train(train_set=train_set, epoch=epoch, enable=enable)
	self._fit_valid(valid_set=valid_set, epoch=epoch, enable=enable)
[..]
</code></pre></div></div>
<p>There is some logic for creating the dataloader if necessary and then the training loop is started. 
<code class="language-plaintext highlighter-rouge">_fit_train</code> (<a href="https://github.com/speechbrain/speechbrain/blob/b1934fa38d9a073eb105e6ec9ffa2119cd4142bf/speechbrain/core.py#L982">code link</a>) encapsulates training for one epoch:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[..]
self.on_stage_start(Stage.TRAIN, epoch)
[..]
with tqdm(train_set, initial=self.step, dynamic_ncols=True, disable=not enable) as t:
	for batch in t:
		[..]
		loss = self.fit_batch(batch)
		self.avg_train_loss = self.update_average(loss, self.avg_train_loss)
		[..]
self.on_stage_end(Stage.TRAIN, self.avg_train_loss, epoch)
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">fit_batch</code> (<a href="https://github.com/speechbrain/speechbrain/blob/develop/speechbrain/core.py#L842">code link</a>), it’s here that <code class="language-plaintext highlighter-rouge">compute_forward</code> and <code class="language-plaintext highlighter-rouge">compute_objectives</code> are used and the optimization step is taken:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>should_step = self.step % self.grad_accumulation_factor == 0
# Managing automatic mixed precision
if self.auto_mix_prec:
	[..]
else:
	outputs = self.compute_forward(batch, Stage.TRAIN)
	loss = self.compute_objectives(outputs, batch, Stage.TRAIN)
	with self.no_sync(not should_step):
		(loss / self.grad_accumulation_factor).backward()

	if should_step:
		if self.check_gradients(loss):
			self.optimizer.step()
		self.optimizer.zero_grad()
		self.optimizer_step += 1
self.on_fit_batch_end(batch, outputs, loss, should_step)
return loss.detach().cpu()
</code></pre></div></div>

<p>Each recipe in <code class="language-plaintext highlighter-rouge">speechbrain/recipes/</code> contains a <code class="language-plaintext highlighter-rouge">train*.py</code> that you use for calling training (there is no single binary like with fairseq). A training command will look like (from the recipe dir):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train_with_wav2vec.py hparams/train_with_wav2vec.yaml
</code></pre></div></div>
<p>or</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train_speaker_embeddings.py hparams/train_x_vectors.yaml
</code></pre></div></div>

<h1 id="k2icefalllhotse">k2/icefall/lhotse</h1>

<p>This is by far the newest of the three tools I’m comparing here. I wrote k2/icefall/lhotse because there are three different repositories you would make use of: <a href="https://github.com/k2-fsa/k2">k2</a> for ragged tensors, lattice related operations and loss functions; <a href="https://github.com/lhotse-speech/lhotse">lhotse</a> for everything related to data such as downloading a corpus, feature extraction and dynamic batching; <a href="https://github.com/k2-fsa/icefall">icefall</a> for the recipes (for example for transducer models trained on LibriSpeech) which use lhotse and k2.</p>

<p>There is no unified training loop, each recipe will have its own straight-forward implementation making use of lhotse and k2 to do training and inference.</p>

<p>lhotse has a commandline interface which is quite powerful. It lets you extract features, get duration statistics and convert kaldi style data dirs to lhotse’s format among other things. See <a href="https://lhotse.readthedocs.io/en/latest/cuts.html">lhotse documentation</a> for an explanation on lhotse’s data constructs like Manifests, Cuts (basically an utterance) and CutSets. An example for getting duration statistics of a CutSet manifest:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ lhotse cut describe cuts_example.jsonl.gz
Cuts count: 16028
Total duration (hh:mm:ss): 17:53:29
Speech duration (hh:mm:ss): 17:53:29 (100.0%)
Duration statistics (seconds):
mean    4.0
std     4.1
min     0.2
25%     1.4
50%     2.6
75%     5.1
99%     20.5
99.5%   23.3
99.9%   27.6
max     30.5
Recordings available: 16028
Features available: 16028
Supervisions available: 16028
</code></pre></div></div>

<p>You can also do all this via code of course. An example, this:</p>
<ol>
  <li>Converts from a kaldi style datadir to a recording and supervision set (lhotse constructs)</li>
  <li>Creates a CutSet object</li>
  <li>Trims the recordings to when the supervisions (transcripts) are (the utterance transcripts have timestamps associated) (now equivalent to a set of utterances)</li>
  <li>Extracts features</li>
  <li>Writes a manifest to a file (lhotse’s data format on disk):
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>recording_set, supervision_set, _ = load_kaldi_data_dir(kdata, 16000, num_jobs=4)
cuts = CutSet.from_manifests(recordings=recording_set, supervisions=supervision_set)
cuts = cuts.trim_to_supervisions()
cuts = cuts.compute_and_store_features(Fbank(), storage_path=outf + '-feats', num_jobs=6, storage_type=LilcomChunkyWriter)
cuts.to_file(outf + '.jsonl.gz')
</code></pre></div>    </div>
  </li>
</ol>

<p>I think lhotse has a very nice interface and definitely plan on basing all my speech recipes around it. I will say though the lhotse codebase has a lot of indirection going on (decorators everywhere) making it hard to reason about the code and debugging sometimes a pain.</p>

<p>k2 is more low-level. It’s for example used for <a href="https://k2-fsa.github.io/k2/core_concepts/index.html#ragged-arrays">RaggedTensors</a> like this where y is a list of lists:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y = k2.RaggedTensor(y)
</code></pre></div></div>
<p>And for FSTs. k2 saves FSTs in pytorch’s (actually pickling) format so you load them from disk like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dct = torch.load(args.decode_graph, map_location=device)
decoding_graph = k2.Fsa.from_dict(dct)
</code></pre></div></div>
<p>Otherwise it contains many constructs for loss functions, FST operations and decoders: <code class="language-plaintext highlighter-rouge">k2.ctc_loss</code>, <code class="language-plaintext highlighter-rouge">k2.rnnt_loss</code>, <code class="language-plaintext highlighter-rouge">k2.rnnt_loss_pruned</code>, <code class="language-plaintext highlighter-rouge">k2.intersect</code>, <code class="language-plaintext highlighter-rouge">k2.top_sort</code>, <code class="language-plaintext highlighter-rouge">k2.RnntDecodingStream</code>  are some examples. You will only use it yourself if you want to do something with the loss function, the decoder or an FST. It’s not necessary to understand if you just want to run recipes and change the data and/or model.</p>

<p>Now about icefall. Regarding the repo layout, <code class="language-plaintext highlighter-rouge">egs/</code> is for the recipes (don’t understand why they didn’t move away from the opaque term kaldi that used) and <code class="language-plaintext highlighter-rouge">icefall/</code> basically contains utility functions. Note, unlike in speechbrain and fairseq, this dir <code class="language-plaintext highlighter-rouge">icefall/</code> (the dir where all the python library code goes) contains not much code as the majority is kept recipe specific, for example the training loop. This makes the recipes easy to read and modify as there is no framework involved (it’s just normal pytorch) and minimal coupling: You don’t have to jump to ten different places to follow the logic.</p>

<p>So for example <a href="https://github.com/k2-fsa/icefall/tree/master/egs/librispeech/ASR/pruned_transducer_stateless2">egs/librispeech/ASR/pruned_transducer_stateless2</a> contains the files <code class="language-plaintext highlighter-rouge">beam_search.py</code>, <code class="language-plaintext highlighter-rouge">conformer.py</code>, <code class="language-plaintext highlighter-rouge">decode.py</code>, <code class="language-plaintext highlighter-rouge">decoder.py</code>, <code class="language-plaintext highlighter-rouge">joiner.py</code>, <code class="language-plaintext highlighter-rouge">model.py</code>, <code class="language-plaintext highlighter-rouge">optim.py</code>, <code class="language-plaintext highlighter-rouge">scaling.py</code> and <code class="language-plaintext highlighter-rouge">train.py</code>. Basically everything is in there, and only the “boring” stuff (like word symbol tables, checkpointing, logging) is relegated to the <code class="language-plaintext highlighter-rouge">icefall/</code> library.</p>

<p>Sidenote, there’s some interesting new ideas regarding the basics of DNNs in icefall. The main one is that instead of using LayerNorm/BatchNorm it makes sense to have a differentiable scale parameter for the weight matrices. The <code class="language-plaintext highlighter-rouge">scaling.py</code> file contains new implementations of all basic modules that follow this new approach and <code class="language-plaintext highlighter-rouge">optim.py</code> has a custom optimizer for these new modules.</p>

<p>Back to the main thread: Everything important is inside the recipe folder. The training loop could not look more normal (that’s good!). After the usual setup (<a href="https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/train.py#L987">code link</a>):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[..]
for epoch in range(params.start_epoch, params.num_epochs):
	[..]
	train_one_epoch(params=params, model=model, optimizer=optimizer, 
        scheduler=scheduler, sp=sp, train_dl=train_dl, valid_dl=valid_dl, 
        scaler=scaler,
tb_writer=tb_writer, world_size=world_size, rank=rank)
	[..]
	save_checkpoint(params=params, model=model, optimizer=optimizer, 
        scheduler=scheduler, sampler=train_dl.sampler, scaler=scaler, rank=rank)
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">train_one_epoch</code> looks like (<a href="https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless2/train.py#L760">code link</a>):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for batch_idx, batch in enumerate(train_dl):
	[..]
	loss, loss_info = compute_loss(params=params, model=model, sp=sp, batch=batch,
is_training=True, warmup=(params.batch_idx_train / params.model_warm_step))
	[..]
	if batch_idx &gt; 0 and batch_idx % params.valid_interval == 0:
		valid_loss = compute_validation_loss(params=params, model=model, sp=sp,
			valid_dl=valid_dl, world_size=world_size)
	[..]
</code></pre></div></div>

<p>One neat feature is that if a training crashes or you ctrl+c it, the current batch will automatically be dumped to disk for you to inspect it.</p>

<p>I like the overall structure, and appreciate the focus on delivering something first and thinking about unifying it later. The results are very good.
However, because this is all very new it’s in a state of flux. Things change and sometimes there are bugs. On the bright side everyone involved is very responsive and if something doesn’t work one can get help very quickly.</p>

<p>Training is similar to speechbrain with the recipe specific <code class="language-plaintext highlighter-rouge">train.py</code> files. Unfortunately there are not many READMEs, but the top of a file usually has documentation. For “egs/librispeech/ASR/pruned_transducer_stateless2” the training command looks like:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PYTHONPATH=/path/to/icefall ./pruned_transducer_stateless2/train.py \
--world-size 4 \
--num-epochs 30 \
--start-epoch 0 \
--use-fp16 1 \
--exp-dir pruned_transducer_stateless2/exp \
--full-libri 1 \
--max-duration 550
</code></pre></div></div>

<p>Note setting the <code class="language-plaintext highlighter-rouge">PYTHONPATH</code> is necessary as icefall is not pip-installable, and instead one uses the <code class="language-plaintext highlighter-rouge">PYTHONPATH</code> so that the imports (<code class="language-plaintext highlighter-rouge">from icefall.utils import bla</code>) in <code class="language-plaintext highlighter-rouge">train.py</code> work.</p>


  

  
</article>

        </div>
        <footer class="footer pure-g">
  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      &copy;&nbsp;<time datetime="2020-10-04T12:40:02+00:00">2020</time>-<time datetime="2024-03-04T03:37:00+00:00">2024</time>&nbsp;<a href="" target="_blank">Rudolf A. Braun</a>. All right reserved.
    </small>
  </div>

  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> & <a href="https://github.com/zivong/jekyll-theme-hydure" target="_blank">Hydure</a>
    </small>
  </div>
</footer>

      </div>
    </div>

    

    
  </body>
</html>
