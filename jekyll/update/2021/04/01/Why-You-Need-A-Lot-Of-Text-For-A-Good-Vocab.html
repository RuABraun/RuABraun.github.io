<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link type="application/atom+xml" rel="alternate" href="https://ruabraun.github.io/feed.xml" title="Sharings" />
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Why you need (at least) a billion words to get a good language model | Sharings</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Why you need (at least) a billion words to get a good language model" />
<meta name="author" content="Rudolf A. Braun" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail." />
<meta property="og:description" content="2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail." />
<link rel="canonical" href="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" />
<meta property="og:url" content="https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html" />
<meta property="og:site_name" content="Sharings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-01T10:06:02+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Why you need (at least) a billion words to get a good language model" />
<script type="application/ld+json">
{"headline":"Why you need (at least) a billion words to get a good language model","dateModified":"2021-04-01T10:06:02+00:00","datePublished":"2021-04-01T10:06:02+00:00","author":{"@type":"Person","name":"Rudolf A. Braun"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html"},"description":"2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail.","url":"https://ruabraun.github.io/jekyll/update/2021/04/01/Why-You-Need-A-Lot-Of-Text-For-A-Good-Vocab.html","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/pure-min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/grids-responsive-min.css">
  <link rel="stylesheet" href="/assets/css/open-color.css">
  <link rel="stylesheet" href="/assets/css/hydure.css">

  <script async src="https://use.fontawesome.com/releases/v5.12.0/js/all.js"></script>

  

</head>


  <body>
    <div id="layout" class="pure-g">
      <div class="sidebar pure-u-1 pure-u-md-1-4" style="background-color: #202421">
        <div class="sidebar-shield">
          <header class="header">
  <img src="https://ruabraun.github.io/images/profpic.jpg" alt="Me!" width="40%" height="40%" class="profile">
  <a class="brand-title" href="/">Sharings</a>
  <p class="brand-name">Rudolf A. Braun</p>
  <p class="brand-tagline">On speech recognition and maybe other things</p>

  
    <nav class="nav pure-menu">
      <ul class="pure-menu-list">
      
        <li class="nav-item pure-menu-item">
          <a href="/" class="pure-menu-link ">
            Home
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/about/" class="pure-menu-link ">
            About
          </a>
        </li>
      
      </ul>
    </nav>
  

  
    <div class="social pure-menu pure-menu-horizontal">
      <ul class="social-list pure-menu-list">
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="mailto:rab014@gmail.com" target="_blank">
              <i class="fas fa-envelope" title="Email"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://twitter.com/fasttosmile" target="_blank">
              <i class="fab fa-twitter" title="Twitter"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://github.com/RuABraun/" target="_blank">
              <i class="fab fa-github" title="GitHub"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://www.linkedin.com/in/rudolf-a-braun-speech/" target="_blank">
              <i class="fab fa-linkedin" title="LinkedIn"></i>
            </a>
          </li>
        
      </ul>
    </div>
  
</header>

        </div>
      </div>
      <div class="content pure-u-1 pure-u-md-3-4">
        <div class="main">
          <article class="post">
  
    <div class="post-meta">
      <ul class="post-categories"><li>
            <span class="post-category">jekyll</span></li><li>
            <span class="post-category">update</span></li></ul>
    </div>
  
  <h1 class="post-title">Why you need (at least) a billion words to get a good language model</h1>
  <div class="post-meta">
    <time datetime="2021-04-01T10:06:02+00:00" itemprop="datePublished">
      01 Apr 2021
    </time></div>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
  <p>2024 Update: The title seems blindingly obvious in light of the current trends of training on trillions of words. Still I think it’s good to point out practically how, for those not aware, language has a surprisingly long tail.</p>

<p>I have a German text corpus with nearly 90 million words. Seems enough to create a decent language model no? Let’s see. The first thing to realise is just covering relatively normal words requires having several hundred thousand words in the vocabulary. Let’s see what happens when I get a count of all words and check what is at the nth position.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>199989 krisenbewältigungen 2
199999 gendersensitiv 2
200002 umgehbar 2
200005 widersinnigen 2
200016 ausmehrungen 2
</code></pre></div></div>
<p>The words I’m showing here are legitimite. Good we have them in our vocabulary. But (!) their counts are very low. The thing to realize is we will never be able to actually learn good models for these words because they appear so inoften in our training corpus. Note that the count of 2 starts from the ~160 000th word!<sup>1</sup></p>

<p>It is kind of expected for this to happen, since it is well known that word counts follow zipf’s law, which put simply states that as you go down a table of words sorted by count, their counts decrease very rapidly, meaning a very large amount of the probability mass is covered by the top words. Here is an image (forgive the lack of axis labels please, y-axis is count in millions x-axis rank of word):</p>

<p><img src="https://ruabraun.github.io/images/words.png" style="display: block; margin: auto;" /></p>

<p>Look at it! (yes I know log scale bla bla, shush pedants) The counts drop extremely quickly to almost nothing.</p>

<p>So the main point is that you need to have seen a word a couple times to know how it is used. But because in language there is a looong tail of words that are used infrequently (but are still normal enough that you do want to estimate them!) you need a <strong>lot</strong> of text to get the counts to a reasonable level.</p>

<p>The additional thing to realise is that because the counts are so low there will be a <strong>ton</strong> of noise in the results. Depending on the corpus some words whose “true” probability is higher will be much lower and vice versa. This is bad.</p>

<p>This is why you need to train on (at least) billion word corpuses to get good results. Then all those 2s will turn into 20s, and then the model can learn something.</p>

<p><sup>1</sup> The total count is around 400k by the way (a good chunk of them are rubbish).</p>


  

  
</article>

        </div>
        <footer class="footer pure-g">
  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      &copy;&nbsp;<time datetime="2020-10-04T12:40:02+00:00">2020</time>-<time datetime="2026-02-08T23:17:47+00:00">2026</time>&nbsp;<a href="" target="_blank">Rudolf A. Braun</a>. All right reserved.
    </small>
  </div>

  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> & <a href="https://github.com/zivong/jekyll-theme-hydure" target="_blank">Hydure</a>
    </small>
  </div>
</footer>

      </div>
    </div>

    

    
  </body>
</html>
