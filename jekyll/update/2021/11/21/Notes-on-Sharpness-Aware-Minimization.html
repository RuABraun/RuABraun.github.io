<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link type="application/atom+xml" rel="alternate" href="https://ruabraun.github.io/feed.xml" title="Sharings" />
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Notes on Sharpness Aware Minimization and LM paper using it | Sharings</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Notes on Sharpness Aware Minimization and LM paper using it" />
<meta name="author" content="Rudolf A. Braun" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="WIP" />
<meta property="og:description" content="WIP" />
<link rel="canonical" href="https://ruabraun.github.io/jekyll/update/2021/11/21/Notes-on-Sharpness-Aware-Minimization.html" />
<meta property="og:url" content="https://ruabraun.github.io/jekyll/update/2021/11/21/Notes-on-Sharpness-Aware-Minimization.html" />
<meta property="og:site_name" content="Sharings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-21T10:06:02+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Notes on Sharpness Aware Minimization and LM paper using it" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Notes on Sharpness Aware Minimization and LM paper using it","dateModified":"2021-11-21T10:06:02+00:00","datePublished":"2021-11-21T10:06:02+00:00","author":{"@type":"Person","name":"Rudolf A. Braun"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ruabraun.github.io/jekyll/update/2021/11/21/Notes-on-Sharpness-Aware-Minimization.html"},"description":"WIP","url":"https://ruabraun.github.io/jekyll/update/2021/11/21/Notes-on-Sharpness-Aware-Minimization.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/pure-min.css" crossorigin="anonymous">
  <link rel="stylesheet" href="https://unpkg.com/purecss@2.0.6/build/grids-responsive-min.css">
  <link rel="stylesheet" href="/assets/css/open-color.css">
  <link rel="stylesheet" href="/assets/css/hydure.css">

  <script async src="https://use.fontawesome.com/releases/v5.12.0/js/all.js"></script>

  

</head>


  <body>
    <div id="layout" class="pure-g">
      <div class="sidebar pure-u-1 pure-u-md-1-4" style="background-color: #202421">
        <div class="sidebar-shield">
          <header class="header">
  <img src="https://ruabraun.github.io/images/profpic.jpg" alt="Me!" width="40%" height="40%" class="profile">
  <a class="brand-title" href="/">Sharings</a>
  <p class="brand-name">Rudolf A. Braun</p>
  <p class="brand-tagline">On speech recognition and maybe other things</p>

  
    <nav class="nav pure-menu">
      <ul class="pure-menu-list">
      
        <li class="nav-item pure-menu-item">
          <a href="/" class="pure-menu-link ">
            Home
          </a>
        </li>
      
        <li class="nav-item pure-menu-item">
          <a href="/about/" class="pure-menu-link ">
            About
          </a>
        </li>
      
      </ul>
    </nav>
  

  
    <div class="social pure-menu pure-menu-horizontal">
      <ul class="social-list pure-menu-list">
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="mailto:rab014@gmail.com" target="_blank">
              <i class="fas fa-envelope" title="Email"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://twitter.com/fasttosmile" target="_blank">
              <i class="fab fa-twitter" title="Twitter"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://github.com/RuABraun/" target="_blank">
              <i class="fab fa-github" title="GitHub"></i>
            </a>
          </li>
        
          <li class="social-item pure-menu-item">
            <a class="pure-menu-link pure-button" href="https://www.linkedin.com/in/rudolf-a-braun-speech/" target="_blank">
              <i class="fab fa-linkedin" title="LinkedIn"></i>
            </a>
          </li>
        
      </ul>
    </div>
  
</header>

        </div>
      </div>
      <div class="content pure-u-1 pure-u-md-3-4">
        <div class="main">
          <article class="post">
  
    <div class="post-meta">
      <ul class="post-categories"><li>
            <span class="post-category">jekyll</span></li><li>
            <span class="post-category">update</span></li></ul>
    </div>
  
  <h1 class="post-title">Notes on Sharpness Aware Minimization and LM paper using it</h1>
  <div class="post-meta">
    <time datetime="2021-11-21T10:06:02+00:00" itemprop="datePublished">
      21 Nov 2021
    </time></div>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  
  <p>WIP</p>

<p>Notes on <a href="https://arxiv.org/pdf/2010.01412.pdf">SAM paper</a> and the child paper using <a href="https://arxiv.org/pdf/2110.08529.pdf">SAM for language modeling</a>.</p>

<p>Basic idea of SAM is to perturb the weights in different directions and minimize the loss where it’s at its max.</p>

<p>So instead of just \(\underset{w}{\min} L_S(w)\) do \(\underset{w}{\min} \underset{\Vert \epsilon \Vert_{p} \le \rho}{\max} L_S(w + \epsilon)\), where p equals 2 and \(\rho\) is a hyperparameter set to a value like 0.05.</p>

<p>They also have an additional term which in practice they set to the L2 loss of the weights. Not clear to me why the term is necessary.</p>

<p>Actually trying out different \(\epsilon\)s would be very costly obviously so instead to do the minimization they first use a taylor expansion like:</p>

\[\underset{\Vert \epsilon \Vert_p}{\arg\max} L_S(w + \epsilon) \approx \underset{\Vert \epsilon \Vert_p}{\arg\max} L_S(w) + \epsilon \nabla_w L_S(w) = \underset{\Vert \epsilon \Vert_p}{\arg\max} \epsilon \nabla_w L_S(w)\]

<p>The value of \(\epsilon\) that solves this approximation is “given by the solution to a classical dual norm problem […]” \(\hat \epsilon\). This leads to the following expression and loss (from the paper):</p>

<p><img src="https://ruabraun.github.io/images/sam_lossequation.png" style="display: block; margin: auto;" /></p>

<p>So in the end they end up throwing away the complicated terms and just using the gradient information to climb in the steepest direction. The amount they climb is determined by the hyperparameter \(\rho\).</p>

<p>Note that this is similar to when trying to find an adversial example: Changing the variables (this time the weights, not the input) by a small amount to maximize the loss.</p>

<p>The procedure is to compute the loss for weights \(w\), then use that loss and gradient to compute \(\hat \epsilon\), perturb the weights, compute the loss again and use that gradient to update.</p>

<p>There’s a pytorch implementation <a href="https://github.com/davda54/sam">here</a>. Code looks quite simple. The result of the first forward-backward is to set <code class="language-plaintext highlighter-rouge">e_w=p.grad*scale</code> and modify the parameters \(p\) by <code class="language-plaintext highlighter-rouge">p.add_(e_w)</code>, upon which after the second forward-backward the parameters are reset and one does a normal update <code class="language-plaintext highlighter-rouge">self.base_optimizer.step()</code> with the new gradient.</p>

<p>Looking at it practically, this means the gradients will always comes from a nearby “higher point” (higher as in the loss is higher). What effect will this have on the optimization trajectory? If you’re already in a minimum it’s already too late I guess. You’re just going to be redirected to the one you’re already in right? But if you’re bouncing between minima then if you land in a sharp one you should get a bigger gradient correction than if you land in a flat minima where the higher points are actually not that high. So I guess it improves the chances of landing in a good minima.</p>

<p>There’s a bunch of other interesting content. One point they mention is that the updates are of course done per batch, which means values like \(\hat \epsilon\) are only estimated with small number of datapoints rather than the entire dataset. They look into what the effect of using subsets (of the batch) of size \(m\) which are then used to compute the SAM update across different GPUs and then averaging, and find that having smaller sizes performs better!</p>

<p>The child paper mentions that one can use a fourth smaller batch size in the first step to speed things up with losing performance.</p>

<p>They use \(m\) to mean the number of subsets that are created from the batch (rather than being \(m\) being the size of the subset).</p>

<p>Finally they show that SAM improves results for all model sizes and especially when the training data is small. The \(\rho\) does need to be tuned, however they identify 0.15 as a good default.</p>


  

  
</article>

        </div>
        <footer class="footer pure-g">
  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      &copy;&nbsp;<time datetime="2020-10-04T12:40:02+00:00">2020</time>-<time datetime="2022-07-01T13:54:25+00:00">2022</time>&nbsp;<a href="" target="_blank">Rudolf A. Braun</a>. All right reserved.
    </small>
  </div>

  <div class="pure-u-1 pure-u-md-1-2">
    <small>
      Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> & <a href="https://github.com/zivong/jekyll-theme-hydure" target="_blank">Hydure</a>
    </small>
  </div>
</footer>

      </div>
    </div>

    

    
  </body>
</html>
